<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-04-15T18:25:02Z</responseDate>
<request verb="ListRecords" until="2015-01-31" from="2010-01-01" metadataPrefix="arXiv" set="stat">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:0704.0923</identifier>
 <datestamp>2010-09-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0704.0923</id><created>2007-04-06</created><authors><author><keyname>Miller</keyname><forenames>Steven J.</forenames></author></authors><title>When the Cramer-Rao Inequality provides no information</title><categories>math.ST stat.TH</categories><comments>10 pages, 1 figure, to appear in Communications in Information and
  Systems</comments><journal-ref>Communications in Information and Systems 7 (2007), no. 3,
  265--272</journal-ref><abstract>  We investigate a one-parameter family of probability densities (related to
the Pareto distribution, which describes many natural phenomena) where the
Cramer-Rao inequality provides no information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0704.1074</identifier>
 <datestamp>2010-02-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0704.1074</id><created>2007-04-09</created><updated>2008-02-05</updated><authors><author><keyname>Aoki</keyname><forenames>Satoshi</forenames></author><author><keyname>Hibi</keyname><forenames>Takayuki</forenames></author><author><keyname>Ohsugi</keyname><forenames>Hidefumi</forenames></author><author><keyname>Takemura</keyname><forenames>Akimichi</forenames></author></authors><title>Markov basis and Groebner basis of Segre-Veronese configuration for
  testing independence in group-wise selections</title><categories>math.ST math.AC stat.AP stat.TH</categories><comments>25 pages, 5 figures</comments><msc-class>62H17</msc-class><journal-ref>Annals of the Institute of Statistical Mathematics (2010). Vol.
  62, 299--321</journal-ref><doi>10.1007/s10463-008-0171-7</doi><abstract>  We consider testing independence in group-wise selections with some
restrictions on combinations of choices. We present models for frequency data
of selections for which it is easy to perform conditional tests by Markov chain
Monte Carlo (MCMC) methods. When the restrictions on the combinations can be
described in terms of a Segre-Veronese configuration, an explicit form of a
Gr\"obner basis consisting of moves of degree two is readily available for
performing a Markov chain. We illustrate our setting with the National Center
Test for university entrance examinations in Japan. We also apply our method to
testing independence hypotheses involving genotypes at more than one locus or
haplotypes of alleles on the same chromosome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0704.1976</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0704.1976</id><created>2007-04-16</created><authors><author><keyname>Brody</keyname><forenames>Dorje C.</forenames></author><author><keyname>Hughston</keyname><forenames>Lane P.</forenames></author><author><keyname>Macrina</keyname><forenames>Andrea</forenames></author></authors><title>Information-Based Asset Pricing</title><categories>q-fin.PR math.PR math.ST stat.TH</categories><comments>32 pages. No figure</comments><journal-ref>International Journal of Theoretical and Applied Finance 11,
  107-142 (2008)</journal-ref><abstract>  A new framework for asset price dynamics is introduced in which the concept
of noisy information about future cash flows is used to derive the price
processes. In this framework an asset is defined by its cash-flow structure.
Each cash flow is modelled by a random variable that can be expressed as a
function of a collection of independent random variables called market factors.
With each such "X-factor" we associate a market information process, the values
of which are accessible to market agents. Each information process is a sum of
two terms; one contains true information about the value of the market factor;
the other represents "noise". The noise term is modelled by an independent
Brownian bridge. The market filtration is assumed to be that generated by the
aggregate of the independent information processes. The price of an asset is
given by the expectation of the discounted cash flows in the risk-neutral
measure, conditional on the information provided by the market filtration. When
the cash flows are the dividend payments associated with equities, an explicit
model is obtained for the share-price, and the prices of options on
dividend-paying assets are derived. Remarkably, the resulting formula for the
price of a European call option is of the Black-Scholes-Merton type. The
information-based framework also generates a natural explanation for the origin
of stochastic volatility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0704.2167</identifier>
 <datestamp>2012-01-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0704.2167</id><created>2007-04-17</created><updated>2012-01-24</updated><authors><author><keyname>Belloni</keyname><forenames>Alexandre</forenames></author><author><keyname>Chernozhukov</keyname><forenames>Victor</forenames></author></authors><title>On the Computational Complexity of MCMC-based Estimators in Large
  Samples</title><categories>math.ST math.PR stat.CO stat.TH</categories><comments>36 pages, 2 figures</comments><journal-ref>Ann. Statist. Volume 37, Number 4 (2009), 2011-2055</journal-ref><doi>10.1214/08-AOS634</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we examine the implications of the statistical large sample
theory for the computational complexity of Bayesian and quasi-Bayesian
estimation carried out using Metropolis random walks. Our analysis is motivated
by the Laplace-Bernstein-Von Mises central limit theorem, which states that in
large samples the posterior or quasi-posterior approaches a normal density.
Using the conditions required for the central limit theorem to hold, we
establish polynomial bounds on the computational complexity of general
Metropolis random walks methods in large samples. Our analysis covers cases
where the underlying log-likelihood or extremum criterion function is possibly
non-concave, discontinuous, and with increasing parameter dimension. However,
the central limit theorem restricts the deviations from continuity and
log-concavity of the log-likelihood or extremum criterion function in a very
specific manner.
  Under minimal assumptions required for the central limit theorem to hold
under the increasing parameter dimension, we show that the Metropolis algorithm
is theoretically efficient even for the canonical Gaussian walk which is
studied in detail. Specifically, we show that the running time of the algorithm
in large samples is bounded in probability by a polynomial in the parameter
dimension $d$, and, in particular, is of stochastic order $d^2$ in the leading
cases after the burn-in period. We then give applications to exponential
families, curved exponential families, and Z-estimation of increasing
dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0704.3649</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0704.3649</id><created>2007-04-27</created><updated>2014-07-14</updated><authors><author><keyname>Chernozhukov</keyname><forenames>Victor</forenames><affiliation>MIT</affiliation></author><author><keyname>Fernandez-Val</keyname><forenames>Ivan</forenames><affiliation>Boston University</affiliation></author><author><keyname>Galichon</keyname><forenames>Alfred</forenames><affiliation>Ecole Polytechnique</affiliation></author></authors><title>Quantile and Probability Curves Without Crossing</title><categories>stat.ME math.ST stat.TH</categories><comments>29 pages, 4 figures</comments><journal-ref>Econometrica (2010) 78 (3): 1093-1125</journal-ref><doi>10.3982/ECTA7880</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a method to address the longstanding problem of lack of
monotonicity in estimation of conditional and structural quantile functions,
also known as the quantile crossing problem. The method consists in sorting or
monotone rearranging the original estimated non-monotone curve into a monotone
rearranged curve. We show that the rearranged curve is closer to the true
quantile curve in finite samples than the original curve, establish a
functional delta method for rearrangement-related operators, and derive
functional limit theory for the entire rearranged curve and its functionals. We
also establish validity of the bootstrap for estimating the limit law of the
the entire rearranged curve and its functionals. Our limit results are generic
in that they apply to every estimator of a monotone econometric function,
provided that the estimator satisfies a functional central limit theorem and
the function satisfies some smoothness conditions. Consequently, our results
apply to estimation of other econometric functions with monotonicity
restrictions, such as demand, production, distribution, and structural
distribution functions. We illustrate the results with an application to
estimation of structural quantile functions using data on Vietnam veteran
status and earnings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0704.3686</identifier>
 <datestamp>2010-11-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0704.3686</id><created>2007-04-27</created><updated>2010-11-03</updated><authors><author><keyname>Chernozhukov</keyname><forenames>Victor</forenames><affiliation>MIT</affiliation></author><author><keyname>Fernandez-Val</keyname><forenames>Ivan</forenames><affiliation>Boston University</affiliation></author><author><keyname>Galichon</keyname><forenames>Alfred</forenames><affiliation>Harvard University</affiliation></author></authors><title>Improving Estimates of Monotone Functions by Rearrangement</title><categories>stat.ME math.ST stat.TH</categories><comments>31 pages, 8 figures, low resolution figures. This paper has been
  withdraw by the authors because it has been replaced by "Improving Point and
  Interval Estimates of Monotone Functions by Rearrangement," arXiv:0806.4730</comments><abstract>  Suppose that a target function is monotonic, namely, weakly increasing, and
an original estimate of the target function is available, which is not weakly
increasing. Many common estimation methods used in statistics produce such
estimates. We show that these estimates can always be improved with no harm
using rearrangement techniques: The rearrangement methods, univariate and
multivariate, transform the original estimate to a monotonic estimate, and the
resulting estimate is closer to the true curve in common metrics than the
original estimate. We illustrate the results with a computational example and
an empirical example dealing with age-height growth charts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0705.1235</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0705.1235</id><created>2007-05-09</created><updated>2009-04-23</updated><authors><author><keyname>Ngoc</keyname><forenames>Thanh Mai Pham</forenames><affiliation>PMA</affiliation></author></authors><title>Statistical minimax approach of the Hausdorff moment problem</title><categories>math.ST stat.TH</categories><comments>21 pages</comments><msc-class>62G07, 44A60</msc-class><journal-ref>Inverse Problems (2008) 24 045018</journal-ref><doi>10.1088/0266-5611/24/4/045018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to study the problem of estimating a compactly
supported density of probability from noisy observations of its moments. In
fact, we provide a statistical approach to the famous Hausdorff classical
moment problem. We prove an upper bound and a lower bound on the rate of
convergence of the mean squared error showing that the considered estimator
attains minimax rate over the corresponding smoothness classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0705.1613</identifier>
 <datestamp>2010-01-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0705.1613</id><created>2007-05-11</created><updated>2010-01-14</updated><authors><author><keyname>Malouche</keyname><forenames>Dhafer</forenames></author></authors><title>Determining full conditional independence by low-order conditioning</title><categories>math.ST stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ193 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><report-no>IMS-BEJ-BEJ193</report-no><journal-ref>Bernoulli 2009, Vol. 15, No. 4, 1179-1189</journal-ref><doi>10.3150/09-BEJ193</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A concentration graph associated with a random vector is an undirected graph
where each vertex corresponds to one random variable in the vector. The absence
of an edge between any pair of vertices (or variables) is equivalent to full
conditional independence between these two variables given all the other
variables. In the multivariate Gaussian case, the absence of an edge
corresponds to a zero coefficient in the precision matrix, which is the inverse
of the covariance matrix. It is well known that this concentration graph
represents some of the conditional independencies in the distribution of the
associated random vector. These conditional independencies correspond to the
"separations" or absence of edges in that graph. In this paper we assume that
there are no other independencies present in the probability distribution than
those represented by the graph. This property is called the perfect
Markovianity of the probability distribution with respect to the associated
concentration graph. We prove in this paper that this particular concentration
graph, the one associated with a perfect Markov distribution, can be determined
by only conditioning on a limited number of variables. We demonstrate that this
number is equal to the maximum size of the minimal separators in the
concentration graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0705.2701</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0705.2701</id><created>2007-05-18</created><authors><author><keyname>Hsieh</keyname><forenames>Meng-Chen</forenames><affiliation>IOMS</affiliation></author><author><keyname>Hurvich</keyname><forenames>Clifford M.</forenames><affiliation>IOMS</affiliation></author><author><keyname>Soulier</keyname><forenames>Philippe</forenames><affiliation>MODAL'X</affiliation></author></authors><title>Asymptotics for Duration-Driven Long Range Dependent Processes</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00147560</proxy><msc-class>60G10</msc-class><journal-ref>Journal of Econometrics Volume 141, Issue 2, December 2007, Pages
  913-949</journal-ref><doi>10.1016/j.jeconom.2006.12.001</doi><abstract>  We consider processes with second order long range dependence resulting from
heavy tailed durations. We refer to this phenomenon as duration-driven long
range dependence (DDLRD), as opposed to the more widely studied linear long
range dependence based on fractional differencing of an $iid$ process. We
consider in detail two specific processes having DDLRD, originally presented in
Taqqu and Levy (1986), and Parke (1999). For these processes, we obtain the
limiting distribution of suitably standardized discrete Fourier transforms
(DFTs) and sample autocovariances. At low frequencies, the standardized DFTs
converge to a stable law, as do the standardized sample autocovariances at
fixed lags. Finite collections of standardized sample autocovariances at a
fixed set of lags converge to a degenerate distribution. The standardized DFTs
at high frequencies converge to a Gaussian law. Our asymptotic results are
strikingly similar for the two DDLRD processes studied. We calibrate our
asymptotic results with a simulation study which also investigates the
properties of the semiparametric log periodogram regression estimator of the
memory parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0705.3693</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0705.3693</id><created>2007-05-25</created><updated>2007-08-23</updated><authors><author><keyname>Beezley</keyname><forenames>Jonathan D.</forenames></author><author><keyname>Mandel</keyname><forenames>Jan</forenames></author></authors><title>Morphing Ensemble Kalman Filters</title><categories>math.DS cs.CV math.ST physics.ao-ph stat.ME stat.TH</categories><comments>17 pages, 7 figures. Added DDDAS references to the introduction</comments><report-no>UCDHSC CCM Report 240</report-no><msc-class>65P99, 62F15, 94A08</msc-class><doi>10.1111/j.1600-0870.2007.00275.x</doi><abstract>  A new type of ensemble filter is proposed, which combines an ensemble Kalman
filter (EnKF) with the ideas of morphing and registration from image
processing. This results in filters suitable for nonlinear problems whose
solutions exhibit moving coherent features, such as thin interfaces in wildfire
modeling. The ensemble members are represented as the composition of one common
state with a spatial transformation, called registration mapping, plus a
residual. A fully automatic registration method is used that requires only
gridded data, so the features in the model state do not need to be identified
by the user. The morphing EnKF operates on a transformed state consisting of
the registration mapping and the residual. Essentially, the morphing EnKF uses
intermediate states obtained by morphing instead of linear combinations of the
states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0705.4485</identifier>
 <datestamp>2010-02-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0705.4485</id><created>2007-05-30</created><authors><author><keyname>Airoldi</keyname><forenames>Edoardo M</forenames></author><author><keyname>Blei</keyname><forenames>David M</forenames></author><author><keyname>Fienberg</keyname><forenames>Stephen E</forenames></author><author><keyname>Xing</keyname><forenames>Eric P</forenames></author></authors><title>Mixed membership stochastic blockmodels</title><categories>stat.ME cs.LG math.ST physics.soc-ph stat.ML stat.TH</categories><comments>46 pages, 14 figures, 3 tables</comments><journal-ref>Journal of Machine Learning Research, 9, 1981-2014.</journal-ref><abstract>  Observations consisting of measurements on relationships for pairs of objects
arise in many settings, such as protein interaction and gene regulatory
networks, collections of author-recipient email, and social networks. Analyzing
such data with probabilisic models can be delicate because the simple
exchangeability assumptions underlying many boilerplate models no longer hold.
In this paper, we describe a latent variable model of such data called the
mixed membership stochastic blockmodel. This model extends blockmodels for
relational data to ones which capture mixed membership latent relational
structure, thus providing an object-specific low-dimensional representation. We
develop a general variational inference algorithm for fast approximate
posterior inference. We explore applications to social and protein interaction
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0705.4516</identifier>
 <datestamp>2010-03-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0705.4516</id><created>2007-05-31</created><authors><author><keyname>Drton</keyname><forenames>Mathias</forenames></author></authors><title>Multiple solutions to the likelihood equations in the Behrens-Fisher
  problem</title><categories>math.ST stat.TH</categories><journal-ref>Statistics &amp; Probability Letters 2008, Vol. 78, No. 18, 3288-3293</journal-ref><doi>10.1016/j.spl.2008.06.012</doi><abstract>  The Behrens-Fisher problem concerns testing the equality of the means of two
normal populations with possibly different variances. The null hypothesis in
this problem induces a statistical model for which the likelihood function may
have more than one local maximum. We show that such multimodality contradicts
the null hypothesis in the sense that if this hypothesis is true then the
probability of multimodality converges to zero when both sample sizes tend to
infinity. Additional results include a finite-sample bound on the probability
of multimodality under the null and asymptotics for the probability of
multimodality under the alternative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0705.4588</identifier>
 <datestamp>2011-02-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0705.4588</id><created>2007-05-31</created><authors><author><keyname>Zheng</keyname><forenames>Shurong</forenames></author><author><keyname>Song</keyname><forenames>Guodong</forenames></author><author><keyname>Shi</keyname><forenames>Ning-Zhong</forenames></author></authors><title>Variable Selection Incorporating Prior Constraint Information into Lasso</title><categories>stat.ME</categories><comments>15 pages</comments><abstract>  We propose the variable selection procedure incorporating prior constraint
information into lasso. The proposed procedure combines the sample and prior
information, and selects significant variables for responses in a narrower
region where the true parameters lie. It increases the efficiency to choose the
true model correctly. The proposed procedure can be executed by many
constrained quadratic programming methods and the initial estimator can be
found by least square or Monte Carlo method. The proposed procedure also enjoys
good theoretical properties. Moreover, the proposed procedure is not only used
for linear models but also can be used for generalized linear models({\sl
GLM}), Cox models, quantile regression models and many others with the help of
Wang and Leng (2007)'s LSA, which changes these models as the approximation of
linear models. The idea of combining sample and prior constraint information
can be also used for other modified lasso procedures. Some examples are used
for illustration of the idea of incorporating prior constraint information in
variable selection procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0706.0534</identifier>
 <datestamp>2012-01-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0706.0534</id><created>2007-06-04</created><updated>2008-01-11</updated><authors><author><keyname>Zhou</keyname><forenames>Shuheng</forenames></author><author><keyname>Lafferty</keyname><forenames>John</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Compressed Regression</title><categories>stat.ML cs.IT math.IT</categories><comments>59 pages, 5 figure, Submitted for review</comments><journal-ref>IEEE Transactions on Information Theory, Volume 55, No.2, pp
  846--866, 2009</journal-ref><abstract>  Recent research has studied the role of sparsity in high dimensional
regression and signal reconstruction, establishing theoretical limits for
recovering sparse models from sparse data. This line of work shows that
$\ell_1$-regularized least squares regression can accurately estimate a sparse
linear model from $n$ noisy examples in $p$ dimensions, even if $p$ is much
larger than $n$. In this paper we study a variant of this problem where the
original $n$ input variables are compressed by a random linear transformation
to $m \ll n$ examples in $p$ dimensions, and establish conditions under which a
sparse linear model can be successfully recovered from the compressed data. A
primary motivation for this compression procedure is to anonymize the data and
preserve privacy by revealing little information about the original data. We
characterize the number of random projections that are required for
$\ell_1$-regularized compressed regression to identify the nonzero coefficients
in the true model with probability approaching one, a property called
``sparsistence.'' In addition, we show that $\ell_1$-regularized compressed
regression asymptotically predicts as well as an oracle linear model, a
property called ``persistence.'' Finally, we characterize the privacy
properties of the compression procedure in information-theoretic terms,
establishing upper bounds on the mutual information between the compressed and
uncompressed data that decay to zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0706.0881</identifier>
 <datestamp>2011-02-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0706.0881</id><created>2007-06-06</created><authors><author><keyname>Ostrovsky</keyname><forenames>E.</forenames></author><author><keyname>Zelikov</keyname><forenames>Y.</forenames></author></authors><title>Adaptive Optimal Nonparametric Regression and Density Estimation Based
  on Fourier-Legendre Expansion</title><categories>math.ST math.SP stat.TH</categories><msc-class>41A10 (Primary), 62G07 (Secondary)</msc-class><abstract>  Motivated by finance and technical applications, the objective of this paper
is to consider adaptive estimation of regression and density distribution based
on Fourier-Legendre expansion, and construction of confidence intervals - also
adaptive. The estimators are asymptotically optimal and adaptive in the sense
that they can adapt to unknown smoothness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0706.2040</identifier>
 <datestamp>2010-02-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0706.2040</id><created>2007-06-14</created><updated>2007-11-10</updated><authors><author><keyname>Airoldi</keyname><forenames>Edoardo M</forenames></author></authors><title>Getting started in probabilistic graphical models</title><categories>q-bio.QM cs.LG physics.soc-ph stat.ME stat.ML</categories><comments>12 pages, 1 figure</comments><journal-ref>Airoldi EM (2007) Getting started in probabilistic graphical
  models. PLoS Comput Biol 3(12): e252</journal-ref><doi>10.1371/journal.pcbi.0030252</doi><abstract>  Probabilistic graphical models (PGMs) have become a popular tool for
computational analysis of biological data in a variety of domains. But, what
exactly are they and how do they work? How can we use PGMs to discover patterns
that are biologically relevant? And to what extent can PGMs help us formulate
new hypotheses that are testable at the bench? This note sketches out some
answers and illustrates the main ideas behind the statistical approach to
biological pattern discovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0706.2281</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0706.2281</id><created>2007-06-15</created><updated>2007-06-21</updated><authors><author><keyname>Vlasov</keyname><forenames>Alexander Yu.</forenames><affiliation>FRC/IRH</affiliation></author></authors><title>Some questions of Monte-Carlo modeling on nontrivial bundles</title><categories>math-ph math.MP stat.CO</categories><comments>LaTeX, 8pp, 1 fig, v2: mistype in eq(4) corrected</comments><abstract>  In this work are considered some questions of Monte-Carlo modeling on
nontrivial bundles. As a basic example is used problem of generation of
straight lines in 3D space, related with modeling of interaction of a solid
body with a flux of particles and with some other tasks. Space of lines used in
given model is example of nontrivial fiber bundle, that is equivalent with
tangent sheaf of a sphere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0706.3071</identifier>
 <datestamp>2010-06-17</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0706.3071</id><created>2007-06-20</created><updated>2010-06-16</updated><authors><author><keyname>Freitas</keyname><forenames>Ana Cristina Moreira</forenames></author><author><keyname>Freitas</keyname><forenames>Jorge Milhazes</forenames></author></authors><title>Extreme values for Benedicks-Carleson quadratic maps</title><categories>math.DS math.PR math.ST stat.TH</categories><comments>18 pages</comments><msc-class>37A50, 37C40, 37E05, 60G10, 60G70</msc-class><journal-ref>Ergodic Theory Dynam. Systems 28, 2008, no. 4, 1117-1133</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the quadratic family of maps given by $f_{a}(x)=1-a x^2$ with
$x\in [-1,1]$, where $a$ is a Benedicks-Carleson parameter. For each of these
chaotic dynamical systems we study the extreme value distribution of the
stationary stochastic processes $X_0,X_1,...$, given by $X_{n}=f_a^n$, for
every integer $n\geq0$, where each random variable $X_n$ is distributed
according to the unique absolutely continuous, invariant probability of $f_a$.
Using techniques developed by Benedicks and Carleson, we show that the limiting
distribution of $M_n=\max\{X_0,...,X_{n-1}\}$ is the same as that which would
apply if the sequence $X_0,X_1,...$ was independent and identically
distributed. This result allows us to conclude that the asymptotic distribution
of $M_n$ is of Type III (Weibull).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0706.3435</identifier>
 <datestamp>2012-01-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0706.3435</id><created>2007-06-23</created><authors><author><keyname>Szabo</keyname><forenames>Zoltan</forenames></author><author><keyname>Poczos</keyname><forenames>Barnabas</forenames></author><author><keyname>Lorincz</keyname><forenames>Andras</forenames></author></authors><title>Undercomplete Blind Subspace Deconvolution via Linear Prediction</title><categories>stat.ME</categories><comments>12 pages</comments><journal-ref>European Conference on Machine Learning (ECML), pages 740-747,
  2007</journal-ref><abstract>  We present a novel solution technique for the blind subspace deconvolution
(BSSD) problem, where temporal convolution of multidimensional hidden
independent components is observed and the task is to uncover the hidden
components using the observation only. We carry out this task for the
undercomplete case (uBSSD): we reduce the original uBSSD task via linear
prediction to independent subspace analysis (ISA), which we can solve. As it
has been shown recently, applying temporal concatenation can also reduce uBSSD
to ISA, but the associated ISA problem can easily become `high dimensional'
[1]. The new reduction method circumvents this dimensionality problem. We
perform detailed studies on the efficiency of the proposed technique by means
of numerical simulations. We have found several advantages: our method can
achieve high quality estimations for smaller number of samples and it can cope
with deeper temporal convolutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0706.4138</identifier>
 <datestamp>2010-08-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0706.4138</id><created>2007-06-28</created><authors><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author><author><keyname>Fazel</keyname><forenames>Maryam</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo A.</forenames></author></authors><title>Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear
  Norm Minimization</title><categories>math.OC math.ST stat.TH</categories><msc-class>90C25, 90C59, 15A52</msc-class><journal-ref>SIAM Review, Volume 52, Issue 3, pp. 471-501 (2010)</journal-ref><doi>10.1137/070697835</doi><abstract>  The affine rank minimization problem consists of finding a matrix of minimum
rank that satisfies a given system of linear equality constraints. Such
problems have appeared in the literature of a diverse set of fields including
system identification and control, Euclidean embedding, and collaborative
filtering. Although specific instances can often be solved with specialized
algorithms, the general affine rank minimization problem is NP-hard. In this
paper, we show that if a certain restricted isometry property holds for the
linear transformation defining the constraints, the minimum rank solution can
be recovered by solving a convex optimization problem, namely the minimization
of the nuclear norm over the given affine space. We present several random
ensembles of equations where the restricted isometry property holds with
overwhelming probability. The techniques used in our analysis have strong
parallels in the compressed sensing framework. We discuss how affine rank
minimization generalizes this pre-existing concept and outline a dictionary
relating concepts from cardinality minimization to those of rank minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0706.4190</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0706.4190</id><created>2007-06-28</created><authors><author><keyname>Rondonotti</keyname><forenames>Vitaliana</forenames></author><author><keyname>Marron</keyname><forenames>J. S.</forenames></author><author><keyname>Park</keyname><forenames>Cheolwoo</forenames></author></authors><title>SiZer for time series: A new approach to the analysis of trends</title><categories>stat.ME</categories><comments>Published at http://dx.doi.org/10.1214/07-EJS006 in the Electronic
  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-EJS-EJS_2007_6</report-no><msc-class>62G08 (Primary) 62-09 (Secondary)</msc-class><journal-ref>Electronic Journal of Statistics 2007, Vol. 1, 268-289</journal-ref><doi>10.1214/07-EJS006</doi><abstract>  Smoothing methods and SiZer are a useful statistical tool for discovering
statistically significant structure in data. Based on scale space ideas
originally developed in the computer vision literature, SiZer (SIgnificant ZERo
crossing of the derivatives) is a graphical device to assess which observed
features are `really there' and which are just spurious sampling artifacts. In
this paper, we develop SiZer like ideas in time series analysis to address the
important issue of significance of trends. This is not a straightforward
extension, since one data set does not contain the information needed to
distinguish `trend' from `dependence'. A new visualization is proposed, which
shows the statistician the range of trade-offs that are available. Simulation
and real data results illustrate the effectiveness of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0707.0082</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0707.0082</id><created>2007-06-30</created><updated>2013-01-05</updated><authors><author><keyname>Barton</keyname><forenames>Richard J.</forenames></author></authors><title>Minimax Robust Function Reconstruction in Reproducing Kernel Hilbert
  Spaces</title><categories>math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a unified approach to function approximation in
reproducing kernel Hilbert spaces (RKHS) that establishes a previously
unrecognized optimality property for several well-known function approximation
techniques, such as minimum-norm interpolation, smoothing splines, and
pseudo-inverses. We consider the problem of approximating a function belonging
to an arbitrary real-valued RKHS on R^d based on approximate observations of
the function. The observations are approximate in the sense that the actual
observations (i.e., the true function values) are known only to belong to a
convex set of admissible observations. We seek a minimax optimal approximation
for the function that minimizes the supremum of the RKHS norm on the error
between the true function and the chosen approximation subject only to the
conditions that the true function belongs to a uniformly bounded uncertainty
set of functions that satisfy the constraints on the observations and that the
approximation is a member of the RKHS. We refer to such a solution as a minimax
robust reconstruction. We characterize the solution to the minimax robust
reconstruction problem and show that it is equivalent to solving a
straightforward convex optimization problem. We demonstrate that a minimax
robust reconstruction will generally be more stable than an approximation based
on interpolation through a nominal set of observations and that, subject to
some mild regularity conditions on the convex set of admissible observations,
the minimax robust reconstruction is unconditionally stable. We motivate our
results by characterizing the minimax robust reconstruction for several
specific convex observational models and discuss relationships with other
approaches to function approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0707.0340</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0707.0340</id><created>2007-07-03</created><updated>2012-04-16</updated><authors><author><keyname>Greenhill</keyname><forenames>Catherine</forenames></author><author><keyname>McKay</keyname><forenames>Brendan D.</forenames></author></authors><title>Asymptotic enumeration of sparse nonnegative integer matrices with
  specified row and column sums</title><categories>math.CO math.ST stat.TH</categories><comments>We fixes a small gap in the proof of Lemma 5.1 and made some other
  minor corrections. No theorem statements have changed</comments><msc-class>05A16 (Primary) 62H17 (Secondary)</msc-class><journal-ref>Advances in Applied Mathematics, 41 (2008) 459-481</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let \svec = (s_1,...,s_m) and \tvec = (t_1,...,t_n) be vectors of nonnegative
integer-valued functions of m,n with equal sum S = sum_{i=1}^m s_i =
sum_{j=1}^n t_j. Let M(\svec,\tvec) be the number of m*n matrices with
nonnegative integer entries such that the i-th row has row sum s_i and the j-th
column has column sum t_j for all i,j. Such matrices occur in many different
settings, an important example being the contingency tables (also called
frequency tables) important in statistics. Define s=max_i s_i and t=max_j t_j.
Previous work has established the asymptotic value of M(\svec,\tvec) as
m,n\to\infty with s and t bounded (various authors independently, 1971-1974),
and when \svec,\tvec are constant vectors with m/n,n/m,s/n &gt;= c/log n for
sufficiently large (Canfield and McKay, 2007). In this paper we extend the
sparse range to the case st=o(S^(2/3)). The proof in part follows a previous
asymptotic enumeration of 0-1 matrices under the same conditions (Greenhill,
McKay and Wang, 2006). We also generalise the enumeration to matrices over any
subset of the nonnegative integers that includes 0 and 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0707.0660</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0707.0660</id><created>2007-07-04</created><authors><author><keyname>Vovk</keyname><forenames>Vladimir</forenames></author></authors><title>Strong confidence intervals for autoregression</title><categories>math.ST stat.ME stat.TH</categories><comments>7 pages, 2 tables, 2 figures</comments><abstract>  In this short note I apply the methodology of game-theoretic probability to
calculating non-asymptotic confidence intervals for the coefficient of a simple
first order scalar autoregressive model. The most distinctive feature of the
proposed procedure is that with high probability it produces confidence
intervals that always cover the true parameter value when applied sequentially.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0707.0805</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0707.0805</id><created>2007-07-05</created><updated>2011-06-24</updated><authors><author><keyname>Chen</keyname><forenames>Xinjia</forenames></author></authors><title>A New Generalization of Chebyshev Inequality for Random Vectors</title><categories>math.ST cs.LG math.PR stat.AP stat.TH</categories><comments>7 pages, 1 figure; added some references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we derive a new generalization of Chebyshev inequality for
random vectors. We demonstrate that the new generalization is much less
conservative than the classical generalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0707.0861</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0707.0861</id><created>2007-07-05</created><authors><author><keyname>Langovoy</keyname><forenames>Mikhail</forenames></author></authors><title>Data-driven efficient score tests for deconvolution problems</title><categories>math.ST stat.AP stat.TH</categories><journal-ref>Inverse Problems 24 (2008) 025028 17pp</journal-ref><doi>10.1088/0266-5611/24/2/025028</doi><abstract>  We consider testing statistical hypotheses about densities of signals in
deconvolution models. A new approach to this problem is proposed. We
constructed score tests for the deconvolution with the known noise density and
efficient score tests for the case of unknown density. The tests are
incorporated with model selection rules to choose reasonable model dimensions
automatically by the data. Consistency of the tests is proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0707.0878</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0707.0878</id><created>2007-07-05</created><authors><author><keyname>Chen</keyname><forenames>Xinjia</forenames></author><author><keyname>Aravena</keyname><forenames>Jorge</forenames></author><author><keyname>Zhou</keyname><forenames>Kemin</forenames></author></authors><title>Risk Analysis in Robust Control -- Making the Case for Probabilistic
  Robust Control</title><categories>math.OC cs.SY math.ST stat.TH</categories><comments>22 pages, 2 figures</comments><journal-ref>Proceedings of American Control Conference, pp. 1533-1538,
  Portland, June 2005.</journal-ref><abstract>  This paper offers a critical view of the "worst-case" approach that is the
cornerstone of robust control design. It is our contention that a blind
acceptance of worst-case scenarios may lead to designs that are actually more
dangerous than designs based on probabilistic techniques with a built-in risk
factor. The real issue is one of modeling. If one accepts that no mathematical
model of uncertainties is perfect then a probabilistic approach can lead to
more reliable control even if it cannot guarantee stability for all possible
cases. Our presentation is based on case analysis. We first establish that
worst-case is not necessarily "all-encompassing." In fact, we show that for
some uncertain control problems to have a conventional robust control solution
it is necessary to make assumptions that leave out some feasible cases. Once we
establish that point, we argue that it is not uncommon for the risk of
unaccounted cases in worst-case design to be greater than that of the accepted
risk in a probabilistic approach. With an example, we quantify the risks and
show that worst-case can be significantly more risky. Finally, we join our
analysis with existing results on computational complexity and probabilistic
robustness to argue that the deterministic worst-case analysis is not
necessarily the better tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0707.2814</identifier>
 <datestamp>2011-04-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0707.2814</id><created>2007-07-19</created><updated>2011-04-10</updated><authors><author><keyname>Chen</keyname><forenames>Xinjia</forenames></author></authors><title>Coverage Probability of Random Intervals</title><categories>math.ST math.PR stat.ME stat.TH</categories><comments>21 pages, 2 figure, revised Theorem 7</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a general theory on the coverage probability of
random intervals defined in terms of discrete random variables with continuous
parameter spaces. The theory shows that the minimum coverage probabilities of
random intervals with respect to corresponding parameters are achieved at
discrete finite sets and that the coverage probabilities are continuous and
unimodal when parameters are varying in between interval endpoints. The theory
applies to common important discrete random variables including binomial
variable, Poisson variable, negative binomial variable and hypergeometrical
random variable. The theory can be used to make relevant statistical inference
more rigorous and less conservative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0707.2908</identifier>
 <datestamp>2012-01-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0707.2908</id><created>2007-07-19</created><updated>2012-01-04</updated><authors><author><keyname>Chambeu</keyname><forenames>Sbastien</forenames></author><author><keyname>Kurtzmann</keyname><forenames>Aline</forenames></author></authors><title>Some particular self-interacting diffusions: Ergodic behaviour and
  almost sure convergence</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ310 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ310</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 4, 1248-1267</journal-ref><doi>10.3150/10-BEJ310</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with some self-interacting diffusions $(X_t,t\geq 0)$ living
on $\mathbb{R}^d$. These diffusions are solutions to stochastic differential
equations: \[\mathrm{d}X_t=\mathrm{d}B_t-g(t)\nabla
V(X_t-\bar{\mu}_t)\,\mathrm{d}t,\] where $\bar{\mu}_t$ is the empirical mean of
the process $X$, $V$ is an asymptotically strictly convex potential and $g$ is
a given function. We study the ergodic behaviour of $X$ and prove that it is
strongly related to $g$. Actually, we show that $X$ is ergodic (in the limit
quotient sense) if and only if $\bar{\mu}_t$ converges a.s. We also give some
conditions (on $g$ and $V$) for the almost sure convergence of $X$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0707.3794</identifier>
 <datestamp>2010-03-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0707.3794</id><created>2007-07-25</created><authors><author><keyname>Drton</keyname><forenames>Mathias</forenames></author><author><keyname>Richardson</keyname><forenames>Thomas S.</forenames></author></authors><title>Binary Models for Marginal Independence</title><categories>math.ST stat.TH</categories><journal-ref>Journal of the Royal Statistical Society Series B 2008, Vol. 70,
  No. 2, 287-309</journal-ref><doi>10.1111/j.1467-9868.2007.00636.x</doi><abstract>  Log-linear models are a classical tool for the analysis of contingency
tables. In particular, the subclass of graphical log-linear models provides a
general framework for modelling conditional independences. However, with the
exception of special structures, marginal independence hypotheses cannot be
accommodated by these traditional models. Focusing on binary variables, we
present a model class that provides a framework for modelling marginal
independences in contingency tables. The approach taken is graphical and draws
on analogies to multivariate Gaussian models for marginal independence. For the
graphical model representation we use bi-directed graphs, which are in the
tradition of path diagrams. We show how the models can be parameterized in a
simple fashion, and how maximum likelihood estimation can be performed using a
version of the Iterated Conditional Fitting algorithm. Finally we consider
combining these models with symmetry restrictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0707.4643</identifier>
 <datestamp>2011-03-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0707.4643</id><created>2007-07-31</created><updated>2011-03-18</updated><authors><author><keyname>Duembgen</keyname><forenames>Lutz</forenames></author><author><keyname>Huesler</keyname><forenames>Andre</forenames></author><author><keyname>Rufibach</keyname><forenames>Kaspar</forenames></author></authors><title>Active Set and EM Algorithms for Log-Concave Densities Based on Complete
  and Censored Data</title><categories>stat.ME stat.CO</categories><comments>Changes in versions 3-4: Updated references, corrections of minor
  errors, shortened section on EM algorithm (which will be treated in more
  detail in a separate paper)</comments><report-no>Technical report 61, IMSV, University of Bern</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an active set algorithm for the maximum likelihood estimation of a
log-concave density based on complete data. Building on this fast algorithm, we
indidate an EM algorithm to treat arbitrarily censored or binned data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.0046</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.0046</id><created>2007-07-31</created><updated>2008-05-29</updated><authors><author><keyname>Brodie</keyname><forenames>Joshua</forenames></author><author><keyname>Daubechies</keyname><forenames>Ingrid</forenames></author><author><keyname>De Mol</keyname><forenames>Christine</forenames></author><author><keyname>Giannone</keyname><forenames>Domenico</forenames></author><author><keyname>Loris</keyname><forenames>Ignace</forenames></author></authors><title>Sparse and stable Markowitz portfolios</title><categories>q-fin.PM math.FA stat.AP</categories><comments>Better emphasis of main result, new abstract, new examples and
  figures. New appendix with full details of algorithm. 17 pages, 6 figures</comments><msc-class>62P20, 91B28</msc-class><doi>10.1073/pnas.0904287106</doi><abstract>  We consider the problem of portfolio selection within the classical Markowitz
mean-variance framework, reformulated as a constrained least-squares regression
problem. We propose to add to the objective function a penalty proportional to
the sum of the absolute values of the portfolio weights. This penalty
regularizes (stabilizes) the optimization problem, encourages sparse portfolios
(i.e. portfolios with only few active positions), and allows to account for
transaction costs. Our approach recovers as special cases the
no-short-positions portfolios, but does allow for short positions in limited
number. We implement this methodology on two benchmark data sets constructed by
Fama and French. Using only a modest amount of training data, we construct
portfolios whose out-of-sample performance, as measured by Sharpe ratio, is
consistently and significantly better than that of the naive evenly-weighted
portfolio which constitutes, as shown in recent literature, a very tough
benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.0079</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.0079</id><created>2007-08-01</created><authors><author><keyname>Hallin</keyname><forenames>Marc</forenames></author><author><keyname>Oja</keyname><forenames>Hannu</forenames></author><author><keyname>Paindaveine</keyname><forenames>Davy</forenames></author></authors><title>Semiparametrically efficient rank-based inference for shape II. Optimal
  R-estimation of shape</title><categories>math.ST stat.TH</categories><comments>Published at http://dx.doi.org/10.1214/009053606000000948 in the
  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS0215</report-no><msc-class>62M15, 62G35 (Primary)</msc-class><journal-ref>Annals of Statistics 2006, Vol. 34, No. 6, 2757-2789</journal-ref><doi>10.1214/009053606000000948</doi><abstract>  A class of R-estimators based on the concepts of multivariate signed ranks
and the optimal rank-based tests developed in Hallin and Paindaveine [Ann.
Statist. 34 (2006)] is proposed for the estimation of the shape matrix of an
elliptical distribution. These R-estimators are root-n consistent under any
radial density g, without any moment assumptions, and semiparametrically
efficient at some prespecified density f. When based on normal scores, they are
uniformly more efficient than the traditional normal-theory estimator based on
empirical covariance matrices (the asymptotic normality of which, moreover,
requires finite moments of order four), irrespective of the actual underlying
elliptical density. They rely on an original rank-based version of Le Cam's
one-step methodology which avoids the unpleasant nonparametric estimation of
cross-information quantities that is generally required in the context of
R-estimation. Although they are not strictly affine-equivariant, they are shown
to be equivariant in a weak asymptotic sense. Simulations confirm their
feasibility and excellent finite-sample performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.0083</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.0083</id><created>2007-08-01</created><authors><author><keyname>Koltchinskii</keyname><forenames>Vladimir</forenames></author></authors><title>2004 IMS Medallion Lecture: Local Rademacher complexities and oracle
  inequalities in risk minimization</title><categories>math.ST stat.TH</categories><comments>This paper discussed in: [arXiv:0708.0089], [arXiv:0708.0094],
  [arXiv:0708.0098], [arXiv:0708.0121], [arXiv:0708.0124], [arXiv:0708.0132].
  Rejoinder in [arXiv:0708.0135]. Published at
  http://dx.doi.org/10.1214/009053606000001019 in the Annals of Statistics
  (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics
  (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS0195</report-no><msc-class>62H30, 60B99, 68Q32 (Primary) 62G08, 68T05, 68T10 (Secondary)</msc-class><journal-ref>Annals of Statistics 2006, Vol. 34, No. 6, 2593-2656</journal-ref><doi>10.1214/009053606000001019</doi><abstract>  Let $\mathcal{F}$ be a class of measurable functions $f:S\mapsto [0,1]$
defined on a probability space $(S,\mathcal{A},P)$. Given a sample
(X_1,...,X_n) of i.i.d. random variables taking values in S with common
distribution P, let P_n denote the empirical measure based on (X_1,...,X_n). We
study an empirical risk minimization problem $P_nf\to \min$, $f\in
\mathcal{F}$. Given a solution $\hat{f}_n$ of this problem, the goal is to
obtain very general upper bounds on its excess risk
\[\mathcal{E}_P(\hat{f}_n):=P\hat{f}_n-\inf_{f\in \mathcal{F}}Pf,\] expressed
in terms of relevant geometric parameters of the class $\mathcal{F}$. Using
concentration inequalities and other empirical processes tools, we obtain both
distribution-dependent and data-dependent upper bounds on the excess risk that
are of asymptotically correct order in many examples. The bounds involve
localized sup-norms of empirical and Rademacher processes indexed by functions
from the class. We use these bounds to develop model selection techniques in
abstract risk minimization problems that can be applied to more specialized
frameworks of regression and classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.0094</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.0094</id><created>2007-08-01</created><authors><author><keyname>Blanchard</keyname><forenames>Gilles</forenames></author><author><keyname>Massart</keyname><forenames>Pascal</forenames></author></authors><title>Discussion of ``2004 IMS Medallion Lecture: Local Rademacher
  complexities and oracle inequalities in risk minimization'' by V.
  Koltchinskii</title><categories>math.ST stat.TH</categories><comments>Published at http://dx.doi.org/10.1214/009053606000001037 in the
  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS0195B</report-no><journal-ref>Annals of Statistics 2006, Vol. 34, No. 6, 2664-2671</journal-ref><doi>10.1214/009053606000001037</doi><abstract>  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and
oracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.0143</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.0143</id><created>2007-08-01</created><authors><author><keyname>Dahlhaus</keyname><forenames>Rainer</forenames></author><author><keyname>Polonik</keyname><forenames>Wolfgang</forenames></author></authors><title>Nonparametric quasi-maximum likelihood estimation for Gaussian locally
  stationary processes</title><categories>math.ST stat.TH</categories><comments>Published at http://dx.doi.org/10.1214/009053606000000867 in the
  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS0138</report-no><msc-class>62M10 (Primary) 62F30 (Secondary)</msc-class><journal-ref>Annals of Statistics 2006, Vol. 34, No. 6, 2790-2824</journal-ref><doi>10.1214/009053606000000867</doi><abstract>  This paper deals with nonparametric maximum likelihood estimation for
Gaussian locally stationary processes. Our nonparametric MLE is constructed by
minimizing a frequency domain likelihood over a class of functions. The
asymptotic behavior of the resulting estimator is studied. The results depend
on the richness of the class of functions. Both sieve estimation and global
estimation are considered. Our results apply, in particular, to estimation
under shape constraints. As an example, autoregressive model fitting with a
monotonic variance function is discussed in detail, including algorithmic
considerations. A key technical tool is the time-varying empirical spectral
process indexed by functions. For this process, a Bernstein-type exponential
inequality and a central limit theorem are derived. These results for empirical
spectral processes are of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.0165</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.0165</id><created>2007-08-01</created><authors><author><keyname>Boente</keyname><forenames>Graciela</forenames></author><author><keyname>He</keyname><forenames>Xuming</forenames></author><author><keyname>Zhou</keyname><forenames>Jianhui</forenames></author></authors><title>Robust estimates in generalized partially linear models</title><categories>stat.ME</categories><comments>Published at http://dx.doi.org/10.1214/009053606000000858 in the
  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS0136</report-no><msc-class>62F35 (Primary) 62G08 (Secondary)</msc-class><journal-ref>Annals of Statistics 2006, Vol. 34, No. 6, 2856-2878</journal-ref><doi>10.1214/009053606000000858</doi><abstract>  In this paper, we introduce a family of robust estimates for the parametric
and nonparametric components under a generalized partially linear model, where
the data are modeled by $y_i|(\mathbf{x}_i,t_i)\sim F(\cdot,\mu_i)$ with
$\mu_i=H(\eta(t_i)+\mathbf{x}_i^{$\mathrm{T}$}\beta)$, for some known
distribution function F and link function H. It is shown that the estimates of
$\beta$ are root-n consistent and asymptotically normal. Through a Monte Carlo
study, the performance of these estimators is compared with that of the
classical ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.0167</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.0167</id><created>2007-08-01</created><authors><author><keyname>Zuo</keyname><forenames>Yijun</forenames></author><author><keyname>He</keyname><forenames>Xuming</forenames></author></authors><title>On the limiting distributions of multivariate depth-based rank sum
  statistics and related tests</title><categories>math.ST stat.TH</categories><comments>Published at http://dx.doi.org/10.1214/009053606000000876 in the
  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS0140</report-no><msc-class>62G20 (Primary) 62G10, 62H10, 62H15 (Secondary)</msc-class><journal-ref>Annals of Statistics 2006, Vol. 34, No. 6, 2879-2896</journal-ref><doi>10.1214/009053606000000876</doi><abstract>  A depth-based rank sum statistic for multivariate data introduced by Liu and
Singh [J. Amer. Statist. Assoc. 88 (1993) 252--260] as an extension of the
Wilcoxon rank sum statistic for univariate data has been used in multivariate
rank tests in quality control and in experimental studies. Those applications,
however, are based on a conjectured limiting distribution, provided by Liu and
Singh [J. Amer. Statist. Assoc. 88 (1993) 252--260]. The present paper proves
the conjecture under general regularity conditions and, therefore, validates
various applications of the rank sum statistic in the literature. The paper
also shows that the corresponding rank sum tests can be more powerful than
Hotelling's T^2 test and some commonly used multivariate rank tests in
detecting location-scale changes in multivariate distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.0185</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.0185</id><created>2007-08-01</created><authors><author><keyname>Chen</keyname><forenames>Willa W.</forenames></author><author><keyname>Hurvich</keyname><forenames>Clifford M.</forenames></author></authors><title>Semiparametric estimation of fractional cointegrating subspaces</title><categories>math.ST stat.TH</categories><comments>Published at http://dx.doi.org/10.1214/009053606000000894 in the
  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS0144</report-no><msc-class>62M10 (Primary) 62M15. (Secondary)</msc-class><journal-ref>Annals of Statistics 2006, Vol. 34, No. 6, 2939-2979</journal-ref><doi>10.1214/009053606000000894</doi><abstract>  We consider a common-components model for multivariate fractional
cointegration, in which the $s\geq1$ components have different memory
parameters. The cointegrating rank may exceed 1. We decompose the true
cointegrating vectors into orthogonal fractional cointegrating subspaces such
that vectors from distinct subspaces yield cointegrating errors with distinct
memory parameters. We estimate each cointegrating subspace separately, using
appropriate sets of eigenvectors of an averaged periodogram matrix of tapered,
differenced observations, based on the first $m$ Fourier frequencies, with $m$
fixed. The angle between the true and estimated cointegrating subspaces is
$o_p(1)$. We use the cointegrating residuals corresponding to an estimated
cointegrating vector to obtain a consistent and asymptotically normal estimate
of the memory parameter for the given cointegrating subspace, using a
univariate Gaussian semiparametric estimator with a bandwidth that tends to
$\infty$ more slowly than $n$. We use these estimates to test for fractional
cointegration and to consistently identify the cointegrating subspaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.0197</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.0197</id><created>2007-08-01</created><authors><author><keyname>Nordman</keyname><forenames>Daniel J.</forenames></author><author><keyname>Lahiri</keyname><forenames>Soumendra N.</forenames></author></authors><title>A frequency domain empirical likelihood for short- and long-range
  dependence</title><categories>math.ST stat.TH</categories><comments>Published at http://dx.doi.org/10.1214/009053606000000902 in the
  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS0146</report-no><msc-class>62F40, 62G09 (Primary) 62G20 (Secondary)</msc-class><journal-ref>Annals of Statistics 2006, Vol. 34, No. 6, 3019-3050</journal-ref><doi>10.1214/009053606000000902</doi><abstract>  This paper introduces a version of empirical likelihood based on the
periodogram and spectral estimating equations. This formulation handles
dependent data through a data transformation (i.e., a Fourier transform) and is
developed in terms of the spectral distribution rather than a time domain
probability distribution. The asymptotic properties of frequency domain
empirical likelihood are studied for linear time processes exhibiting both
short- and long-range dependence. The method results in likelihood ratios which
can be used to build nonparametric, asymptotically correct confidence regions
for a class of normalized (or ratio) spectral parameters, including
autocorrelations. Maximum empirical likelihood estimators are possible, as well
as tests of spectral moment conditions. The methodology can be applied to
several inference problems such as Whittle estimation and goodness-of-fit
testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.0614</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.0614</id><created>2007-08-04</created><updated>2007-08-17</updated><authors><author><keyname>James</keyname><forenames>Lancelot F.</forenames></author></authors><title>New Dirichlet Mean Identities</title><categories>math.PR math.ST stat.TH</categories><comments>Section 5 has been added</comments><abstract>  An important line of research is the investigation of the laws of random
variables known as Dirichlet means as discussed in Cifarelli and
Regazzini(1990). However there is not much information on inter-relationships
between different Dirichlet means. Here we introduce two distributional
operations, which consist of multiplying a mean functional by an independent
beta random variable and an operation involving an exponential change of
measure. These operations identify relationships between different means and
their densities. This allows one to use the often considerable analytic work to
obtain results for one Dirichlet mean to obtain results for an entire family of
otherwise seemingly unrelated Dirichlet means. Additionally, it allows one to
obtain explicit densities for the related class of random variables that have
generalized gamma convolution distributions, and the finite-dimensional
distribution of their associated L\'evy processes. This has implications in,
for instance, the explicit description of Bayesian nonparametric prior and
posterior models, and more generally in a variety of applications in
probability and statistics involving Levy processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.0711</identifier>
 <datestamp>2010-10-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.0711</id><created>2007-08-06</created><authors><author><keyname>Douc</keyname><forenames>R.</forenames></author><author><keyname>Guillin</keyname><forenames>A.</forenames></author><author><keyname>Marin</keyname><forenames>J. -M.</forenames></author><author><keyname>Robert</keyname><forenames>C. P.</forenames></author></authors><title>Convergence of adaptive mixtures of importance sampling schemes</title><categories>math.ST stat.CO stat.TH</categories><comments>Published at http://dx.doi.org/10.1214/009053606000001154 in the
  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS0211</report-no><msc-class>60F05, 62L12, 65-04, 65C05, 65C40, 65C60 (Primary)</msc-class><journal-ref>Annals of Statistics (2007), Vol. 35, No. 1, 420-448</journal-ref><doi>10.1214/009053606000001154</doi><abstract>  In the design of efficient simulation algorithms, one is often beset with a
poor choice of proposal distributions. Although the performance of a given
simulation kernel can clarify a posteriori how adequate this kernel is for the
problem at hand, a permanent on-line modification of kernels causes concerns
about the validity of the resulting algorithm. While the issue is most often
intractable for MCMC algorithms, the equivalent version for importance sampling
algorithms can be validated quite precisely. We derive sufficient convergence
conditions for adaptive mixtures of population Monte Carlo algorithms and show
that Rao--Blackwellized versions asymptotically achieve an optimum in terms of
a Kullback divergence criterion, while more rudimentary versions do not benefit
from repeated updating.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.1580</identifier>
 <datestamp>2010-08-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.1580</id><created>2007-08-11</created><updated>2010-08-19</updated><authors><author><keyname>Still</keyname><forenames>Susanne</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author><author><keyname>Ellison</keyname><forenames>Christopher J.</forenames></author></authors><title>Optimal Causal Inference: Estimating Stored Information and
  Approximating Causal Architecture</title><categories>cs.IT cond-mat.stat-mech cs.LG math.IT math.ST stat.TH</categories><comments>14 pages, 13 figures;
  http://cse.ucdavis.edu/~cmg/compmech/pubs/oci.htm; Updated figures and
  citations; added corrections and clarifications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an approach to inferring the causal architecture of stochastic
dynamical systems that extends rate distortion theory to use causal
shielding---a natural principle of learning. We study two distinct cases of
causal inference: optimal causal filtering and optimal causal estimation.
  Filtering corresponds to the ideal case in which the probability distribution
of measurement sequences is known, giving a principled method to approximate a
system's causal structure at a desired level of representation. We show that,
in the limit in which a model complexity constraint is relaxed, filtering finds
the exact causal architecture of a stochastic dynamical system, known as the
causal-state partition. From this, one can estimate the amount of historical
information the process stores. More generally, causal filtering finds a graded
model-complexity hierarchy of approximations to the causal architecture. Abrupt
changes in the hierarchy, as a function of approximation, capture distinct
scales of structural organization.
  For nonideal cases with finite data, we show how the correct number of
underlying causal states can be found by optimal causal estimation. A
previously derived model complexity control term allows us to correct for the
effect of statistical fluctuations in probability estimates and thereby avoid
over-fitting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.1627</identifier>
 <datestamp>2013-06-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.1627</id><created>2007-08-12</created><updated>2013-05-30</updated><authors><author><keyname>Chernozhukov</keyname><forenames>Victor</forenames></author><author><keyname>Fernandez-Val</keyname><forenames>Ivan</forenames></author><author><keyname>Galichon</keyname><forenames>Alfred</forenames></author></authors><title>Rearranging Edgeworth-Cornish-Fisher Expansions</title><categories>stat.ME</categories><comments>17 pages, 3 figures</comments><journal-ref>Economic Theory February 2010, Volume 42, Issue 2, pp 419-435</journal-ref><doi>10.1007/s00199-008-0431-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper applies a regularization procedure called increasing rearrangement
to monotonize Edgeworth and Cornish-Fisher expansions and any other related
approximations of distribution and quantile functions of sample statistics.
Besides satisfying the logical monotonicity, required of distribution and
quantile functions, the procedure often delivers strikingly better
approximations to the distribution and quantile functions of the sample mean
than the original Edgeworth-Cornish-Fisher expansions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.1866</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.1866</id><created>2007-08-14</created><updated>2007-09-07</updated><authors><author><keyname>Lopez-Ruiz</keyname><forenames>Ricardo</forenames></author><author><keyname>Sanudo</keyname><forenames>Jaime</forenames></author><author><keyname>Calbet</keyname><forenames>Xavier</forenames></author></authors><title>On the equivalence of the microcanonical and the canonical ensembles: a
  geometrical approach</title><categories>nlin.CD cond-mat.stat-mech stat.ME</categories><comments>9 pages, 0 figures</comments><abstract>  In this paper, we consider the volume enclosed by the microcanonical ensemble
in phase space as a statistical ensemble. This can be interpreted as an
intermediate image between the microcanonical and the canonical pictures. By
maintaining the ergodic hypothesis over this ensemble, that is, the
equiprobability of all its accessible states, the equivalence of this ensemble
in the thermodynamic limit with the microcanonical and the canonical ensembles
is suggested by means of geometrical arguments. The Maxwellian and the
Boltzmann-Gibbs distributions are obtained from this formalism. In the
appendix, the derivation of the Boltzmann factor from a new microcanonical
image of the canonical ensemble is also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.1874</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.1874</id><created>2007-08-14</created><authors><author><keyname>Schennach</keyname><forenames>Susanne M.</forenames></author></authors><title>Point estimation with exponentially tilted empirical likelihood</title><categories>math.ST q-fin.ST stat.TH</categories><comments>Published at http://dx.doi.org/10.1214/009053606000001208 in the
  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS0176</report-no><msc-class>62F10 (Primary); 62F12 (Secondary)</msc-class><journal-ref>Annals of Statistics 2007, Vol. 35, No. 2, 634-672</journal-ref><doi>10.1214/009053606000001208</doi><abstract>  Parameters defined via general estimating equations (GEE) can be estimated by
maximizing the empirical likelihood (EL). Newey and Smith [Econometrica 72
(2004) 219--255] have recently shown that this EL estimator exhibits desirable
higher-order asymptotic properties, namely, that its $O(n^{-1})$ bias is small
and that bias-corrected EL is higher-order efficient. Although EL possesses
these properties when the model is correctly specified, this paper shows that,
in the presence of model misspecification, EL may cease to be root n convergent
when the functions defining the moment conditions are unbounded (even when
their expectations are bounded). In contrast, the related exponential tilting
(ET) estimator avoids this problem. This paper shows that the ET and EL
estimators can be naturally combined to yield an estimator called exponentially
tilted empirical likelihood (ETEL) exhibiting the same $O(n^{-1})$ bias and the
same $O(n^{-2})$ variance as EL, while maintaining root n convergence under
model misspecification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.3556</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.3556</id><created>2007-08-27</created><authors><author><keyname>Shen</keyname><forenames>Xiaotong</forenames></author><author><keyname>Wang</keyname><forenames>Lifeng</forenames></author></authors><title>Generalization error for multi-class margin classification</title><categories>math.ST stat.TH</categories><comments>Published at http://dx.doi.org/10.1214/07-EJS069 in the Electronic
  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-EJS-EJS_2007_69</report-no><msc-class>68T10, 62H30 (Primary)</msc-class><journal-ref>Electronic Journal of Statistics 2007, Vol. 1, 307-330</journal-ref><doi>10.1214/07-EJS069</doi><abstract>  In this article, we study rates of convergence of the generalization error of
multi-class margin classifiers. In particular, we develop an upper bound theory
quantifying the generalization error of various large margin classifiers. The
theory permits a treatment of general margin losses, convex or nonconvex, in
presence or absence of a dominating class. Three main results are established.
First, for any fixed margin loss, there may be a trade-off between the ideal
and actual generalization performances with respect to the choice of the class
of candidate decision functions, which is governed by the trade-off between the
approximation and estimation errors. In fact, different margin losses lead to
different ideal or actual performances in specific cases. Second, we
demonstrate, in a problem of linear learning, that the convergence rate can be
arbitrarily fast in the sample size $n$ depending on the joint distribution of
the input/output pair. This goes beyond the anticipated rate $O(n^{-1})$.
Third, we establish rates of convergence of several margin classifiers in
feature selection with the number of candidate variables $p$ allowed to greatly
exceed the sample size $n$ but no faster than $\exp(n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.4104</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.4104</id><created>2007-08-30</created><authors><author><keyname>Chesneau</keyname><forenames>Christophe</forenames></author></authors><title>Wavelet block thresholding for samples with random design: a minimax
  approach under the $L^p$ risk</title><categories>math.ST stat.TH</categories><comments>Published at http://dx.doi.org/10.1214/07-EJS067 in the Electronic
  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-EJS-EJS_2007_67</report-no><msc-class>62G07, 60K35 (Primary); 62G20 (Secondary)</msc-class><journal-ref>Electronic Journal of Statistics 2007, Vol. 1, 331-346</journal-ref><doi>10.1214/07-EJS067</doi><abstract>  We consider the regression model with (known) random design. We investigate
the minimax performances of an adaptive wavelet block thresholding estimator
under the $\mathbb{L}^p$ risk with $p\ge 2$ over Besov balls. We prove that it
is near optimal and that it achieves better rates of convergence than the
conventional term-by-term estimators (hard, soft,...).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0708.4177</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0708.4177</id><created>2007-08-30</created><authors><author><keyname>Puig</keyname><forenames>Pedro</forenames></author><author><keyname>Valero</keyname><forenames>Jordi</forenames></author></authors><title>Characterization of count data distributions involving additivity and
  binomial subsampling</title><categories>math.ST stat.TH</categories><comments>Published at http://dx.doi.org/10.3150/07-BEJ6021 in the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ6021</report-no><journal-ref>Bernoulli 2007, Vol. 13, No. 2, 544-555</journal-ref><doi>10.3150/07-BEJ6021</doi><abstract>  In this paper we characterize all the $r$-parameter families of count
distributions (satisfying mild conditions) that are closed under addition and
under binomial subsampling. Surprisingly, few families satisfy both properties
and the resulting models consist of the $r$th-order univariate Hermite
distributions. Among these, we find the Poisson ($r=1$) and the ordinary
Hermite distributions ($r=2$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0709.1616</identifier>
 <datestamp>2011-11-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0709.1616</id><created>2007-09-11</created><updated>2011-11-23</updated><authors><author><keyname>Wang</keyname><forenames>Bin</forenames></author><author><keyname>Wang</keyname><forenames>Xiaofeng</forenames></author></authors><title>Bandwidth Selection for Weighted Kernel Density Estimation</title><categories>stat.ME</categories><comments>Will be rewritten for resubmission</comments><proxy>vtex</proxy><report-no>IMS-EJS-EJS_2007_112</report-no><abstract>  In the this paper, the authors propose to estimate the density of a targeted
population with a weighted kernel density estimator (wKDE) based on a weighted
sample. Bandwidth selection for wKDE is discussed. Three mean integrated
squared error based bandwidth estimators are introduced and their performance
is illustrated via Monte Carlo simulation. The least-squares cross-validation
method and the adaptive weight kernel density estimator are also studied. The
authors also consider the boundary problem for interval bounded data and apply
the new method to a real data set subject to informative censoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0709.1801</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0709.1801</id><created>2007-09-12</created><updated>2010-01-13</updated><authors><author><keyname>Dereudre</keyname><forenames>David</forenames></author><author><keyname>Lavancier</keyname><forenames>Frdric</forenames></author></authors><title>Campbell equilibrium equation and pseudo-likelihood estimation for
  non-hereditary Gibbs point processes</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ198 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><report-no>IMS-BEJ-BEJ198</report-no><journal-ref>Bernoulli 2009, Vol. 15, No. 4, 1368-1396</journal-ref><doi>10.3150/09-BEJ198</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study Gibbs point processes involving a hardcore
interaction which is not necessarily hereditary. We first extend the famous
Campbell equilibrium equation, initially proposed by Nguyen and Zessin [Math.
Nachr. 88 (1979) 105--115], to the non-hereditary setting and consequently
introduce the new concept of removable points. A modified version of the
pseudo-likelihood estimator is then proposed, which involves these removable
points. We consider the following two-step estimation procedure: first estimate
the hardcore parameter, then estimate the smooth interaction parameter by
pseudo-likelihood, where the hardcore parameter estimator is plugged in. We
prove the consistency of this procedure in both the hereditary and
non-hereditary settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0709.3526</identifier>
 <datestamp>2012-07-31</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0709.3526</id><created>2007-09-21</created><updated>2012-07-30</updated><authors><author><keyname>Nardi</keyname><forenames>Yuval</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author></authors><title>The log-linear group-lasso estimator and its asymptotic properties</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/11-BEJ364 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ364</report-no><journal-ref>Bernoulli 2012, Vol. 18, No. 3, 945-974</journal-ref><doi>10.3150/11-BEJ364</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define the group-lasso estimator for the natural parameters of the
exponential families of distributions representing hierarchical log-linear
models under multinomial sampling scheme. Such estimator arises as the solution
of a convex penalized likelihood optimization problem based on the group-lasso
penalty. We illustrate how it is possible to construct an estimator of the
underlying log-linear model using the blocks of nonzero coefficients recovered
by the group-lasso procedure. We investigate the asymptotic properties of the
group-lasso estimator as a model selection method in a double-asymptotic
framework, in which both the sample size and the model complexity grow
simultaneously. We provide conditions guaranteeing that the group-lasso
estimator is model selection consistent, in the sense that, with overwhelming
probability as the sample size increases, it correctly identifies all the sets
of nonzero interactions among the variables. Provided the sequences of true
underlying models is sparse enough, recovery is possible even if the number of
cells grows larger than the sample size. Finally, we derive some central limit
type of results for the log-linear group-lasso estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0709.4316</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0709.4316</id><created>2007-09-27</created><updated>2007-11-12</updated><authors><author><keyname>Farchione</keyname><forenames>David</forenames></author><author><keyname>Kabaila</keyname><forenames>Paul</forenames></author></authors><title>Confidence intervals for the normal mean utilizing prior information</title><categories>math.ST stat.TH</categories><comments>The paper has been accepted for publication in Statistics and
  Probability Letters</comments><proxy>vtex</proxy><msc-class>62F25 (Primary)</msc-class><journal-ref>Statistics and Probability Letters, 78, 1094-1100 (2008)</journal-ref><abstract>  Consider X_1,X_2,...,X_n that are independent and identically N(mu,sigma^2)
distributed. Suppose that we have uncertain prior information that mu = 0. We
answer the question: to what extent can a frequentist 1-alpha confidence
interval for mu utilize this prior information?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0710.0178</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0710.0178</id><created>2007-09-30</created><updated>2007-11-16</updated><authors><author><keyname>Brettschneider</keyname><forenames>Julia</forenames></author><author><keyname>Collin</keyname><forenames>Francois</forenames></author><author><keyname>Bolstad</keyname><forenames>Benjamin M.</forenames></author><author><keyname>Speed</keyname><forenames>Terence P.</forenames></author></authors><title>Quality assessment for short oligonucleotide microarray data</title><categories>stat.ME stat.AP</categories><comments>32 pages plus 12 figure pages (17 figures total), correction of
  typos, conversion of some figures into color</comments><abstract>  Quality of microarray gene expression data has emerged as a new research
topic. As in other areas, microarray quality is assessed by comparing suitable
numerical summaries across microarrays, so that outliers and trends can be
visualized, and poor quality arrays or variable quality sets of arrays can be
identified. Since each single array comprises tens or hundreds of thousands of
measurements, the challenge is to find numerical summaries which can be used to
make accurate quality calls. To this end, several new quality measures are
introduced based on probe level and probeset level information, all obtained as
a by-product of the low-level analysis algorithms RMA/fitPLM for Affymetrix
GeneChips. Quality landscapes spatially localize chip or hybridization
problems. Numerical chip quality measures are derived from the distributions of
Normalized Unscaled Standard Errors and of Relative Log Expressions. Quality of
chip batches is assessed by Residual Scale Factors. These quality assessment
measures are demonstrated on a variety of datasets (spike-in experiments, small
lab experiments, multi-site studies). They are compared with Affymetrix's
individual chip quality report.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0710.0262</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0710.0262</id><created>2007-10-01</created><updated>2007-11-01</updated><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Roch</keyname><forenames>Sebastien</forenames></author></authors><title>Incomplete Lineage Sorting: Consistent Phylogeny Estimation From
  Multiple Loci</title><categories>q-bio.PE cs.CE cs.DS math.PR math.ST stat.TH</categories><comments>Added a section on more general distance-based methods</comments><abstract>  We introduce a simple algorithm for reconstructing phylogenies from multiple
gene trees in the presence of incomplete lineage sorting, that is, when the
topology of the gene trees may differ from that of the species tree. We show
that our technique is statistically consistent under standard stochastic
assumptions, that is, it returns the correct tree given sufficiently many
unlinked loci. We also show that it can tolerate moderate estimation errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0710.2183</identifier>
 <datestamp>2011-03-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0710.2183</id><created>2007-10-11</created><updated>2008-02-08</updated><authors><author><keyname>Tanaka</keyname><forenames>Kentaro</forenames></author></authors><title>Strong consistency of the maximum likelihood estimator for finite
  mixtures of location-scale distributions when penalty is imposed on the
  ratios of the scale parameters</title><categories>math.ST math.PR stat.TH</categories><comments>29 pages, 2 figures</comments><msc-class>62F12</msc-class><journal-ref>Scandinavian Journal of Statistics, 36, (2009), 171-184</journal-ref><doi>10.1111/j.1467-9469.2008.00615.x</doi><abstract>  In finite mixtures of location-scale distributions, if there is no constraint
or penalty on the parameters, then the maximum likelihood estimator does not
exist because the likelihood is unbounded. To avoid this problem, we consider a
penalized likelihood, where the penalty is a function of the minimum of the
ratios of the scale parameters and the sample size. It is shown that the
penalized maximum likelihood estimator is strongly consistent. We also analyze
the consistency of a penalized maximum likelihood estimator where the penalty
is imposed on the scale parameters themselves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0710.2963</identifier>
 <datestamp>2010-06-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0710.2963</id><created>2007-10-16</created><updated>2008-03-03</updated><authors><author><keyname>Reegen</keyname><forenames>P.</forenames></author><author><keyname>Gruberbauer</keyname><forenames>M.</forenames></author><author><keyname>Schneider</keyname><forenames>L.</forenames></author><author><keyname>Weiss</keyname><forenames>W. W.</forenames></author></authors><title>Cinderella - Comparison of INDEpendent RELative Least-squares Amplitudes</title><categories>astro-ph stat.ME</categories><comments>8 pages, 6 figures, A&amp;A, in press</comments><abstract>  The identification of increasingly smaller signal from objects observed with
a non-perfect instrument in a noisy environment poses a challenge for a
statistically clean data analysis. We want to compute the probability of
frequencies determined in various data sets to be related or not, which cannot
be answered with a simple comparison of amplitudes. Our method provides a
statistical estimator for a given signal with different strengths in a set of
observations to be of instrumental origin or to be intrinsic. Based on the
spectral significance as an unbiased statistical quantity in frequency
analysis, Discrete Fourier Transforms (DFTs) of target and background light
curves are comparatively examined. The individual False-Alarm Probabilities are
used to deduce conditional probabilities for a peak in a target spectrum to be
real in spite of a corresponding peak in the spectrum of a background or of
comparison stars. Alternatively, we can compute joint probabilities of
frequencies to occur in the DFT spectra of several data sets simultaneously but
with different amplitude, which leads to composed spectral significances. These
are useful to investigate a star observed in different filters or during
several observing runs. The composed spectral significance is a measure for the
probability that none of coinciding peaks in the DFT spectra under
consideration are due to noise. Cinderella is a mathematical approach to a
general statistical problem. Its potential reaches beyond photometry from
ground or space: to all cases where a quantitative statistical comparison of
periodicities in different data sets is desired. Examples for the composed and
the conditional Cinderella mode for different observation setups are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0710.3183</identifier>
 <datestamp>2010-03-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0710.3183</id><created>2007-10-16</created><authors><author><keyname>Predd</keyname><forenames>Joel</forenames></author><author><keyname>Seiringer</keyname><forenames>Robert</forenames></author><author><keyname>Lieb</keyname><forenames>Elliott H.</forenames></author><author><keyname>Osherson</keyname><forenames>Daniel</forenames></author><author><keyname>Poor</keyname><forenames>Vincent</forenames></author><author><keyname>Kulkarni</keyname><forenames>Sanjeev</forenames></author></authors><title>Probabilistic coherence and proper scoring rules</title><categories>stat.ML</categories><comments>LaTeX2, 15 pages</comments><journal-ref>IEEE T. Inform. Theory 55, 4786 (2009)</journal-ref><abstract>  We provide self-contained proof of a theorem relating probabilistic coherence
of forecasts to their non-domination by rival forecasts with respect to any
proper scoring rule. The theorem appears to be new but is closely related to
results achieved by other investigators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0710.3473</identifier>
 <datestamp>2012-01-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0710.3473</id><created>2007-10-18</created><updated>2012-01-26</updated><authors><author><keyname>Lee</keyname><forenames>Duncan</forenames></author><author><keyname>Shaddick</keyname><forenames>Gavin</forenames></author></authors><title>Modelling the effects of air pollution on health using Bayesian Dynamic
  Generalised Linear Models</title><categories>stat.AP stat.ME</categories><comments>Accepted for publication in Environmetrics, October 2nd 2007</comments><abstract>  The relationship between short-term exposure to air pollution and mortality
or morbidity has been the subject of much recent research, in which the
standard method of analysis uses Poisson linear or additive models. In this
paper we use a Bayesian dynamic generalised linear model (DGLM) to estimate
this relationship, which allows the standard linear or additive model to be
extended in two ways: (i) the long-term trend and temporal correlation present
in the health data can be modelled by an autoregressive process rather than a
smooth function of calendar time; (ii) the effects of air pollution are allowed
to evolve over time. The efficacy of these two extensions are investigated by
applying a series of dynamic and non-dynamic models to air pollution and
mortality data from Greater London. A Bayesian approach is taken throughout,
and a Markov chain monte carlo simulation algorithm is presented for inference.
An alternative likelihood based analysis is also presented, in order to allow a
direct comparison with the only previous analysis of air pollution and health
data using a DGLM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0710.5218</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0710.5218</id><created>2007-10-27</created><authors><author><keyname>Berlinet</keyname><forenames>Alain</forenames><affiliation>I3M</affiliation></author><author><keyname>Elamine</keyname><forenames>Abdallah</forenames><affiliation>I3M</affiliation></author><author><keyname>Mas</keyname><forenames>Andr</forenames><affiliation>I3M</affiliation></author></authors><title>Local linear regression for functional data</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00182746</proxy><journal-ref>Annals of the Institute of Statistical Mathematics (2011) 63,
  1047-1075</journal-ref><abstract>  We study a non linear regression model with functional data as inputs and
scalar response. We propose a pointwise estimate of the regression function
that maps a Hilbert space onto the real line by a local linear method. We
provide the asymptotic mean square error. Computations involve a linear inverse
problem as well as a representation of the small ball probability of the data
and are based on recent advances in this area. The rate of convergence of our
estimate outperforms those already obtained in the literature on this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0710.5270</identifier>
 <datestamp>2011-04-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0710.5270</id><created>2007-10-28</created><updated>2011-04-20</updated><authors><author><keyname>Kimiagar</keyname><forenames>S.</forenames></author><author><keyname>Movahed</keyname><forenames>M. Sadegh</forenames></author><author><keyname>Khorram</keyname><forenames>S.</forenames></author><author><keyname>Tabar</keyname><forenames>M. Reza Rahimi</forenames></author></authors><title>Markov Properties of Electrical Discharge Current Fluctuations in Plasma</title><categories>cond-mat.stat-mech math.ST stat.TH</categories><comments>25 pages, 9 figures and 4 tables. V3: Added comments, references,
  figures and major corrections</comments><journal-ref>J Stat Phys (2011) 143: 148-167</journal-ref><doi>10.1007/s10955-011-0171-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the Markovian method, we study the stochastic nature of electrical
discharge current fluctuations in the Helium plasma. Sinusoidal trends are
extracted from the data set by the Fourier-Detrended Fluctuation analysis and
consequently cleaned data is retrieved. We determine the Markov time scale of
the detrended data set by using likelihood analysis. We also estimate the
Kramers-Moyal's coefficients of the discharge current fluctuations and derive
the corresponding Fokker-Planck equation. In addition, the obtained Langevin
equation enables us to reconstruct discharge time series with similar
statistical properties compared with the observed in the experiment. We also
provide an exact decomposition of temporal correlation function by using
Kramers-Moyal's coefficients. We show that for the stationary time series, the
two point temporal correlation function has an exponential decaying behavior
with a characteristic correlation time scale. Our results confirm that, there
is no definite relation between correlation and Markov time scales. However
both of them behave as monotonic increasing function of discharge current
intensity. Finally to complete our analysis, the multifractal behavior of
reconstructed time series using its Keramers-Moyal's coefficients and original
data set are investigated. Extended self similarity analysis demonstrates that
fluctuations in our experimental setup deviates from Kolmogorov (K41) theory
for fully developed turbulence regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0710.5874</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0710.5874</id><created>2007-10-31</created><authors><author><keyname>Didelez</keyname><forenames>Vanessa</forenames></author></authors><title>Graphical models for marked point processes based on local independence</title><categories>math.ST stat.TH</categories><comments>To appear in the Journal of the Royal Statistical Society Series B</comments><doi>10.1111/j.1467-9868.2007.00634.x</doi><abstract>  A new class of graphical models capturing the dependence structure of events
that occur in time is proposed. The graphs represent so-called local
independences, meaning that the intensities of certain types of events are
independent of some (but not necessarily all) events in the past. This dynamic
concept of independence is asymmetric, similar to Granger non-causality, so
that the corresponding local independence graphs differ considerably from
classical graphical models. Hence a new notion of graph separation, called
delta-separation, is introduced and implications for the underlying model as
well as for likelihood inference are explored. Benefits regarding facilitation
of reasoning about and understanding of dynamic dependencies as well as
computational simplifications are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0710.5935</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0710.5935</id><created>2007-10-31</created><authors><author><keyname>Pollak</keyname><forenames>Moshe</forenames><affiliation>The Hebrew University of Jerusalem</affiliation></author><author><keyname>Tartakovsky</keyname><forenames>Alexander G.</forenames><affiliation>University of Southern California</affiliation></author></authors><title>On Optimality Properties of the Shiryaev-Roberts Procedure</title><categories>math.ST math.PR stat.TH</categories><comments>12</comments><msc-class>62L10; 62L15</msc-class><journal-ref>Statistica Sinica, vol. 19, pp. 1729-1739, 2009</journal-ref><abstract>  We consider the simple changepoint problem setting, where observations are
independent, iid pre-change and iid post-change, with known pre- and
post-change distributions. The Shiryaev-Roberts detection procedure is known to
be asymptotically minimax in the sense of minimizing maximal expected detection
delay subject to a bound on the average run length to false alarm, as the
latter goes to infinity. Here we present other optimality properties of the
Shiryaev-Roberts procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0711.0198</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0711.0198</id><created>2007-11-01</created><authors><author><keyname>von Luxburg</keyname><forenames>Ulrike</forenames></author><author><keyname>Franz</keyname><forenames>Volker H.</forenames></author></authors><title>A Geometric Approach to Confidence Sets for Ratios: Fieller's Theorem,
  Generalizations, and Bootstrap</title><categories>stat.ME math.ST stat.TH</categories><journal-ref>Statistica Sinica, 19(3), 1095-1117. (2009)</journal-ref><abstract>  We present a geometric method to determine confidence sets for the ratio
E(Y)/E(X) of the means of random variables X and Y. This method reduces the
problem of constructing confidence sets for the ratio of two random variables
to the problem of constructing confidence sets for the means of one-dimensional
random variables. It is valid in a large variety of circumstances. In the case
of normally distributed random variables, the so constructed confidence sets
coincide with the standard Fieller confidence sets. Generalizations of our
construction lead to definitions of exact and conservative confidence sets for
very general classes of distributions, provided the joint expectation of (X,Y)
exists and the linear combinations of the form aX + bY are well-behaved.
Finally, our geometric method allows to derive a very simple bootstrap approach
for constructing conservative confidence sets for ratios which perform
favorably in certain situations, in particular in the asymmetric heavy-tailed
regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0711.0501</identifier>
 <datestamp>2010-07-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0711.0501</id><created>2007-11-04</created><updated>2010-07-01</updated><authors><author><keyname>Chatterjee</keyname><forenames>Sourav</forenames></author></authors><title>A new approach to strong embeddings</title><categories>math.PR math.ST stat.TH</categories><comments>A new example has been added. To appear in PTRF. 31 pages</comments><msc-class>60F17, 60F99, 60G50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit strong approximation theory from a new perspective, culminating in
a proof of the Koml\'os-Major-Tusn\'ady embedding theorem for the simple random
walk. The proof is almost entirely based on a series of soft arguments and easy
inequalities. The new technique, inspired by Stein's method of normal
approximation, is applicable to any setting where Stein's method works. In
particular, one can hope to take it beyond sums of independent random
variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0711.0993</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0711.0993</id><created>2007-11-06</created><authors><author><keyname>Kabaila</keyname><forenames>Paul</forenames></author><author><keyname>Giri</keyname><forenames>Khageswor</forenames></author></authors><title>Upper bounds on the minimum coverage probability of confidence intervals
  in regression after variable selection</title><categories>math.ST stat.AP stat.TH</categories><journal-ref>Australian &amp; New Zealand Journal of Statistics, 51, 271-288 (2009)</journal-ref><abstract>  We consider a linear regression model, with the parameter of interest a
specified linear combination of the regression parameter vector. We suppose
that, as a first step, a data-based model selection (e.g. by preliminary
hypothesis tests or minimizing AIC) is used to select a model. It is common
statistical practice to then construct a confidence interval for the parameter
of interest based on the assumption that the selected model had been given to
us a priori. This assumption is false and it can lead to a confidence interval
with poor coverage properties. We provide an easily-computed finite sample
upper bound (calculated by repeated numerical evaluation of a double integral)
to the minimum coverage probability of this confidence interval. This bound
applies for model selection by any of the following methods: minimum AIC,
minimum BIC, maximum adjusted R-squared, minimum Mallows' Cp and t-tests. The
importance of this upper bound is that it delineates general categories of
design matrices and model selection procedures for which this confidence
interval has poor coverage properties. This upper bound is shown to be a finite
sample analogue of an earlier large sample upper bound due to Kabaila and Leeb.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0711.1036</identifier>
 <datestamp>2010-01-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0711.1036</id><created>2007-11-07</created><updated>2009-05-07</updated><authors><author><keyname>Ptscher</keyname><forenames>Benedikt M.</forenames></author></authors><title>Confidence Sets Based on Sparse Estimators Are Necessarily Large</title><categories>math.ST stat.ME stat.TH</categories><comments>Revision containing correction of some minor errors and typos; some
  additional remarks added</comments><msc-class>62F25; 62C25; 62J07</msc-class><journal-ref>Sankhya 71-A (2009), Part 1, 1-18</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Confidence sets based on sparse estimators are shown to be large compared to
more standard confidence sets, demonstrating that sparsity of an estimator
comes at a substantial price in terms of the quality of the estimator. The
results are set in a general parametric or semiparametric framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0711.2345</identifier>
 <datestamp>2013-09-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0711.2345</id><created>2007-11-15</created><authors><author><keyname>Fougres</keyname><forenames>Anne-Laure</forenames><affiliation>MODAL'X</affiliation></author><author><keyname>Nolan</keyname><forenames>John P.</forenames></author><author><keyname>Rootzn</keyname><forenames>Holger</forenames></author></authors><title>Models for dependent extremes using stable mixtures</title><categories>stat.ME math.ST stat.TH</categories><proxy>ccsd hal-00187600</proxy><journal-ref>Scandinavian Journal of Statistics 36 (2009) 42-59</journal-ref><doi>10.1111/j.1467-9469.2008.00613.x</doi><abstract>  This paper unifies and extends results on a class of multivariate Extreme
Value (EV) models studied by Hougaard, Crowder, and Tawn. In these models both
unconditional and conditional distributions are EV, and all lower-dimensional
marginals and maxima belong to the class. This leads to substantial economies
of understanding, analysis and prediction. One interpretation of the models is
as size mixtures of EV distributions, where the mixing is by positive stable
distributions. A second interpretation is as exponential-stable location
mixtures (for Gumbel) or as power-stable scale mixtures (for non-Gumbel EV
distributions). A third interpretation is through a Peaks over Thresholds model
with a positive stable intensity. The mixing variables are used as a modeling
tool and for better understanding and model checking. We study extreme value
analogues of components of variance models, and new time series, spatial, and
continuous parameter models for extreme values. The results are applied to data
from a pitting corrosion investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0711.2801</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0711.2801</id><created>2007-11-18</created><updated>2007-12-02</updated><authors><author><keyname>Chen</keyname><forenames>Xinjia</forenames></author></authors><title>Inverse Sampling for Nonasymptotic Sequential Estimation of Bounded
  Variable Means</title><categories>math.ST cs.LG math.PR stat.TH</categories><comments>31 pages, 4 figures, added proofs</comments><msc-class>62L12; 62D05; 65C05</msc-class><abstract>  In this paper, we consider the nonasymptotic sequential estimation of means
of random variables bounded in between zero and one. We have rigorously
demonstrated that, in order to guarantee prescribed relative precision and
confidence level, it suffices to continue sampling until the sample sum is no
less than a certain bound and then take the average of samples as an estimate
for the mean of the bounded random variable. We have developed an explicit
formula and a bisection search method for the determination of such bound of
sample sum, without any knowledge of the bounded variable. Moreover, we have
derived bounds for the distribution of sample size. In the special case of
Bernoulli random variables, we have established analytical and numerical
methods to further reduce the bound of sample sum and thus improve the
efficiency of sampling. Furthermore, the fallacy of existing results are
detected and analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0711.2893</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0711.2893</id><created>2007-11-19</created><updated>2008-02-13</updated><authors><author><keyname>Zukovic</keyname><forenames>Milan</forenames></author><author><keyname>Hristopulos</keyname><forenames>Dionissios T.</forenames></author></authors><title>The Method of Normalized Correlations - A Fast Alternative to Maximum
  Likelihood Estimation for Random Processes and Isotropic Random Fields with
  Short-Range Dependence</title><categories>stat.CO stat.ME</categories><comments>This paper has been withdrawn</comments><journal-ref>Technometrics 51 173 (2009)</journal-ref><doi>10.1198/TECH.2009.0018</doi><abstract>  This paper has been withdrawn by the authors, due the copyright policy of the
journal it has been submited to.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0711.3236</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0711.3236</id><created>2007-11-20</created><updated>2009-03-25</updated><authors><author><keyname>Kabaila</keyname><forenames>Paul</forenames></author><author><keyname>Giri</keyname><forenames>Khageswor</forenames></author></authors><title>Confidence intervals in regression utilizing prior information</title><categories>stat.ME</categories><comments>This version differs from v2 in 2 respects. Firstly, a few typos have
  been corrected. Secondly, the paper has been shortened. This version has been
  accepted for publication in Journal of Statistical Planning and Inference</comments><journal-ref>Journal of Statistical Planning and Inference, 139, 3419-3429
  (2009)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a linear regression model with regression parameter
beta=(beta_1,...,beta_p) and independent and identically N(0,sigma^2)
distributed errors. Suppose that the parameter of interest is theta = a^T beta
where a is a specified vector. Define the parameter tau=c^T beta-t where the
vector c and the number t are specified and a and c are linearly independent.
Also suppose that we have uncertain prior information that tau = 0. We present
a new frequentist 1-alpha confidence interval for theta that utilizes this
prior information. We require this confidence interval to (a) have endpoints
that are continuous functions of the data and (b) coincide with the standard
1-alpha confidence interval when the data strongly contradicts this prior
information. This interval is optimal in the sense that it has minimum weighted
average expected length where the largest weight is given to this expected
length when tau=0. This minimization leads to an interval that has the
following desirable properties. This interval has expected length that (a) is
relatively small when the prior information about tau is correct and (b) has a
maximum value that is not too large. The following problem will be used to
illustrate the application of this new confidence interval. Consider a 2-by 2
factorial experiment with 20 replicates. Suppose that the parameter of interest
theta is a specified simple effect and that we have uncertain prior information
that the two-factor interaction is zero. Our aim is to find a frequentist 0.95
confidence interval for theta that utilizes this prior information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0711.3400</identifier>
 <datestamp>2010-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0711.3400</id><created>2007-11-21</created><updated>2010-01-19</updated><authors><author><keyname>Pinelis</keyname><forenames>Iosif</forenames></author></authors><title>On the Non-degeneracy of Kendall's and Spearman's Correlation
  Coefficients</title><categories>math.ST math.GM stat.TH</categories><comments>4 pages; to the statements in version 1 on Kendall's correlation
  statistic now added quite similar statements on Spearman's one; accordingly
  changed the title and abstract</comments><msc-class>62G10; 62G20; 62G05; 62G30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hoeffding proved that Kendall's and Spearman's nonparametric measures of
correlation between two continuous random variables X and Y are each
asymptotically normal with an asymptotic variance of the form sigma^2/n --
provided the non-degeneracy condition sigma^2&gt;0 holds, where sigma^2 is a
certain (always nonnegative) expression which is determined by the joint
distribution (say mu) of X and Y. Sufficient conditions for sigma^2&gt;0 in terms
of the support set (say S) of mu are given, the same for both correlation
statistics. One of them is that there exist a rectangle with all its vertices
in S, sides parallel to the X and Y axes, and an interior point also in S.
Another sufficient condition is that the Lebesgue measure of S be nonzero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0711.3834</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0711.3834</id><created>2007-11-24</created><updated>2011-10-15</updated><authors><author><keyname>Lilly</keyname><forenames>Jonathan M.</forenames></author><author><keyname>Olhede</keyname><forenames>Sofia C.</forenames></author></authors><title>On the Analytic Wavelet Transform</title><categories>math.ST math.FA stat.ME stat.TH</categories><msc-class>42C40, 62G08</msc-class><journal-ref>Lilly, J. M., and S. C. Olhede (2010). On the analytic wavelet
  transform. IEEE Transactions on Information Theory, 56 (8), 4135--4156</journal-ref><doi>10.1109/TIT.2010.2050935</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An exact and general expression for the analytic wavelet transform of a
real-valued signal is constructed, resolving the time-dependent effects of
non-negligible amplitude and frequency modulation. The analytic signal is first
locally represented as a modulated oscillation, demodulated by its own
instantaneous frequency, and then Taylor-expanded at each point in time. The
terms in this expansion, called the instantaneous modulation functions, are
time-varying functions which quantify, at increasingly higher orders, the local
departures of the signal from a uniform sinusoidal oscillation. Closed-form
expressions for these functions are found in terms of Bell polynomials and
derivatives of the signal's instantaneous frequency and bandwidth. The analytic
wavelet transform is shown to depend upon the interaction between the signal's
instantaneous modulation functions and frequency-domain derivatives of the
wavelet, inducing a hierarchy of departures of the transform away from a
perfect representation of the signal. The form of these deviation terms
suggests a set of conditions for matching the wavelet properties to suit the
variability of the signal, in which case our expressions simplify considerably.
One may then quantify the time-varying bias associated with signal estimation
via wavelet ridge analysis, and choose wavelets to minimize this bias.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0711.3955</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0711.3955</id><created>2007-11-26</created><authors><author><keyname>Castillo</keyname><forenames>I.</forenames></author></authors><title>Semi-parametric second-order efficient estimation of the period of a
  signal</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/07-BEJ5077 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ5077</report-no><journal-ref>Bernoulli 2007, Vol. 13, No. 4, 910-932</journal-ref><doi>10.3150/07-BEJ5077</doi><abstract>  This paper is concerned with the estimation of the period of an unknown
periodic function in Gaussian white noise. A class of estimators of the period
is constructed by means of a penalized maximum likelihood method. A
second-order asymptotic expansion of the risk of these estimators is obtained.
Moreover, the minimax problem for the second-order term is studied and an
estimator of the preceding class is shown to be second order efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0711.3957</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0711.3957</id><created>2007-11-26</created><authors><author><keyname>Kutoyants</keyname><forenames>Yury A.</forenames></author><author><keyname>Yoshida</keyname><forenames>Nakahiro</forenames></author></authors><title>Moment estimation for ergodic diffusion processes</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/07-BEJ1040 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ1040</report-no><journal-ref>Bernoulli 2007, Vol. 13, No. 4, 933-951</journal-ref><doi>10.3150/07-BEJ1040</doi><abstract>  We investigate the moment estimation for an ergodic diffusion process with
unknown trend coefficient. We consider nonparametric and parametric estimation.
In each case, we present a lower bound for the risk and then construct an
asymptotically efficient estimator of the moment type functional or of a
parameter which has a one-to-one correspondence to such a functional. Next, we
clarify a higher order property of the moment type estimator by the Edgeworth
expansion of the distribution function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0711.4457</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0711.4457</id><created>2007-11-28</created><authors><author><keyname>Pipiras</keyname><forenames>Vladas</forenames></author><author><keyname>Taqqu</keyname><forenames>Murad S.</forenames></author><author><keyname>Abry</keyname><forenames>Patrice</forenames></author></authors><title>Bounds for the covariance of functions of infinite variance stable
  random variables with applications to central limit theorems and
  wavelet-based estimation</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/07-BEJ6143 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ6143</report-no><journal-ref>Bernoulli 2007, Vol. 13, No. 4, 1091-1123</journal-ref><doi>10.3150/07-BEJ6143</doi><abstract>  We establish bounds for the covariance of a large class of functions of
infinite variance stable random variables, including unbounded functions such
as the power function and the logarithm. These bounds involve measures of
dependence between the stable variables, some of which are new. The bounds are
also used to deduce the central limit theorem for unbounded functions of stable
moving average time series. This result extends the earlier results of Tailen
Hsing and the authors on central limit theorems for bounded functions of stable
moving averages. It can be used to show asymptotic normality of wavelet-based
estimators of the self-similarity parameter in fractional stable motions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0711.4493</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0711.4493</id><created>2007-11-28</created><authors><author><keyname>Synowiecki</keyname><forenames>Rafal</forenames></author></authors><title>Consistency and application of moving block bootstrap for non-stationary
  time series with periodic and almost periodic structure</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/07-BEJ102 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ102</report-no><journal-ref>Bernoulli 2007, Vol. 13, No. 4, 1151-1178</journal-ref><doi>10.3150/07-BEJ102</doi><abstract>  The aim of this paper it to establish sufficient conditions for consistency
of moving block bootstrap for non-stationary time series with periodic and
almost periodic structure. The parameter of the study is the mean value of the
expectation function. Consistency holds in quite general situations: if all
joint distributions of the series are periodic, then it suffices to assume the
central limit theorem and strong mixing property, together with summability of
the autocovariance function. In the case where the mean function is almost
periodic, we additionally need uniform boundedness of the fourth moments of the
root statistics. It is shown that these theoretical results can be applied in
statistical inference concerning the Fourier coefficients of periodically (PC)
and almost periodically (APC) correlated time series. A simulation example
shows how to use a graphical diagnostic test for significant frequencies and
stationarity within these classes of time series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0711.4734</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0711.4734</id><created>2007-11-29</created><updated>2007-12-17</updated><authors><author><keyname>Vlasov</keyname><forenames>Alexander Yu.</forenames></author></authors><title>Signed Chord Length Distribution. I</title><categories>math-ph math.MP math.PR stat.CO</categories><comments>LaTeX2e, 24 pp, 18 fig (8 EPS files), part I (technicalities), v2:
  few corrections in equations and figures, v3: typos and bookmarks, for
  version in Russian, 25 pp, PDF and DjVu, see
  http://friedmann.objectis.net/Members/vlasov/hordes</comments><report-no>EN11790303</report-no><abstract>  In this paper is discussed an application of signed measures (charges) to
description of segment and chord length distributions in nonconvex bodies. The
signed distribution may naturally appears due to definition via derivatives of
nonnegative autocorrelation function simply related with distances distribution
between pairs of points in the body. In the work is suggested constructive
geometrical interpretation of such derivatives and illustrated appearance of
"positive" and "negative" elements similar with usual Hanh-Jordan decomposition
in measure theory. The construction is also close related with applications of
Dirac method of chords.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0712.0096</identifier>
 <datestamp>2010-02-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0712.0096</id><created>2007-12-01</created><authors><author><keyname>Singh</keyname><forenames>Rajesh</forenames></author><author><keyname>Chauhan</keyname><forenames>Pankaj</forenames></author><author><keyname>Sawan</keyname><forenames>Nirmala</forenames></author><author><keyname>Smarandache</keyname><forenames>Florentin</forenames></author></authors><title>Auxiliary Information and A Priori Values in Construction of Improved
  Estimators</title><categories>stat.ME</categories><comments>74 pages</comments><msc-class>62J10</msc-class><journal-ref>Third paper published in Bulletin of Statistics &amp; Economics, Vol.
  3, No. A09, pp. 13-18, Fall 2009.</journal-ref><abstract>  This volume is a collection of six papers on the use of auxiliary information
and 'a priori' values in construction of improved estimators. The work included
here will be of immense application for researchers and students who emply
auxiliary information in any form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0712.0775</identifier>
 <datestamp>2010-07-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0712.0775</id><created>2007-12-05</created><updated>2010-01-11</updated><authors><author><keyname>Arlot</keyname><forenames>Sylvain</forenames></author><author><keyname>Blanchard</keyname><forenames>Gilles</forenames></author><author><keyname>Roquain</keyname><forenames>Etienne</forenames></author></authors><title>Some nonasymptotic results on resampling in high dimension, I:
  Confidence regions, II: Multiple tests</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/08-AOS667;
  http://dx.doi.org/10.1214/08-AOS668 the Annals of Statistics
  (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics
  (http://www.imstat.org)</comments><proxy>ccsd hal-00194145</proxy><report-no>IMS-AOS-AOS667; IMS-AOS-AOS668</report-no><msc-class>62G15 (Primary) 62G09 (Secondary), 62G10 (Primary) 62G09 (Secondary)</msc-class><journal-ref>The Annals of Statistics 38, 1 (2010) 51-99</journal-ref><doi>10.1214/08-AOS667; 10.1214/08-AOS668</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study generalized bootstrap confidence regions for the mean of a random
vector whose coordinates have an unknown dependency structure. The random
vector is supposed to be either Gaussian or to have a symmetric and bounded
distribution. The dimensionality of the vector can possibly be much larger than
the number of observations and we focus on a nonasymptotic control of the
confidence level, following ideas inspired by recent results in learning
theory. We consider two approaches, the first based on a concentration
principle (valid for a large class of resampling weights) and the second on a
resampled quantile, specifically using Rademacher weights. Several intermediate
results established in the approach based on concentration principles are of
interest in their own right. We also discuss the question of accuracy when
using Monte Carlo approximations of the resampled quantities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0712.2526</identifier>
 <datestamp>2010-06-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0712.2526</id><created>2007-12-15</created><updated>2008-01-15</updated><authors><author><keyname>Braun</keyname><forenames>Michael</forenames></author><author><keyname>McAuliffe</keyname><forenames>Jon</forenames></author></authors><title>Variational inference for large-scale models of discrete choice</title><categories>stat.ME stat.CO stat.ML</categories><comments>29 pages, 2 tables, 2 figures</comments><journal-ref>Journal of the American Statistical Association (2010) 105(489):
  324-334</journal-ref><doi>10.1198/jasa.2009.tm08030</doi><abstract>  Discrete choice models are commonly used by applied statisticians in numerous
fields, such as marketing, economics, finance, and operations research. When
agents in discrete choice models are assumed to have differing preferences,
exact inference is often intractable. Markov chain Monte Carlo techniques make
approximate inference possible, but the computational cost is prohibitive on
the large data sets now becoming routinely available. Variational methods
provide a deterministic alternative for approximation of the posterior
distribution. We derive variational procedures for empirical Bayes and fully
Bayesian inference in the mixed multinomial logit model of discrete choice. The
algorithms require only that we solve a sequence of unconstrained optimization
problems, which are shown to be convex. Extensive simulations demonstrate that
variational methods achieve accuracy competitive with Markov chain Monte Carlo,
at a small fraction of the computational cost. Thus, variational methods permit
inferences on data sets that otherwise could not be analyzed without
bias-inducing modifications to the underlying model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0712.2708</identifier>
 <datestamp>2011-02-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0712.2708</id><created>2007-12-17</created><updated>2011-02-17</updated><authors><author><keyname>Davison</keyname><forenames>A. C.</forenames></author><author><keyname>Sartori</keyname><forenames>N.</forenames></author></authors><title>The Banff Challenge: Statistical Detection of a Noisy Signal</title><categories>stat.AP stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/08-STS260 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS260</report-no><journal-ref>Statistical Science 2008, Vol. 23, No. 3, 354-364</journal-ref><doi>10.1214/08-STS260</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particle physics experiments such as those run in the Large Hadron Collider
result in huge quantities of data, which are boiled down to a few numbers from
which it is hoped that a signal will be detected. We discuss a simple
probability model for this and derive frequentist and noninformative Bayesian
procedures for inference about the signal. Both are highly accurate in
realistic cases, with the frequentist procedure having the edge for interval
estimation, and the Bayesian procedure yielding slightly better point
estimates. We also argue that the significance, or $p$-value, function based on
the modified likelihood root provides a comprehensive presentation of the
information in the data and should be used for inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0712.3056</identifier>
 <datestamp>2010-01-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0712.3056</id><created>2007-12-18</created><updated>2010-01-21</updated><authors><author><keyname>Johnson</keyname><forenames>Alicia A.</forenames></author><author><keyname>Jones</keyname><forenames>Galin L.</forenames></author></authors><title>Gibbs Sampling for a Bayesian Hierarchical General Linear Model</title><categories>stat.CO math.ST stat.TH</categories><comments>20 pages, 1 figure, submitted to Electronic Journal of Statistics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a Bayesian hierarchical version of the normal theory general
linear model which is practically relevant in the sense that it is general
enough to have many applications and it is not straightforward to sample
directly from the corresponding posterior distribution. Thus we study a block
Gibbs sampler that has the posterior as its invariant distribution. In
particular, we establish that the Gibbs sampler converges at a geometric rate.
This allows us to establish conditions for a central limit theorem for the
ergodic averages used to estimate features of the posterior. Geometric
ergodicity is also a key component for using batch means methods to
consistently estimate the variance of the asymptotic normal distribution.
Together, our results give practitioners the tools to be as confident in
inferences based on the observations from the Gibbs sampler as they would be
with inferences based on random samples from the posterior. Our theoretical
results are illustrated with an application to data on the cost of health plans
issued by health maintenance organizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0712.3744</identifier>
 <datestamp>2010-06-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0712.3744</id><created>2007-12-21</created><updated>2010-06-13</updated><authors><author><keyname>Vazquez</keyname><forenames>Emmanuel</forenames></author><author><keyname>Bect</keyname><forenames>Julien</forenames></author></authors><title>Convergence properties of the expected improvement algorithm</title><categories>stat.CO math.OC</categories><comments>This paper has been withdrawn</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn from the arXiv. It is now published by Elsevier
in the Journal of Statistical Planning and Inference, under the modified title
"Convergence properties of the expected improvement algorithm with fixed mean
and covariance functions". See http://dx.doi.org/10.1016/j.jspi.2010.04.018
  An author-generated post-print version is available from the HAL repository
of SUPELEC at http://hal-supelec.archives-ouvertes.fr/hal-00217562
  Abstract : "This paper deals with the convergence of the expected improvement
algorithm, a popular global optimization algorithm based on a Gaussian process
model of the function to be optimized. The first result is that under some mild
hypotheses on the covariance function k of the Gaussian process, the expected
improvement algorithm produces a dense sequence of evaluation points in the
search domain, when the function to be optimized is in the reproducing kernel
Hilbert space generated by k. The second result states that the density
property also holds for P-almost all continuous functions, where P is the
(prior) probability distribution induced by the Gaussian process."
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0712.4273</identifier>
 <datestamp>2011-12-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0712.4273</id><created>2007-12-27</created><updated>2011-12-02</updated><authors><author><keyname>Capp</keyname><forenames>Olivier</forenames><affiliation>LTCI</affiliation></author><author><keyname>Moulines</keyname><forenames>Eric</forenames><affiliation>LTCI</affiliation></author></authors><title>Online EM Algorithm for Latent Data Models</title><categories>stat.CO cs.LG</categories><comments>Version that includes the corrigendum published in volume 73, part 5
  (2011), of the Journal of the Royal Statistical Society, Series B</comments><proxy>ccsd</proxy><journal-ref>Journal of the Royal Statistical Society Series B (Statistical
  Methodology) 71, 3 (2009) 593-613</journal-ref><doi>10.1111/j.1467-9868.2009.00698.x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this contribution, we propose a generic online (also sometimes called
adaptive or recursive) version of the Expectation-Maximisation (EM) algorithm
applicable to latent variable models of independent observations. Compared to
the algorithm of Titterington (1984), this approach is more directly connected
to the usual EM algorithm and does not rely on integration with respect to the
complete data distribution. The resulting algorithm is usually simpler and is
shown to achieve convergence to the stationary points of the Kullback-Leibler
divergence between the marginal distribution of the observation and the model
distribution at the optimal rate, i.e., that of the maximum likelihood
estimator. In addition, the proposed approach is also suitable for conditional
(or regression) models, as illustrated in the case of the mixture of linear
regressions model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0801.0254</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0801.0254</id><created>2007-12-31</created><authors><author><keyname>Laubenbacher</keyname><forenames>Reinhard</forenames></author><author><keyname>Stigler</keyname><forenames>Brandilyn</forenames></author></authors><title>Design of experiments and biochemical network inference</title><categories>q-bio.MN stat.AP</categories><comments>To appear in "Algebraic and geometric methods in statistics," P.
  Gibilisco, E. Riccomagno, M.-P. Rogantin, H. P. Wynn, eds., Cambridge
  University Press, 2008</comments><abstract>  Design of experiments is a branch of statistics that aims to identify
efficient procedures for planning experiments in order to optimize knowledge
discovery. Network inference is a subfield of systems biology devoted to the
identification of biochemical networks from experimental data. Common to both
areas of research is their focus on the maximization of information gathered
from experimentation. The goal of this paper is to establish a connection
between these two areas coming from the common use of polynomial models and
techniques from computational algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0801.0461</identifier>
 <datestamp>2010-10-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0801.0461</id><created>2008-01-02</created><updated>2010-10-15</updated><authors><author><keyname>Wallach</keyname><forenames>Hanna M.</forenames></author><author><keyname>Jensen</keyname><forenames>Shane T.</forenames></author><author><keyname>Dicker</keyname><forenames>Lee</forenames></author><author><keyname>Heller</keyname><forenames>Katherine A.</forenames></author></authors><title>An Alternative Prior Process for Nonparametric Bayesian Clustering</title><categories>stat.ME math.ST stat.TH</categories><journal-ref>Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics (AISTATS) 2010, JMLR W &amp; CP 9, pp.
  892-899</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prior distributions play a crucial role in Bayesian approaches to clustering.
Two commonly-used prior distributions are the Dirichlet and Pitman-Yor
processes. In this paper, we investigate the predictive probabilities that
underlie these processes, and the implicit "rich-get-richer" characteristic of
the resulting partitions. We explore an alternative prior for nonparametric
Bayesian clustering -- the uniform process -- for applications where the
"rich-get-richer" property is undesirable. We also explore the cost of this
process: partitions are no longer exchangeable with respect to the ordering of
variables. We present new asymptotic and simulation-based results for the
clustering characteristics of the uniform process and compare these with known
results for the Dirichlet and Pitman-Yor processes. We compare performance on a
real document clustering task, demonstrating the practical advantage of the
uniform process despite its lack of exchangeability over orderings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0801.0712</identifier>
 <datestamp>2010-01-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0801.0712</id><created>2008-01-04</created><updated>2010-01-14</updated><authors><author><keyname>Jankowski</keyname><forenames>Hanna K.</forenames></author><author><keyname>Wellner</keyname><forenames>Jon A.</forenames></author></authors><title>Nonparametric estimation of a convex bathtub-shaped hazard function</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ202 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><report-no>IMS-BEJ-BEJ202</report-no><journal-ref>Bernoulli 2009, Vol. 15, No. 4, 1010-1035</journal-ref><doi>10.3150/09-BEJ202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the nonparametric maximum likelihood estimator (MLE)
of a convex hazard function. We show that the MLE is consistent and converges
at a local rate of $n^{2/5}$ at points $x_0$ where the true hazard function is
positive and strictly convex. Moreover, we establish the pointwise asymptotic
distribution theory of our estimator under these same assumptions. One notable
feature of the nonparametric MLE studied here is that no arbitrary choice of
tuning parameter (or complicated data-adaptive selection of the tuning
parameter) is required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0801.1095</identifier>
 <datestamp>2010-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0801.1095</id><created>2008-01-07</created><updated>2010-11-09</updated><authors><author><keyname>Bickel</keyname><forenames>Peter J.</forenames></author><author><keyname>Ritov</keyname><forenames>Ya'acov</forenames></author><author><keyname>Tsybakov</keyname><forenames>Alexandre B.</forenames></author></authors><title>Simultaneous analysis of Lasso and Dantzig selector</title><categories>math.ST stat.TH</categories><comments>Noramlization factor corrected</comments><journal-ref>The Annals of Statistics 2009, Vol. 37, No. 4, 1705-1732</journal-ref><doi>10.1214/08-AOS620</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We exhibit an approximate equivalence between the Lasso estimator and Dantzig
selector. For both methods we derive parallel oracle inequalities for the
prediction risk in the general nonparametric regression model, as well as
bounds on the $\ell_p$ estimation loss for $1\le p\le 2$ in the linear model
when the number of variables can be much larger than the sample size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0801.1758</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0801.1758</id><created>2008-01-11</created><authors><author><keyname>Barone</keyname><forenames>Piero</forenames></author></authors><title>A new transform for solving the noisy complex exponentials approximation
  problem</title><categories>math.ST math.NA stat.ME stat.TH</categories><comments>42 pages, 5 figures</comments><msc-class>60G57, 41A21</msc-class><journal-ref>Journal of Approximation Theory, vol.155, pp. 1-27, 2008</journal-ref><doi>10.1016/j.jat.2008.04.007</doi><abstract>  The problem of estimating a complex measure made up by a linear combination
of Dirac distributions centered on points of the complex plane from a finite
number of its complex moments affected by additive i.i.d. Gaussian noise is
considered. A random measure is defined whose expectation approximates the
unknown measure under suitable conditions. An estimator of the approximating
measure is then proposed as well as a new discrete transform of the noisy
moments that allows to compute an estimate of the unknown measure. A small
simulation study is also performed to experimentally check the goodness of the
approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0801.2456</identifier>
 <datestamp>2015-01-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0801.2456</id><created>2008-01-16</created><authors><author><keyname>Boucheron</keyname><forenames>Stphane</forenames><affiliation>PMA</affiliation></author><author><keyname>Garivier</keyname><forenames>Aurlien</forenames><affiliation>LM-Orsay</affiliation></author><author><keyname>Gassiat</keyname><forenames>Elisabeth</forenames><affiliation>LM-Orsay</affiliation></author></authors><title>Coding on countably infinite alphabets</title><categories>math.ST stat.TH</categories><comments>33 pages</comments><proxy>ccsd hal-00121892</proxy><msc-class>62B10, 68P30, 94A29</msc-class><journal-ref>Information Theory, IEEE Transactions on (Volume:55 , Issue: 1 )
  358 - 373 Jan. 2009</journal-ref><doi>10.1109/TIT.2008.2008150</doi><abstract>  This paper describes universal lossless coding strategies for compressing
sources on countably infinite alphabets. Classes of memoryless sources defined
by an envelope condition on the marginal distribution provide benchmarks for
coding techniques originating from the theory of universal coding over finite
alphabets. We prove general upper-bounds on minimax regret and lower-bounds on
minimax redundancy for such source classes. The general upper bounds emphasize
the role of the Normalized Maximum Likelihood codes with respect to minimax
regret in the infinite alphabet context. Lower bounds are derived by tailoring
sharp bounds on the redundancy of Krichevsky-Trofimov coders for sources over
finite alphabets. Up to logarithmic (resp. constant) factors the bounds are
matching for source classes defined by algebraically declining (resp.
exponentially vanishing) envelopes. Effective and (almost) adaptive coding
techniques are described for the collection of source classes defined by
algebraically vanishing envelopes. Those results extend ourknowledge concerning
universal coding to contexts where the key tools from parametric inference
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0801.2800</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0801.2800</id><created>2008-01-17</created><updated>2008-01-30</updated><authors><author><keyname>Sheridan</keyname><forenames>Paul</forenames></author><author><keyname>Yagahara</keyname><forenames>Yuichi</forenames></author><author><keyname>Shimodaira</keyname><forenames>Hidetoshi</forenames></author></authors><title>A preferential attachment model with Poisson growth for scale-free
  networks</title><categories>stat.AP</categories><comments>18 pages with 2 figures; correction to a proof in the appendix</comments><journal-ref>Annals of the Institute of Statistical Mathematics 2008, Vol. 60,
  pp. 747-761</journal-ref><doi>10.1007/s10463-008-0181-5</doi><abstract>  We propose a scale-free network model with a tunable power-law exponent. The
Poisson growth model, as we call it, is an offshoot of the celebrated model of
Barab\'{a}si and Albert where a network is generated iteratively from a small
seed network; at each step a node is added together with a number of incident
edges preferentially attached to nodes already in the network. A key feature of
our model is that the number of edges added at each step is a random variable
with Poisson distribution, and, unlike the Barab\'{a}si-Albert model where this
quantity is fixed, it can generate any network. Our model is motivated by an
application in Bayesian inference implemented as Markov chain Monte Carlo to
estimate a network; for this purpose, we also give a formula for the
probability of a network under our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0801.3352</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0801.3352</id><created>2008-01-22</created><updated>2010-09-07</updated><authors><author><keyname>Barone</keyname><forenames>Piero</forenames></author></authors><title>On the condensed density of the generalized eigenvalues of pencils of
  Hankel Gaussian random matrices and applications</title><categories>math.ST math.PR stat.CO stat.TH</categories><comments>30 pages, 16 figures, better approximations provided</comments><msc-class>15A52, 44A60</msc-class><journal-ref>Journal of Multivariate Analysis 111 (2012) 160-173</journal-ref><doi>10.1016/j.jmva.2012.05.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pencils of Hankel matrices whose elements have a joint Gaussian distribution
with nonzero mean and not identical covariance are considered. An approximation
to the distribution of the squared modulus of their determinant is computed
which allows to get a closed form approximation of the condensed density of the
generalized eigenvalues of the pencils. Implications of this result for solving
several moments problems are discussed and some numerical examples are
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0801.3513</identifier>
 <datestamp>2010-10-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0801.3513</id><created>2008-01-23</created><updated>2008-06-05</updated><authors><author><keyname>Robert</keyname><forenames>Christian</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Marin</keyname><forenames>Jean-Michel</forenames><affiliation>INRIA Futurs</affiliation></author></authors><title>On some difficulties with a posterior probability approximation
  technique</title><categories>stat.CO math.ST stat.TH</categories><comments>Second version, resubmitted</comments><journal-ref>Bayesian Analysis(2008), 3(2), 427-442</journal-ref><doi>10.1214/08-BA316</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Scott (2002) and Congdon (2006), a new method is advanced to compute
posterior probabilities of models under consideration. It is based solely on
MCMC outputs restricted to single models, i.e., it is bypassing reversible jump
and other model exploration techniques. While it is indeed possible to
approximate posterior probabilities based solely on MCMC outputs from single
models, as demonstrated by Gelfand and Dey (1994) and Bartolucci et al. (2006),
we show that the proposals of Scott (2002) and Congdon (2006) are biased and
advance several arguments towards this thesis, the primary one being the
confusion between model-based posteriors and joint pseudo-posteriors. From a
practical point of view, the bias in Scott's (2002) approximation appears to be
much more severe than the one in Congdon's (2006), the later being often of the
same magnitude as the posterior probability it approximates, although we also
exhibit an example where the divergence from the true posterior probability is
extreme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0801.3552</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0801.3552</id><created>2008-01-23</created><updated>2010-11-07</updated><authors><author><keyname>Clifford</keyname><forenames>Peter</forenames></author><author><keyname>Cosma</keyname><forenames>Ioana A.</forenames></author></authors><title>A statistical analysis of probabilistic counting algorithms</title><categories>stat.CO</categories><comments>19 pages, 0 figures</comments><journal-ref>Scandinavian Journal of Statistics, 39, 1, 1-14, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of cardinality estimation in data stream
applications. We present a statistical analysis of probabilistic counting
algorithms, focusing on two techniques that use pseudo-random variates to form
low-dimensional data sketches. We apply conventional statistical methods to
compare probabilistic algorithms based on storing either selected order
statistics, or random projections. We derive estimators of the cardinality in
both cases, and show that the maximal-term estimator is recursively computable
and has exponentially decreasing error bounds. Furthermore, we show that the
estimators have comparable asymptotic efficiency, and explain this result by
demonstrating an unexpected connection between the two approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0801.3887</identifier>
 <datestamp>2010-10-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0801.3887</id><created>2008-01-25</created><updated>2009-07-10</updated><authors><author><keyname>Chopin</keyname><forenames>Nicolas</forenames><affiliation>CREST</affiliation></author><author><keyname>Robert</keyname><forenames>Christian</forenames><affiliation>CREST, Ceremade</affiliation></author></authors><title>Properties of Nested Sampling</title><categories>stat.CO math.ST stat.TH</categories><comments>Revision submitted to Biometrika</comments><proxy>ccsd hal-00216003</proxy><journal-ref>Biometrika 97(3):741-755, 2010</journal-ref><doi>10.1093/biomet/asq021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nested sampling is a simulation method for approximating marginal likelihoods
proposed by Skilling (2006). We establish that nested sampling has an
approximation error that vanishes at the standard Monte Carlo rate and that
this error is asymptotically Gaussian. We show that the asymptotic variance of
the nested sampling approximation typically grows linearly with the dimension
of the parameter. We discuss the applicability and efficiency of nested
sampling in realistic problems, and we compare it with two current methods for
computing marginal likelihood. We propose an extension that avoids resorting to
Markov chain Monte Carlo to obtain the simulated points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0801.4172</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0801.4172</id><created>2008-01-28</created><authors><author><keyname>Barone</keyname><forenames>Piero</forenames></author></authors><title>Computational aspects and applications of a new transform for solving
  the complex exponentials approximation problem</title><categories>math.NA stat.AP stat.CO</categories><comments>28 pages, 20 figures</comments><msc-class>62M15, 30Exx</msc-class><journal-ref>Digital Signal processing 20 (2010) 724-735</journal-ref><doi>10.1016/j.dsp.2009.10.003</doi><abstract>  Many real life problems can be reduced to the solution of a complex
exponentials approximation problem which is usually ill posed. Recently a new
transform for solving this problem, formulated as a specific moments problem in
the plane, has been proposed in a theoretical framework. In this work some
computational issues are addressed to make this new tool useful in practice. An
algorithm is developed and used to solve a Nuclear Magnetic Resonance
spectrometry problem, two time series interpolation and extrapolation problems
and a shape from moments problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0801.4190</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0801.4190</id><created>2008-01-28</created><updated>2009-07-27</updated><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Roch</keyname><forenames>Sebastien</forenames></author></authors><title>Phylogenies without Branch Bounds: Contracting the Short, Pruning the
  Deep</title><categories>q-bio.PE cs.CE cs.DS math.PR math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new phylogenetic reconstruction algorithm which, unlike most
previous rigorous inference techniques, does not rely on assumptions regarding
the branch lengths or the depth of the tree. The algorithm returns a forest
which is guaranteed to contain all edges that are: 1) sufficiently long and 2)
sufficiently close to the leaves. How much of the true tree is recovered
depends on the sequence length provided. The algorithm is distance-based and
runs in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0801.4410</identifier>
 <datestamp>2012-02-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0801.4410</id><created>2008-01-28</created><updated>2012-02-23</updated><authors><author><keyname>Maruyama</keyname><forenames>Yuzo</forenames></author><author><keyname>George</keyname><forenames>Edward I.</forenames></author></authors><title>Fully Bayes factors with a generalized g-prior</title><categories>stat.ME math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/11-AOS917 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS917</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 5, 2740-2765</journal-ref><doi>10.1214/11-AOS917</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the normal linear model variable selection problem, we propose selection
criteria based on a fully Bayes formulation with a generalization of Zellner's
$g$-prior which allows for $p&gt;n$. A special case of the prior formulation is
seen to yield tractable closed forms for marginal densities and Bayes factors
which reveal new model evaluation characteristics of potential interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0802.0082</identifier>
 <datestamp>2012-01-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0802.0082</id><created>2008-02-01</created><updated>2012-01-09</updated><authors><author><keyname>Pan</keyname><forenames>G. M.</forenames></author><author><keyname>Zhou</keyname><forenames>W.</forenames></author></authors><title>Central limit theorem for Hotelling's $T^2$ statistic under large
  dimension</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AAP742 the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP742</report-no><journal-ref>Annals of Applied Probability 2011, Vol. 21, No. 5, 1860-1910</journal-ref><doi>10.1214/10-AAP742</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove the central limit theorem for Hotelling's $T^2$
statistic when the dimension of the random vectors is proportional to the
sample size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0802.0489</identifier>
 <datestamp>2010-07-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0802.0489</id><created>2008-02-04</created><updated>2010-05-31</updated><authors><author><keyname>Bardet</keyname><forenames>Jean-Marc</forenames><affiliation>SAMM</affiliation></author><author><keyname>Surgailis</keyname><forenames>Donatas</forenames></author></authors><title>Measuring the roughness of random paths by increment ratios</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A statistic based on increment ratios (IR) and related to zero crossings of
increment sequence is defined and studied for measuring the roughness of random
paths. The main advantages of this statistic are robustness to smooth additive
and multiplicative trends and applicability to infinite variance processes. The
existence of the IR statistic limit (called the IR-roughness below) is closely
related to the existence of a tangent process. Three particular cases where the
IR-roughness exists and is explicitly computed are considered. Firstly, for a
diffusion process with smooth diffusion and drift coefficients, the
IR-roughness coincides with the IR-roughness of a Brownian motion and its
convergence rate is obtained. Secondly, the case of rough Gaussian processes is
studied in detail under general assumptions which do not require stationarity
conditions. Thirdly, the IR-roughness of a L\'evy process with $\alpha-$stable
tangent process is established and can be used to estimate the fractional
parameter $\alpha \in (0,2)$ following a central limit theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0802.0837</identifier>
 <datestamp>2010-07-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0802.0837</id><created>2008-02-06</created><updated>2008-12-17</updated><authors><author><keyname>Arlot</keyname><forenames>Sylvain</forenames><affiliation>LM-Orsay, INRIA Futurs</affiliation></author><author><keyname>Massart</keyname><forenames>Pascal</forenames><affiliation>LM-Orsay, INRIA Futurs</affiliation></author></authors><title>Data-driven calibration of penalties for least-squares regression</title><categories>math.ST stat.ME stat.TH</categories><proxy>ccsd hal-00243116</proxy><msc-class>62G05 (Primary) 62J05 (Secondary)</msc-class><journal-ref>Journal of Machine Learning Research 10 (2009) 245-279</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Penalization procedures often suffer from their dependence on multiplying
factors, whose optimal values are either unknown or hard to estimate from the
data. We propose a completely data-driven calibration algorithm for this
parameter in the least-squares regression framework, without assuming a
particular shape for the penalty. Our algorithm relies on the concept of
minimal penalty, recently introduced by Birge and Massart (2007) in the context
of penalized least squares for Gaussian homoscedastic regression. On the
positive side, the minimal penalty can be evaluated from the data themselves,
leading to a data-driven estimation of an optimal penalty which can be used in
practice; on the negative side, their approach heavily relies on the
homoscedastic Gaussian nature of their stochastic framework. The purpose of
this paper is twofold: stating a more general heuristics for designing a
data-driven penalty (the slope heuristics) and proving that it works for
penalized least-squares regression with a random design, even for
heteroscedastic non-Gaussian data. For technical reasons, some exact
mathematical results will be proved only for regressogram bin-width selection.
This is at least a first step towards further results, since the approach and
the method that we use are indeed general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0802.0914</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0802.0914</id><created>2008-02-07</created><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Roch</keyname><forenames>Sebastien</forenames></author><author><keyname>Steel</keyname><forenames>Mike</forenames></author></authors><title>Shrinkage Effect in Ancestral Maximum Likelihood</title><categories>q-bio.PE math.PR math.ST stat.TH</categories><abstract>  Ancestral maximum likelihood (AML) is a method that simultaneously
reconstructs a phylogenetic tree and ancestral sequences from extant data
(sequences at the leaves). The tree and ancestral sequences maximize the
probability of observing the given data under a Markov model of sequence
evolution, in which branch lengths are also optimized but constrained to take
the same value on any edge across all sequence sites. AML differs from the more
usual form of maximum likelihood (ML) in phylogenetics because ML averages over
all possible ancestral sequences. ML has long been known to be statistically
consistent -- that is, it converges on the correct tree with probability
approaching 1 as the sequence length grows. However, the statistical
consistency of AML has not been formally determined, despite informal remarks
in a literature that dates back 20 years. In this short note we prove a general
result that implies that AML is statistically inconsistent. In particular we
show that AML can `shrink' short edges in a tree, resulting in a tree that has
no internal resolution as the sequence length grows. Our results apply to any
number of taxa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0802.1669</identifier>
 <datestamp>2010-04-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0802.1669</id><created>2008-02-12</created><updated>2010-04-21</updated><authors><author><keyname>Chia</keyname><forenames>Nicholas</forenames></author><author><keyname>Nakano</keyname><forenames>Junji</forenames></author></authors><title>M-decomposability, elliptical unimodal densities, and applications to
  clustering and kernel density estimation</title><categories>stat.ME math.ST stat.ML stat.TH</categories><comments>30 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chia and Nakano (2009) introduced the concept of M-decomposability of
probability densities in one-dimension. In this paper, we generalize
M-decomposability to any dimension. We prove that all elliptical unimodal
densities are M-undecomposable. We also derive an inequality to show that it is
better to represent an M-decomposable density via a mixture of unimodal
densities. Finally, we demonstrate the application of M-decomposability to
clustering and kernel density estimation, using real and simulated data. Our
results show that M-decomposability can be used as a non-parametric criterion
to locate modes in probability densities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0802.2377</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0802.2377</id><created>2008-02-17</created><updated>2009-02-15</updated><authors><author><keyname>Lilly</keyname><forenames>J. M.</forenames></author><author><keyname>Olhede</keyname><forenames>S. C.</forenames></author></authors><title>Higher-Order Properties of Analytic Wavelets</title><categories>stat.ME math.ST stat.TH</categories><comments>15 pages, 6 Postscript figures</comments><report-no>Research Report 289</report-no><journal-ref>Lilly, J. M., and S. C. Olhede, (2009). Higher-order properties of
  analytic wavelets. IEEE Transactions on Signal Processing, 57 (1), 146--160</journal-ref><doi>10.1109/TSP.2008.2007607</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The influence of higher-order wavelet properties on the analytic wavelet
transform behavior is investigated, and wavelet functions offering advantageous
performance are identified. This is accomplished through detailed investigation
of the generalized Morse wavelets, a two-parameter family of exactly analytic
continuous wavelets. The degree of time/frequency localization, the existence
of a mapping between scale and frequency, and the bias involved in estimating
properties of modulated oscillatory signals, are proposed as important
considerations. Wavelet behavior is found to be strongly impacted by the degree
of asymmetry of the wavelet in both the frequency and the time domain, as
quantified by the third central moments. A particular subset of the generalized
Morse wavelets, recognized as deriving from an inhomogeneous Airy function,
emerge as having particularly desirable properties. These "Airy wavelets"
substantially outperform the only approximately analytic Morlet wavelets for
high time localization. Special cases of the generalized Morse wavelets are
examined, revealing a broad range of behaviors which can be matched to the
characteristics of a signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0802.2424</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0802.2424</id><created>2008-02-18</created><authors><author><keyname>Autin</keyname><forenames>Florent</forenames><affiliation>MODAL'X</affiliation></author><author><keyname>Pennec</keyname><forenames>Erwan Le</forenames><affiliation>PMA</affiliation></author><author><keyname>Tribouley</keyname><forenames>Karine</forenames><affiliation>MODAL'X, PMA</affiliation></author></authors><title>Thresholding methods to estimate the copula density</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00256197</proxy><journal-ref>Journal of Multivariate Analysis 101, 1 (2010) 200-222</journal-ref><doi>10.1016/j.jmva.2009.07.009</doi><abstract>  This paper deals with the problem of the multivariate copula density
estimation. Using wavelet methods we provide two shrinkage procedures based on
thresholding rules for which the knowledge of the regularity of the copula
density to be estimated is not necessary. These methods, said to be adaptive,
are proved to perform very well when adopting the minimax and the maxiset
approaches. Moreover we show that these procedures can be discriminated in the
maxiset sense. We produce an estimation algorithm whose qualities are evaluated
thanks some simulation. Last, we propose a real life application for financial
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0802.2581</identifier>
 <datestamp>2010-07-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0802.2581</id><created>2008-02-18</created><updated>2008-05-27</updated><authors><author><keyname>Hara</keyname><forenames>Hisayuki</forenames></author><author><keyname>Takemura</keyname><forenames>Akimichi</forenames></author></authors><title>A Localization Approach to Improve Iterative Proportional Scaling in
  Gaussian Graphical Models</title><categories>stat.CO stat.ME</categories><comments>12 pages</comments><journal-ref>Communications in Statistics Theory and Methods, 39, No.8,
  1643-1654, 2010</journal-ref><doi>10.1080/03610920802238662</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss an efficient implementation of the iterative proportional scaling
procedure in the multivariate Gaussian graphical models. We show that the
computational cost can be reduced by localization of the update procedure in
each iterative step by using the structure of a decomposable model obtained by
triangulation of the graph associated with the model. Some numerical
experiments demonstrate the competitive performance of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0802.2655</identifier>
 <datestamp>2010-07-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0802.2655</id><created>2008-02-19</created><updated>2010-06-09</updated><authors><author><keyname>Bubeck</keyname><forenames>Sbastien</forenames><affiliation>INRIA Futurs</affiliation></author><author><keyname>Munos</keyname><forenames>Rmi</forenames><affiliation>INRIA Futurs</affiliation></author><author><keyname>Stoltz</keyname><forenames>Gilles</forenames><affiliation>DMA, GREGH</affiliation></author></authors><title>Pure Exploration for Multi-Armed Bandit Problems</title><categories>math.ST cs.LG stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the framework of stochastic multi-armed bandit problems and study
the possibilities and limitations of forecasters that perform an on-line
exploration of the arms. These forecasters are assessed in terms of their
simple regret, a regret notion that captures the fact that exploration is only
constrained by the number of available rounds (not necessarily known in
advance), in contrast to the case when the cumulative regret is considered and
when exploitation needs to be performed at the same time. We believe that this
performance criterion is suited to situations when the cost of pulling an arm
is expressed in terms of resources rather than rewards. We discuss the links
between the simple and the cumulative regret. One of the main results in the
case of a finite number of arms is a general lower bound on the simple regret
of a forecaster in terms of its cumulative regret: the smaller the latter, the
larger the former. Keeping this result in mind, we then exhibit upper bounds on
the simple regret of some forecasters. The paper ends with a study devoted to
continuous-armed bandit problems; we show that the simple regret can be
minimized with respect to a family of probability distributions if and only if
the cumulative regret can be minimized for it. Based on this equivalence, we
are able to prove that the separable metric spaces are exactly the metric
spaces on which these regrets can be minimized with respect to the family of
all probability distributions with continuous mean-payoff functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0802.2959</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0802.2959</id><created>2008-02-20</created><authors><author><keyname>Desai</keyname><forenames>Keyur</forenames></author><author><keyname>Deller,</keyname><forenames>J. R.</forenames><suffix>Jr.</suffix></author><author><keyname>McCormick</keyname><forenames>J. Justin</forenames></author></authors><title>Tellipsoid: Exploiting inter-gene correlation for improved detection of
  differential gene expression</title><categories>stat.ME q-bio.GN stat.AP</categories><comments>19 pages, Submitted to Bioinformatics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Algorithms for differential analysis of microarray data are vital
to modern biomedical research. Their accuracy strongly depends on effective
treatment of inter-gene correlation. Correlation is ordinarily accounted for in
terms of its effect on significance cut-offs. In this paper it is shown that
correlation can, in fact, be exploited {to share information across tests},
which, in turn, can increase statistical power.
  Results: Vastly and demonstrably improved differential analysis approaches
are the result of combining identifiability (the fact that in most microarray
data sets, a large proportion of genes can be identified a priori as
non-differential) with optimization criteria that incorporate correlation. As a
special case, we develop a method which builds upon the widely used two-sample
t-statistic based approach and uses the Mahalanobis distance as an optimality
criterion. Results on the prostate cancer data of Singh et al. (2002) suggest
that the proposed method outperforms all published approaches in terms of
statistical power.
  Availability: The proposed algorithm is implemented in MATLAB and in R. The
software, called Tellipsoid, and relevant data sets are available at
http://www.egr.msu.edu/~desaikey
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0802.3458</identifier>
 <datestamp>2010-11-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0802.3458</id><created>2008-02-23</created><updated>2010-11-24</updated><authors><author><keyname>Chen</keyname><forenames>Xinjia</forenames></author></authors><title>Confidence Interval for the Mean of a Bounded Random Variable and Its
  Applications in Point Estimation</title><categories>math.ST math.PR stat.AP stat.TH</categories><comments>7 pages, no figure; added proof of Theorem 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we derive an explicit formula for computing confidence
interval for the mean of a bounded random variable. Moreover, we have developed
multistage point estimation methods for estimating the mean value with
prescribed precision and confidence level based on the proposed confidence
interval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0802.4190</identifier>
 <datestamp>2011-09-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0802.4190</id><created>2008-02-28</created><authors><author><keyname>Gautier</keyname><forenames>Eric</forenames><affiliation>CREST</affiliation></author></authors><title>Bayesian Estimation of Inequalities with Non-Rectangular Censored Survey
  Data</title><categories>stat.AP stat.ME</categories><proxy>ccsd hal-00259011</proxy><journal-ref>Annals Of Applied Statistics 5, 2B (2011) 1632-1656</journal-ref><doi>10.1214/10-AOAS443</doi><abstract>  Synthetic indices are used in Economics to measure various aspects of
monetary inequalities. These scalar indices take as input the distribution over
a finite population, for example the population of a specific country. In this
article we consider the case of the French 2004 Wealth survey. We have at hand
a partial measurement on the distribution of interest consisting of bracketed
and sometimes missing data, over a subsample of the population of interest. We
present in this article the statistical methodology used to obtain point and
interval estimates taking into account the various uncertainties. The
inequality indices being nonlinear in the input distribution, we rely on a
simulation based approach where the model for the wealth per household is
multivariate. Using the survey data as well as matched auxiliary tax
declarations data, we have at hand a quite intricate non-rectangle
multidimensional censoring. For practical issues we use a Bayesian approach.
Inference using Monte-Carlo approximations relies on a Monte-Carlo Markov chain
algorithm namely the Gibbs sampler. The quantities interesting to the decision
maker are taken to be the various inequality indices for the French population.
Their distribution conditional on the data of the subsample are assumed to be
normal centered on the design-based estimates with variance computed through
linearization and taking into account the sample design and total nonresponse.
Exogeneous selection of the subsample, in particular the nonresponse mechanism,
is assumed and we condition on the adequate covariates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0802.4411</identifier>
 <datestamp>2012-07-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0802.4411</id><created>2008-02-29</created><updated>2012-07-02</updated><authors><author><keyname>Malham</keyname><forenames>Simon J. A.</forenames></author><author><keyname>Wiese</keyname><forenames>Anke</forenames></author></authors><title>Chi-square simulation of the CIR process and the Heston model</title><categories>q-fin.CP math.NA math.PR math.ST q-fin.PR stat.TH</categories><comments>32 pages, 6 figures, 8 tables, update</comments><msc-class>60H10, 60H35, 93E20, 91G20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The transition probability of a Cox-Ingersoll-Ross process can be represented
by a non-central chi-square density. First we prove a new representation for
the central chi-square density based on sums of powers of generalized Gaussian
random variables. Second we prove Marsaglia's polar method extends to this
distribution, providing a simple, exact, robust and efficient
acceptance-rejection method for generalized Gaussian sampling and thus central
chi-square sampling. Third we derive a simple, high-accuracy, robust and
efficient direct inversion method for generalized Gaussian sampling based on
the Beasley-Springer-Moro method. Indeed the accuracy of the approximation to
the inverse cumulative distribution function is to the tenth decimal place. We
then apply our methods to non-central chi-square variance sampling in the
Heston model. We focus on the case when the number of degrees of freedom is
small and the zero boundary is attracting and attainable, typical in foreign
exchange markets. Using the additivity property of the chi-square distribution,
our methods apply in all parameter regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0803.0879</identifier>
 <datestamp>2011-02-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0803.0879</id><created>2008-03-06</created><updated>2011-02-15</updated><authors><author><keyname>Hoffmann</keyname><forenames>Marc</forenames></author><author><keyname>Krell</keyname><forenames>Nathalie</forenames></author></authors><title>Statistical analysis of self-similar conservative fragmentation chains</title><categories>math.ST math.PR stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ274 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ274</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 1, 395-423</journal-ref><doi>10.3150/10-BEJ274</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore statistical inference in self-similar conservative fragmentation
chains when only approximate observations of the sizes of the fragments below a
given threshold are available. This framework, introduced by Bertoin and
Martinez [Adv. Appl. Probab. 37 (2005) 553--570], is motivated by mineral
crushing in the mining industry. The underlying object that can be identified
from the data is the step distribution of the random walk associated with a
randomly tagged fragment that evolves along the genealogical tree
representation of the fragmentation process. We compute upper and lower rates
of estimation in a parametric framework and show that in the nonparametric
case, the difficulty of the estimation is comparable to ill-posed linear
inverse problems of order 1 in signal denoising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0803.1276</identifier>
 <datestamp>2010-09-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0803.1276</id><created>2008-03-08</created><authors><author><keyname>Maruyama</keyname><forenames>Yuzo</forenames></author><author><keyname>Strawderman</keyname><forenames>William E.</forenames></author></authors><title>An extended class of minimax generalized Bayes estimators of regression
  coefficients</title><categories>math.ST stat.TH</categories><msc-class>62C20, 62J07</msc-class><journal-ref>Journal of Multivariate Analysis, 100 (2009), 2155-2166</journal-ref><doi>10.1016/j.jmva.2009.06.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive minimax generalized Bayes estimators of regression coefficients in
the general linear model with spherically symmetric errors under invariant
quadratic loss for the case of unknown scale. The class of estimators
generalizes the class considered in Maruyama and Strawderman (2005) to include
non-monotone shrinkage functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0803.1572</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0803.1572</id><created>2008-03-11</created><updated>2011-06-20</updated><authors><author><keyname>Bouzebda</keyname><forenames>Salim</forenames><affiliation>LSTA</affiliation></author><author><keyname>Keziou</keyname><forenames>Amor</forenames><affiliation>LM-Reims</affiliation></author></authors><title>A new test procedure of independence in copula models via
  chi-square-divergence</title><categories>math.ST stat.TH</categories><comments>23 pages (2 figures). Submitted to publication</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new test procedure of independence in the framework of
parametric copulas with unknown marginals. The method is based essentially on
the dual representation of $\chi^2$-divergence on signed finite measures. The
asymptotic properties of the proposed estimate and the test statistic are
studied under the null and alternative hypotheses, with simple and standard
limit distributions both when the parameter is an interior point or not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0803.2443</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0803.2443</id><created>2008-03-17</created><authors><author><keyname>Claussen</keyname><forenames>Jens Christian</forenames></author></authors><title>Discrete stochastic processes, replicator and Fokker-Planck equations of
  coevolutionary dynamics in finite and infinite populations</title><categories>q-bio.PE cond-mat.stat-mech cs.SI math.PR math.ST physics.bio-ph physics.soc-ph stat.TH</categories><comments>Banach Center publications, in press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite-size fluctuations in coevolutionary dynamics arise in models of
biological as well as of social and economic systems. This brief tutorial
review surveys a systematic approach starting from a stochastic process
discrete both in time and state. The limit $N\to \infty$ of an infinite
population can be considered explicitly, generally leading to a replicator-type
equation in zero order, and to a Fokker-Planck-type equation in first order in
$1/\sqrt{N}$. Consequences and relations to some previous approaches are
outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0803.2679</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0803.2679</id><created>2008-03-18</created><updated>2008-09-25</updated><authors><author><keyname>Aletti</keyname><forenames>Giacomo</forenames></author><author><keyname>Bongiorno</keyname><forenames>Enea G.</forenames></author><author><keyname>Capasso</keyname><forenames>Vincenzo</forenames></author></authors><title>Statistical aspects of birth--and--growth stochastic processes</title><categories>stat.AP</categories><comments>simpler notations typos</comments><doi>10.1016/j.fss.2008.12.011</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The paper considers a particular family of set--valued stochastic processes
modeling birth--and--growth processes. The proposed setting allows us to
investigate the nucleation and the growth processes. A decomposition theorem is
established to characterize the nucleation and the growth. As a consequence,
different consistent set--valued estimators are studied for growth process.
Moreover, the nucleation process is studied via the hitting function, and a
consistent estimator of the nucleation hitting function is derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0803.2839</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0803.2839</id><created>2008-03-19</created><updated>2013-03-22</updated><authors><author><keyname>Dalalyan</keyname><forenames>Arnak</forenames><affiliation>LPMA</affiliation></author><author><keyname>Tsybakov</keyname><forenames>Alexandre</forenames><affiliation>LPMA</affiliation></author></authors><title>Aggregation by exponential weighting, sharp PAC-Bayesian bounds and
  sparsity</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><journal-ref>Machine Learning 72, 1-2 (2008) 39-61</journal-ref><doi>10.1007/s10994-008-5051-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of aggregation under the squared loss in the model of
regression with deterministic design. We obtain sharp PAC-Bayesian risk bounds
for aggregates defined via exponential weights, under general assumptions on
the distribution of errors and on the functions to aggregate. We then apply
these results to derive sparsity oracle inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0803.3675</identifier>
 <datestamp>2011-12-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0803.3675</id><created>2008-03-26</created><authors><author><keyname>Kammoun</keyname><forenames>Imen</forenames><affiliation>CES, SAMOS</affiliation></author><author><keyname>Billat</keyname><forenames>Vronique</forenames><affiliation>LEPHE</affiliation></author><author><keyname>Bardet</keyname><forenames>Jean-Marc</forenames><affiliation>CES, SAMOS</affiliation></author></authors><title>A new stochastic process to model Heart Rate series during exhaustive
  run and an estimator of its fractality parameter</title><categories>stat.AP stat.ME</categories><proxy>ccsd hal-00176298</proxy><journal-ref>Journal of Applied Statistics (2011) 1-24</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to interpret and explain the physiological signal behaviors, it can
be interesting to find some constants among the fluctuations of these data
during all the effort or during different stages of the race (which can be
detected using a change points detection method). Several recent papers have
proposed the long-range dependence (Hurst) parameter as such a constant.
However, their results induce two main problems. Firstly, DFA method is usually
applied for estimating this parameter. Clearly, such a method does not provide
the most efficient estimator and moreover it is not at all robust even in the
case of smooth trends. Secondly, this method often gives estimated Hurst
parameters larger than 1, which is the larger possible value for long memory
stationary processes. In this article we propose solutions for both these
problems and we define a new model allowing such estimated parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0804.0510</identifier>
 <datestamp>2012-04-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0804.0510</id><created>2008-04-03</created><updated>2012-04-03</updated><authors><author><keyname>Ryabko</keyname><forenames>Daniil</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Ryabko</keyname><forenames>Boris</forenames><affiliation>SIBSUTI, ICT SBRAS</affiliation></author></authors><title>Nonparametric Statistical Inference for Ergodic Processes</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>Conference version in: D. Ryabko, B. Ryabko, On hypotheses testing
  for ergodic processes, in Proceedgings of Information Theory Workshop, 2008,
  Porto, Portugal, pp. 281-283</comments><proxy>ccsd</proxy><journal-ref>IEEE Transactions on Information Theory 56, 3 (2010) 1430-1435</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work a method for statistical analysis of time series is proposed,
which is used to obtain solutions to some classical problems of mathematical
statistics under the only assumption that the process generating the data is
stationary ergodic. Namely, three problems are considered: goodness-of-fit (or
identity) testing, process classification, and the change point problem. For
each of the problems a test is constructed that is asymptotically accurate for
the case when the data is generated by stationary ergodic processes. The tests
are based on empirical estimates of distributional distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0804.0676</identifier>
 <datestamp>2012-02-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0804.0676</id><created>2008-04-04</created><updated>2010-08-02</updated><authors><author><keyname>Dalalyan</keyname><forenames>Arnak</forenames><affiliation>IGM-LabInfo</affiliation></author><author><keyname>Yoshida</keyname><forenames>Nakahiro</forenames></author></authors><title>Second-order asymptotic expansion for a non-synchronous covariation
  estimator</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><journal-ref>Annales de l'Institut Henri Poincar\'e (B) Probabilit\'es et
  Statistiques 47, 3 (2011) 748-789</journal-ref><doi>10.1214/10-AIHP383</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of estimating the covariation of two
diffusion processes when observations are subject to non-synchronicity.
Building on recent papers \cite{Hay-Yos03, Hay-Yos04}, we derive second-order
asymptotic expansions for the distribution of the Hayashi-Yoshida estimator in
a fairly general setup including random sampling schemes and non-anticipative
random drifts. The key steps leading to our results are a second-order
decomposition of the estimator's distribution in the Gaussian set-up, a
stochastic decomposition of the estimator itself and an accurate evaluation of
the Malliavin covariance. To give a concrete example, we compute the constants
involved in the resulting expansions for the particular case of sampling scheme
generated by two independent Poisson processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0804.0686</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0804.0686</id><created>2008-04-04</created><authors><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author></authors><title>Discrimination of two channels by adaptive methods and its application
  to quantum system</title><categories>quant-ph cs.IT math.IT math.ST stat.TH</categories><journal-ref>IEEE Transactions on Information Theory, Volume 55, Issue 8, 3807
  - 3820 (2009)</journal-ref><doi>10.1109/TIT.2009.2023726</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The optimal exponential error rate for adaptive discrimination of two
channels is discussed. In this problem, adaptive choice of input signal is
allowed. This problem is discussed in various settings. It is proved that
adaptive choice does not improve the exponential error rate in these settings.
These results are applied to quantum state discrimination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0804.1143</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0804.1143</id><created>2008-04-07</created><updated>2013-08-24</updated><authors><author><keyname>Ye</keyname><forenames>Zhishen</forenames></author><author><keyname>Yang</keyname><forenames>Jie</forenames></author></authors><title>Sliced Inverse Moment Regression Using Weighted Chi-Squared Tests for
  Dimension Reduction</title><categories>stat.ME math.ST stat.TH</categories><comments>30 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method for dimension reduction in regression using the first
two inverse moments. We develop corresponding weighted chi-squared tests for
the dimension of the regression. The proposed method considers linear
combinations of Sliced Inverse Regression (SIR) and the method using a new
candidate matrix which is designed to recover the entire inverse second moment
subspace. The optimal combination may be selected based on the p-values derived
from the dimension tests. Theoretically, the proposed method, as well as Sliced
Average Variance Estimate (SAVE), are more capable of recovering the complete
central dimension reduction subspace than SIR and Principle Hessian Directions
(pHd). Therefore it can substitute for SIR, pHd, SAVE, or any linear
combination of them at a theoretical level. Simulation study indicates that the
proposed method may have consistently greater power than SIR, pHd, and SAVE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0804.1399</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0804.1399</id><created>2008-04-08</created><updated>2012-12-04</updated><authors><author><keyname>Chen</keyname><forenames>Xinjia</forenames></author></authors><title>On Estimation and Optimization of Mean Values of Bounded Variables</title><categories>math.ST math.PR stat.AP stat.TH</categories><comments>10 pages, no figure. Generalized to the case of bounded variables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a general approach for probabilistic estimation and
optimization. An explicit formula and a computational approach are established
for controlling the reliability of probabilistic estimation based on a mixed
criterion of absolute and relative errors. By employing the Chernoff-Hoeffding
bound and the concept of sampling, the minimization of a probabilistic function
is transformed into an optimization problem amenable for gradient descendent
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0804.2138</identifier>
 <datestamp>2012-07-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0804.2138</id><created>2008-04-14</created><authors><author><keyname>Lember</keyname><forenames>J.</forenames></author><author><keyname>Koloydenko</keyname><forenames>A.</forenames></author></authors><title>A constructive proof of the existence of Viterbi processes</title><categories>math.ST cs.IT math.IT math.PR stat.CO stat.ML stat.TH</categories><comments>Submitted to the IEEE Transactions on Information Theory, focuses on
  the proofs of the results presented in arXiv:0709.2317, and arXiv:0803.2394</comments><journal-ref>IEEE Transactions on Information Theory, volume 56, issue 4, 2010,
  pages 2017 - 2033</journal-ref><doi>10.1109/TIT.2010.2040897</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the early days of digital communication, hidden Markov models (HMMs)
have now been also routinely used in speech recognition, processing of natural
languages, images, and in bioinformatics. In an HMM $(X_i,Y_i)_{i\ge 1}$,
observations $X_1,X_2,...$ are assumed to be conditionally independent given an
``explanatory'' Markov process $Y_1,Y_2,...$, which itself is not observed;
moreover, the conditional distribution of $X_i$ depends solely on $Y_i$.
Central to the theory and applications of HMM is the Viterbi algorithm to find
{\em a maximum a posteriori} (MAP) estimate $q_{1:n}=(q_1,q_2,...,q_n)$ of
$Y_{1:n}$ given observed data $x_{1:n}$. Maximum {\em a posteriori} paths are
also known as Viterbi paths or alignments. Recently, attempts have been made to
study the behavior of Viterbi alignments when $n\to \infty$. Thus, it has been
shown that in some special cases a well-defined limiting Viterbi alignment
exists. While innovative, these attempts have relied on rather strong
assumptions and involved proofs which are existential. This work proves the
existence of infinite Viterbi alignments in a more constructive manner and for
a very general class of HMMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0804.2937</identifier>
 <datestamp>2011-05-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0804.2937</id><created>2008-04-18</created><updated>2011-04-22</updated><authors><author><keyname>Arlot</keyname><forenames>Sylvain</forenames></author><author><keyname>Bartlett</keyname><forenames>Peter L.</forenames></author></authors><title>Margin-adaptive model selection in statistical learning</title><categories>math.ST stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ288 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ288</report-no><journal-ref>Bernoulli 17, 2 (2011) 687-713</journal-ref><doi>10.3150/10-BEJ288</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classical condition for fast learning rates is the margin condition, first
introduced by Mammen and Tsybakov. We tackle in this paper the problem of
adaptivity to this condition in the context of model selection, in a general
learning framework. Actually, we consider a weaker version of this condition
that allows one to take into account that learning within a small model can be
much easier than within a large one. Requiring this "strong margin adaptivity"
makes the model selection problem more challenging. We first prove, in a
general framework, that some penalization procedures (including local
Rademacher complexities) exhibit this adaptivity when the models are nested.
Contrary to previous results, this holds with penalties that only depend on the
data. Our second main result is that strong margin adaptivity is not always
possible when the models are not nested: for every model selection procedure
(even a randomized one), there is a problem for which it does not demonstrate
strong margin adaptivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0804.3037</identifier>
 <datestamp>2010-10-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0804.3037</id><created>2008-04-18</created><updated>2010-10-11</updated><authors><author><keyname>Fournier</keyname><forenames>Nicolas</forenames></author><author><keyname>Printems</keyname><forenames>Jacques</forenames></author></authors><title>Absolute continuity for some one-dimensional processes</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ215 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ215</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 2, 343-360</journal-ref><doi>10.3150/09-BEJ215</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an elementary method for proving the absolute continuity of the
time marginals of one-dimensional processes. It is based on a comparison
between the Fourier transform of such time marginals with those of the one-step
Euler approximation of the underlying process. We obtain some absolute
continuity results for stochastic differential equations with H\"{o}lder
continuous coefficients. Furthermore, we allow such coefficients to be random
and to depend on the whole path of the solution. We also show how it can be
extended to some stochastic partial differential equations and to some
L\'{e}vy-driven stochastic differential equations. In the cases under study,
the Malliavin calculus cannot be used, because the solution in generally not
Malliavin differentiable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0804.3173</identifier>
 <datestamp>2010-10-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0804.3173</id><created>2008-04-21</created><updated>2010-01-18</updated><authors><author><keyname>Robert</keyname><forenames>Christian P.</forenames></author><author><keyname>Chopin</keyname><forenames>Nicolas</forenames></author><author><keyname>Rousseau</keyname><forenames>Judith</forenames></author></authors><title>Harold Jeffreys's Theory of Probability Revisited</title><categories>math.ST math.HO stat.TH</categories><comments>This paper commented in: [arXiv:1001.2967], [arXiv:1001.2968],
  [arXiv:1001.2970], [arXiv:1001.2975], [arXiv:1001.2985], [arXiv:1001.3073].
  Rejoinder in [arXiv:0909.1008]. Published in at
  http://dx.doi.org/10.1214/09-STS284 the Statistical Science
  (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics
  (http://www.imstat.org)</comments><report-no>IMS-STS-STS284</report-no><journal-ref>Statistical Science (2009), Vol. 24, No. 2, 141-172</journal-ref><doi>10.1214/09-STS284</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Published exactly seventy years ago, Jeffreys's Theory of Probability (1939)
has had a unique impact on the Bayesian community and is now considered to be
one of the main classics in Bayesian Statistics as well as the initiator of the
objective Bayes school. In particular, its advances on the derivation of
noninformative priors as well as on the scaling of Bayes factors have had a
lasting impact on the field. However, the book reflects the characteristics of
the time, especially in terms of mathematical rigor. In this paper we point out
the fundamental aspects of this reference work, especially the thorough
coverage of testing problems and the construction of both estimation and
testing noninformative priors based on functional divergences. Our major aim
here is to help modern readers in navigating in this difficult text and in
concentrating on passages that are still relevant today.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0804.3835</identifier>
 <datestamp>2010-11-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0804.3835</id><created>2008-04-24</created><updated>2010-02-22</updated><authors><author><keyname>Yu</keyname><forenames>Jin</forenames></author><author><keyname>Vishwanathan</keyname><forenames>S. V. N.</forenames></author><author><keyname>Guenter</keyname><forenames>Simon</forenames></author><author><keyname>Schraudolph</keyname><forenames>Nicol N.</forenames></author></authors><title>A Quasi-Newton Approach to Nonsmooth Convex Optimization Problems in
  Machine Learning</title><categories>stat.ML math.OC</categories><journal-ref>Journal of Machine Learning Research 11(Mar):1145-1200, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the well-known BFGS quasi-Newton method and its memory-limited
variant LBFGS to the optimization of nonsmooth convex objectives. This is done
in a rigorous fashion by generalizing three components of BFGS to
subdifferentials: the local quadratic model, the identification of a descent
direction, and the Wolfe line search conditions. We prove that under some
technical conditions, the resulting subBFGS algorithm is globally convergent in
objective function value. We apply its memory-limited variant (subLBFGS) to
L_2-regularized risk minimization with the binary hinge loss. To extend our
algorithm to the multiclass and multilabel settings, we develop a new,
efficient, exact line search algorithm. We prove its worst-case time complexity
bounds, and show that our line search can also be used to extend a recently
developed bundle method to the multiclass and multilabel settings. We also
apply the direction-finding component of our algorithm to L_1-regularized risk
minimization with logistic loss. In all these contexts our methods perform
comparable to or better than specialized state-of-the-art solvers on a number
of publicly available datasets. An open source implementation of our algorithms
is freely available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0805.0053</identifier>
 <datestamp>2011-04-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0805.0053</id><created>2008-05-01</created><authors><author><keyname>Vaswani</keyname><forenames>Namrata</forenames></author></authors><title>Particle Filtering for Large Dimensional State Spaces with Multimodal
  Observation Likelihoods</title><categories>cs.IT math.IT math.ST stat.ME stat.TH</categories><comments>To appear in IEEE Trans. Signal Processing</comments><journal-ref>IEEE Trans. Sig. Proc., vol. 56(10-1), pp. 4583-4597, Oct. 2008</journal-ref><doi>10.1109/TSP.2008.925969</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study efficient importance sampling techniques for particle filtering (PF)
when either (a) the observation likelihood (OL) is frequently multimodal or
heavy-tailed, or (b) the state space dimension is large or both. When the OL is
multimodal, but the state transition pdf (STP) is narrow enough, the optimal
importance density is usually unimodal. Under this assumption, many techniques
have been proposed. But when the STP is broad, this assumption does not hold.
We study how existing techniques can be generalized to situations where the
optimal importance density is multimodal, but is unimodal conditioned on a part
of the state vector. Sufficient conditions to test for the unimodality of this
conditional posterior are derived. The number of particles, N, to accurately
track using a PF increases with state space dimension, thus making any regular
PF impractical for large dimensional tracking problems. We propose a solution
that partially addresses this problem. An important class of large dimensional
problems with multimodal OL is tracking spatially varying physical quantities
such as temperature or pressure in a large area using a network of sensors
which may be nonlinear and/or may have non-negligible failure probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0805.0056</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0805.0056</id><created>2008-05-01</created><updated>2013-11-26</updated><authors><author><keyname>Kong</keyname><forenames>Linglong</forenames></author><author><keyname>Mizera</keyname><forenames>Ivan</forenames></author></authors><title>Quantile tomography: using quantiles with multivariate data</title><categories>stat.ME</categories><journal-ref>Statsitica Sinica, 2012, Vol. 22, No. 4. 1589-1610</journal-ref><doi>10.5705/ss.2010.224</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of quantiles to obtain insights about multivariate data is addressed.
It is argued that incisive insights can be obtained by considering directional
quantiles, the quantiles of projections. Directional quantile envelopes are
proposed as a way to condense this kind of information; it is demonstrated that
they are essentially halfspace (Tukey) depth levels sets, coinciding for
elliptic distributions (in particular multivariate normal) with density
contours. Relevant questions concerning their indexing, the possibility of the
reverse retrieval of directional quantile information, invariance with respect
to affine transformations, and approximation/asymptotic properties are studied.
It is argued that the analysis in terms of directional quantiles and their
envelopes offers a straightforward probabilistic interpretation and thus
conveys a concrete quantitative meaning; the directional definition can be
adapted to elaborate frameworks, like estimation of extreme quantiles and
directional quantile regression, the regression of depth contours on
covariates. The latter facilitates the construction of multivariate growth
charts---the question that motivated all the development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0805.0228</identifier>
 <datestamp>2010-01-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0805.0228</id><created>2008-05-02</created><updated>2010-01-14</updated><authors><author><keyname>Doukhan</keyname><forenames>Paul</forenames></author><author><keyname>Lang</keyname><forenames>Gabriel</forenames></author></authors><title>Evaluation for moments of a ratio with application to regression
  estimation</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ190 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><report-no>IMS-BEJ-BEJ190</report-no><journal-ref>Bernoulli 2009, Vol. 15, No. 4, 1259-1286</journal-ref><doi>10.3150/09-BEJ190</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ratios of random variables often appear in probability and statistical
applications. We aim to approximate the moments of such ratios under several
dependence assumptions. Extending the ideas in Collomb [C. R. Acad. Sci. Paris
285 (1977) 289--292], we propose sharper bounds for the moments of randomly
weighted sums and for the $L^p$-deviations from the asymptotic normal law when
the central limit theorem holds. We indicate suitable applications in finance
and censored data analysis and focus on the applications in the field of
functional estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0805.1404</identifier>
 <datestamp>2011-01-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0805.1404</id><created>2008-05-09</created><updated>2011-01-07</updated><authors><author><keyname>Gin</keyname><forenames>Evarist</forenames></author><author><keyname>Nickl</keyname><forenames>Richard</forenames></author></authors><title>Adaptive estimation of a distribution function and its density in
  sup-norm loss by wavelet and spline projections</title><categories>math.ST math.PR stat.ME stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ239 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ239</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 4, 1137-1163</journal-ref><doi>10.3150/09-BEJ239</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an i.i.d. sample from a distribution $F$ on $\mathbb{R}$ with uniformly
continuous density $p_0$, purely data-driven estimators are constructed that
efficiently estimate $F$ in sup-norm loss and simultaneously estimate $p_0$ at
the best possible rate of convergence over H\"older balls, also in sup-norm
loss. The estimators are obtained by applying a model selection procedure close
to Lepski's method with random thresholds to projections of the empirical
measure onto spaces spanned by wavelets or $B$-splines. The random thresholds
are based on suprema of Rademacher processes indexed by wavelet or spline
projection kernels. This requires Bernstein-type analogs of the inequalities in
Koltchinskii [Ann. Statist. 34 (2006) 2593-2656] for the deviation of suprema
of empirical processes from their Rademacher symmetrizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0805.1805</identifier>
 <datestamp>2010-03-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0805.1805</id><created>2008-05-13</created><authors><author><keyname>Bianchi</keyname><forenames>Gabriele</forenames></author></authors><title>The cross covariogram of a pair of polygons determines both polygons,
  with a few exceptions</title><categories>math.MG math.ST stat.TH</categories><comments>26 pages, 9 figures</comments><msc-class>60D05 (Primary); 52A22, 52A10, 52A38 (Secondary)</msc-class><journal-ref>Adv. in Appl. Math. 42 (2009), 519--544</journal-ref><doi>10.1016/j.aam.2008.10.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cross covariogram g_{K,L} of two convex sets K and L in R^n is the
function which associates to each x in R^n the volume of the intersection of K
and L+x.
  Very recently Averkov and Bianchi [AB] have confirmed Matheron's conjecture
on the covariogram problem, that asserts that any planar convex body K is
determined by the knowledge of g_{K,K}.
  The problem of determining the sets from their covariogram is relevant in
probability, in statistical shape recognition and in the determination of the
atomic structure of a quasicrystal from X-ray diffraction images.
  We prove that when K and L are convex polygons (and also when K and L are
planar convex cones) g_{K,L} determines both K and L, up to a described family
of exceptions. These results imply that, when K and L are in these classes, the
information provided by the cross covariogram is so rich as to determine not
only one unknown body, as required by Matheron's conjecture, but two bodies,
with a few classified exceptions.
  These results are also used by Bianchi [Bia] to prove that any convex
polytope P in R^3 is determined by g_{P,P}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0805.1971</identifier>
 <datestamp>2010-06-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0805.1971</id><created>2008-05-14</created><updated>2008-12-12</updated><authors><author><keyname>Chafai</keyname><forenames>Djalil</forenames><affiliation>UPTE, IMT</affiliation></author><author><keyname>Concordet</keyname><forenames>Didier</forenames><affiliation>UPTE, IMT</affiliation></author></authors><title>Confidence regions for the multinomial parameter with small sample size</title><categories>stat.CO</categories><comments>Accepted for publication in Journal of the American Statistical
  Association (JASA)</comments><proxy>ccsd hal-00278790</proxy><journal-ref>Journal of the American Statistical Association 104, 1071-1079
  (2009)</journal-ref><doi>10.1198/jasa.2009.tm08152</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the observation of n iid realizations of an experiment with d&gt;1
possible outcomes, which corresponds to a single observation of a multinomial
distribution M(n,p) where p is an unknown discrete distribution on {1,...,d}.
In many applications, the construction of a confidence region for p when n is
small is crucial. This concrete challenging problem has a long history. It is
well known that the confidence regions built from asymptotic statistics do not
have good coverage when n is small. On the other hand, most available methods
providing non-asymptotic regions with controlled coverage are limited to the
binomial case d=2. In the present work, we propose a new method valid for any
d&gt;1. This method provides confidence regions with controlled coverage and small
volume, and consists of the inversion of the "covering collection"' associated
with level-sets of the likelihood. The behavior when d/n tends to infinity
remains an interesting open problem beyond the scope of this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0805.2256</identifier>
 <datestamp>2010-10-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0805.2256</id><created>2008-05-15</created><updated>2009-03-28</updated><authors><author><keyname>Beaumont</keyname><forenames>Mark A.</forenames></author><author><keyname>Cornuet</keyname><forenames>Jean-Marie</forenames></author><author><keyname>Marin</keyname><forenames>Jean-Michel</forenames></author><author><keyname>Robert</keyname><forenames>Christian P.</forenames></author></authors><title>Adaptive approximate Bayesian computation</title><categories>stat.CO</categories><comments>8 pages, 2 figures, one algorithm, third revised resubmission to
  Biometrika</comments><journal-ref>Biometrika 96(4), 983-990, 2009</journal-ref><doi>10.1093/biomet/asp052</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequential techniques can enhance the efficiency of the approximate Bayesian
computation algorithm, as in Sisson et al.'s (2007) partial rejection control
version. While this method is based upon the theoretical works of Del Moral et
al. (2006), the application to approximate Bayesian computation results in a
bias in the approximation to the posterior. An alternative version based on
genuine importance sampling arguments bypasses this difficulty, in connection
with the population Monte Carlo method of Cappe et al. (2004), and it includes
an automatic scaling of the forward kernel. When applied to a population
genetics example, it compares favourably with two other versions of the
approximate algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0805.2744</identifier>
 <datestamp>2011-01-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0805.2744</id><created>2008-05-18</created><updated>2009-06-01</updated><authors><author><keyname>Murtagh</keyname><forenames>Fionn</forenames></author></authors><title>Symmetry in Data Mining and Analysis: A Unifying View based on Hierarchy</title><categories>stat.ML math.GM</categories><comments>35 pages, 3 figures, 84 references</comments><journal-ref>Proceedings of Steklov Institute of Mathematics, 265, 177-198,
  2009</journal-ref><doi>10.1134/S0081543809020175</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data analysis and data mining are concerned with unsupervised pattern finding
and structure determination in data sets. The data sets themselves are
explicitly linked as a form of representation to an observational or otherwise
empirical domain of interest. "Structure" has long been understood as symmetry
which can take many forms with respect to any transformation, including point,
translational, rotational, and many others. Beginning with the role of number
theory in expressing data, we show how we can naturally proceed to hierarchical
structures. We show how this both encapsulates traditional paradigms in data
analysis, and also opens up new perspectives towards issues that are on the
order of the day, including data mining of massive, high dimensional,
heterogeneous data sets. Linkages with other fields are also discussed
including computational logic and symbolic dynamics. The structures in data
surveyed here are based on hierarchy, represented as p-adic numbers or an
ultrametric topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0805.2756</identifier>
 <datestamp>2011-01-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0805.2756</id><created>2008-05-18</created><updated>2008-11-16</updated><authors><author><keyname>Murtagh</keyname><forenames>Fionn</forenames></author></authors><title>The Remarkable Simplicity of Very High Dimensional Data: Application of
  Model-Based Clustering</title><categories>stat.ME math.GM</categories><comments>36 pages, 18 figures, 36 references</comments><journal-ref>Journal of Classification, 26 (3), 249-277, 2009</journal-ref><doi>10.1007/s00357-009-9037-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An ultrametric topology formalizes the notion of hierarchical structure. An
ultrametric embedding, referred to here as ultrametricity, is implied by a
hierarchical embedding. Such hierarchical structure can be global in the data
set, or local. By quantifying extent or degree of ultrametricity in a data set,
we show that ultrametricity becomes pervasive as dimensionality and/or spatial
sparsity increases. This leads us to assert that very high dimensional data are
of simple structure. We exemplify this finding through a range of simulated
data cases. We discuss also application to very high frequency time series
segmentation and modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0805.3476</identifier>
 <datestamp>2010-01-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0805.3476</id><created>2008-05-22</created><authors><author><keyname>Bolla</keyname><forenames>Marianna</forenames></author><author><keyname>Friedl</keyname><forenames>Katalin</forenames></author><author><keyname>Kramli</keyname><forenames>Andras</forenames></author></authors><title>Singular value decomposition of large random matrices (for two-way
  classification of microarrays)</title><categories>math.PR math.ST stat.TH</categories><comments>to be submitted to a special ussue of JMVA</comments><msc-class>15A42; 15A52; 60E15</msc-class><journal-ref>JMVA 101 (2010) 434-446</journal-ref><doi>10.1016/j.jmva.2009.09.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asymptotic behavior of the singular value decomposition (SVD) of blown up
matrices and normalized blown up contingency tables exposed to Wigner-noise is
investigated.It is proved that such an m\times n matrix almost surely has a
constant number of large singular values (of order \sqrt{mn}), while the rest
of the singular values are of order \sqrt{m+n} as m,n\to\infty. Concentration
results of Alon et al. for the eigenvalues of large symmetric random matrices
are adapted to the rectangular case, and on this basis, almost sure results for
the singular values as well as for the corresponding isotropic subspaces are
proved. An algorithm, applicable to two-way classification of microarrays, is
also given that finds the underlying block structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0805.3591</identifier>
 <datestamp>2010-06-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0805.3591</id><created>2008-05-23</created><updated>2008-10-20</updated><authors><author><keyname>Neddermeyer</keyname><forenames>Jan C.</forenames></author></authors><title>Computationally Efficient Nonparametric Importance Sampling</title><categories>stat.ME stat.CO</categories><comments>29 pages, 7 figures</comments><journal-ref>Journal of the American Statistical Association, 2009, 104,
  788-802</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The variance reduction established by importance sampling strongly depends on
the choice of the importance sampling distribution. A good choice is often hard
to achieve especially for high-dimensional integration problems. Nonparametric
estimation of the optimal importance sampling distribution (known as
nonparametric importance sampling) is a reasonable alternative to parametric
approaches.In this article nonparametric variants of both the self-normalized
and the unnormalized importance sampling estimator are proposed and
investigated. A common critique on nonparametric importance sampling is the
increased computational burden compared to parametric methods. We solve this
problem to a large degree by utilizing the linear blend frequency polygon
estimator instead of a kernel estimator. Mean square error convergence
properties are investigated leading to recommendations for the efficient
application of nonparametric importance sampling. Particularly, we show that
nonparametric importance sampling asymptotically attains optimal importance
sampling variance. The efficiency of nonparametric importance sampling
algorithms heavily relies on the computational efficiency of the employed
nonparametric estimator. The linear blend frequency polygon outperforms kernel
estimators in terms of certain criteria such as efficient sampling and
evaluation. Furthermore, it is compatible with the inversion method for sample
generation. This allows to combine our algorithms with other variance reduction
techniques such as stratified sampling. Empirical evidence for the usefulness
of the suggested algorithms is obtained by means of three benchmark integration
problems. As an application we estimate the distribution of the queue length of
a spam filter queueing system based on real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0805.3912</identifier>
 <datestamp>2011-09-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0805.3912</id><created>2008-05-26</created><authors><author><keyname>Aletti</keyname><forenames>Giacomo</forenames></author><author><keyname>Bongiorno</keyname><forenames>Enea G.</forenames></author><author><keyname>Capasso</keyname><forenames>Vincenzo</forenames></author></authors><title>A set-valued framework for birth-and-growth process</title><categories>math.PR math.ST stat.AP stat.TH</categories><doi>10.1051/ps/2010009</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose a set-valued framework for the well-posedness of birth-and-growth
process. Our birth-and-growth model is rigorously defined as a suitable
combination, involving Minkowski sum and Aumann integral, of two very general
set-valued processes representing nucleation and growth respectively. The
simplicity of the used geometrical approach leads us to avoid problems arising
by an analytical definition of the front growth such as boundary regularities.
In this framework, growth is generally anisotropic and, according to a
mesoscale point of view, it is not local, i.e. for a fixed time instant, growth
is the same at each space point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0805.4154</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0805.4154</id><created>2008-05-27</created><updated>2009-04-24</updated><authors><author><keyname>Lan</keyname><forenames>Xiaohong</forenames></author><author><keyname>Marinucci</keyname><forenames>Domenico</forenames></author></authors><title>On The Dependence Structure of Wavelet Coefficients for Spherical Random
  Fields</title><categories>math.ST astro-ph math.PR stat.ME stat.TH</categories><comments>Revised version for Stochastic Processes and their Applications</comments><msc-class>60G60 (Primary); 62M40, 42C40, 42C10 (Secondary)</msc-class><journal-ref>Stochastic Processes and their Applications, Vol. 119 (2009), pp.
  3749-3766</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the correlation structure of the random coefficients for a wide
class of wavelet systems on the sphere (Mexican needlets) which were recently
introduced in the literature by Geller and Mayeli (2007). We provide necessary
and sufficient conditions for these coefficients to be asymptotic uncorrelated
in the real and in the frequency domain. Here, the asymptotic theory is
developed in the high resolution sense. Statistical applications are also
discussed, in particular with reference to the analysis of cosmological data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0805.4226</identifier>
 <datestamp>2010-09-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0805.4226</id><created>2008-05-27</created><updated>2008-06-15</updated><authors><author><keyname>Jang</keyname><forenames>Dennis</forenames></author><author><keyname>Kang</keyname><forenames>Jung Uk</forenames></author><author><keyname>Kruckman</keyname><forenames>Alex</forenames></author><author><keyname>Kudo</keyname><forenames>Jun</forenames></author><author><keyname>Miller</keyname><forenames>Steven J.</forenames></author></authors><title>Chains of distributions, hierarchical Bayesian models and Benford's Law</title><categories>math.PR math.ST stat.TH</categories><comments>15 pages, second draft: added some additional remarks on connections
  to Hierarchical Bayes and MCMC, fixed some typos, added additional
  explanations</comments><msc-class>11K06, 60A10 (Primary), 62F99 (Secondary)</msc-class><journal-ref>Journal of Algebra, Number Theory: Advances and Applications,
  volume 1, number 1 (March 2009), 37--60</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kossovsky recently conjectured that the distribution of leading digits of a
chain of probability distributions converges to Benford's law as the length of
the chain grows. We prove his conjecture in many cases, and provide an
interpretation in terms of products of independent random variables and a
central limit theorem. An interesting consequence is that in hierarchical
Bayesian models priors tend to satisfy Benford's Law as the number of levels of
the hierarchy increases, which allows us to develop some simple tests (based on
Benford's law) to test proposed models. We give explicit formulas for the error
terms as sums of Mellin transforms, which converges extremely rapidly as the
number of terms in the chain grows. We may interpret our results as showing
that certain Markov chain Monte Carlo processes are rapidly mixing to Benford's
law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0805.4373</identifier>
 <datestamp>2011-02-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0805.4373</id><created>2008-05-28</created><updated>2011-02-10</updated><authors><author><keyname>Das</keyname><forenames>Bikramjit</forenames></author><author><keyname>Resnick</keyname><forenames>Sidney I.</forenames></author></authors><title>Conditioning on an extreme component: Model consistency with regular
  variation on cones</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ271 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ271</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 1, 226-252</journal-ref><doi>10.3150/10-BEJ271</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multivariate extreme value theory assumes a multivariate domain of attraction
condition for the distribution of a random vector. This necessitates that each
component satisfies a marginal domain of attraction condition. An approximation
of the joint distribution of a random vector obtained by conditioning on one of
the components being extreme was developed by Heffernan and Tawn [12] and
further studied by Heffernan and Resnick [11]. These papers left unresolved the
consistency of different models obtained by conditioning on different
components being extreme and we here provide clarification of this issue. We
also clarify the relationship between these conditional distributions,
multivariate extreme value theory and standard regular variation on cones of
the form $[0,\infty]\times(0,\infty]$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0806.0582</identifier>
 <datestamp>2012-07-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0806.0582</id><created>2008-06-03</created><authors><author><keyname>Bustos</keyname><forenames>O. H.</forenames></author><author><keyname>Flesia</keyname><forenames>A. G.</forenames></author><author><keyname>Frery</keyname><forenames>A. C.</forenames></author><author><keyname>Lucini</keyname><forenames>M. M.</forenames></author></authors><title>Sampling Spatially Correlated Clutter</title><categories>stat.ME</categories><comments>First draft</comments><journal-ref>Communications in Statistics - Simulation and Computation, 2009,
  2134--2151</journal-ref><doi>10.1080/03610910903249536</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlated ${\cal G}$ distributions can be used to describe the clutter seen
in images obtained with coherent illumination, as is the case of B-scan
ultrasound, laser, sonar and synthetic aperture radar (SAR) imagery. These
distributions are derived using the square root of the generalized inverse
Gaussian distribution for the amplitude backscatter within the multiplicative
model. A two-parameters particular case of the amplitude ${\mathcal G}$
distribution, called ${\mathcal G}_{A}^{0}$, constitutes a modeling improvement
with respect to the widespread ${\mathcal K}_{A}$ distribution when fitting
urban, forested and deforested areas in remote sensing data. This article deals
with the modeling and the simulation of correlated ${\mathcal
G}_{A}^{0}$-distributed random fields. It is accomplished by means of the
Inverse Transform method, applied to Gaussian random fields with spatial
correlation. The main feature of this approach is its generality, since it
allows the introduction of negative correlation values in the resulting
process, necessary for the proper explanation of the shadowing effect in many
SAR images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0806.1141</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0806.1141</id><created>2008-06-06</created><authors><author><keyname>Bai</keyname><forenames>Zhidong</forenames><affiliation>KLASMOE, Dsap</affiliation></author><author><keyname>Yao</keyname><forenames>Jian-Feng</forenames><affiliation>IRMAR</affiliation></author></authors><title>Limit theorems for sample eigenvalues in a generalized spiked population
  model</title><categories>math.ST math.PR stat.TH</categories><comments>24 pages; 4 figures</comments><proxy>ccsd hal-00284468</proxy><msc-class>Primary 60F15, 60F05, Secondary 15A52, 62H25 62H25</msc-class><journal-ref>Journal of Multivariate Analysis, 2012, volume 106, pp. 167-177</journal-ref><doi>10.1016/j.jmva.2011.10.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the spiked population model introduced by Johnstone (2001),the population
covariance matrix has all its eigenvalues equal to unit except for a few fixed
eigenvalues (spikes). The question is to quantify the effect of the
perturbation caused by the spike eigenvalues. Baik and Silverstein (2006)
establishes the almost sure limits of the extreme sample eigenvalues associated
to the spike eigenvalues when the population and the sample sizes become large.
In a recent work (Bai and Yao, 2008), we have provided the limiting
distributions for these extreme sample eigenvalues. In this paper, we extend
this theory to a {\em generalized} spiked population model where the base
population covariance matrix is arbitrary, instead of the identity matrix as in
Johnstone's case. New mathematical tools are introduced for establishing the
almost sure convergence of the sample eigenvalues generated by the spikes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0806.1631</identifier>
 <datestamp>2011-11-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0806.1631</id><created>2008-06-10</created><updated>2009-04-23</updated><authors><author><keyname>De Giuli</keyname><forenames>Maria Elena</forenames><affiliation>Department of Business Research, University of Pavia</affiliation></author><author><keyname>Maggi</keyname><forenames>Mario Alessandro</forenames><affiliation>Department of Business Research, University of Pavia</affiliation></author><author><keyname>Tarantola</keyname><forenames>Claudia</forenames><affiliation>Department of Economics and Quantitative Methods, University of Pavia, Italy</affiliation></author></authors><title>Bayesian outlier detection in Capital Asset Pricing Model</title><categories>stat.AP</categories><journal-ref>statistical Modelling ,2010, 10, 379--390</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel Bayesian optimisation procedure for outlier detection in
the Capital Asset Pricing Model. We use a parametric product partition model to
robustly estimate the systematic risk of an asset. We assume that the returns
follow independent normal distributions and we impose a partition structure on
the parameters of interest. The partition structure imposed on the parameters
induces a corresponding clustering of the returns. We identify via an
optimisation procedure the partition that best separates standard observations
from the atypical ones. The methodology is illustrated with reference to a real
data set, for which we also provide a microeconomic interpretation of the
detected outliers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0806.1652</identifier>
 <datestamp>2010-03-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0806.1652</id><created>2008-06-10</created><updated>2010-02-01</updated><authors><author><keyname>Ptscher</keyname><forenames>Benedikt M.</forenames></author><author><keyname>Schneider</keyname><forenames>Ulrike</forenames></author></authors><title>Confidence Sets Based on Penalized Maximum Likelihood Estimators in
  Gaussian Regression</title><categories>math.ST stat.ME stat.ML stat.TH</categories><comments>second revision: new title, some comments added, proofs moved to
  appendix</comments><journal-ref>Electron. J. Statist. 4 (2010), 334-360</journal-ref><doi>10.1214/09-EJS523</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Confidence intervals based on penalized maximum likelihood estimators such as
the LASSO, adaptive LASSO, and hard-thresholding are analyzed. In the
known-variance case, the finite-sample coverage properties of such intervals
are determined and it is shown that symmetric intervals are the shortest. The
length of the shortest intervals based on the hard-thresholding estimator is
larger than the length of the shortest interval based on the adaptive LASSO,
which is larger than the length of the shortest interval based on the LASSO,
which in turn is larger than the standard interval based on the maximum
likelihood estimator. In the case where the penalized estimators are tuned to
possess the `sparsity property', the intervals based on these estimators are
larger than the standard interval by an order of magnitude. Furthermore, a
simple asymptotic confidence interval construction in the `sparse' case, that
also applies to the smoothly clipped absolute deviation estimator, is
discussed. The results for the known-variance case are shown to carry over to
the unknown-variance case in an appropriate asymptotic sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0806.2305</identifier>
 <datestamp>2010-12-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0806.2305</id><created>2008-06-13</created><updated>2009-07-10</updated><authors><author><keyname>Lorenz</keyname><forenames>Jan</forenames></author></authors><title>Universality in movie rating distributions</title><categories>physics.soc-ph physics.data-an stat.AP</categories><comments>8 pages, 5 figures, accepted for publication</comments><journal-ref>European Physical Journal B (2009), 71, 251-258</journal-ref><doi>10.1140/epjb/e2009-00283-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper histograms of user ratings for movies (1,...,10) are analysed.
The evolving stabilised shapes of histograms follow the rule that all are
either double- or triple-peaked. Moreover, at most one peak can be on the
central bins 2,...,9 and the distribution in these bins looks smooth
`Gaussian-like' while changes at the extremes (1 and 10) often look abrupt. It
is shown that this is well approximated under the assumption that histograms
are confined and discretised probability density functions of L\'evy skew
alpha-stable distributions. These distributions are the only stable
distributions which could emerge due to a generalized central limit theorem
from averaging of various independent random avriables as which one can see the
initial opinions of users. Averaging is also an appropriate assumption about
the social process which underlies the process of continuous opinion formation.
Surprisingly, not the normal distribution achieves the best fit over histograms
obseved on the web, but distributions with fat tails which decay as power-laws
with exponent -(1+alpha) (alpha=4/3). The scale and skewness parameters of the
Levy skew alpha-stable distributions seem to depend on the deviation from an
average movie (with mean about 7.6). The histogram of such an average movie has
no skewness and is the most narrow one. If a movie deviates from average the
distribution gets broader and skew. The skewness pronounces the deviation. This
is used to construct a one parameter fit which gives some evidence of
universality in processes of continuous opinion dynamics about taste.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0806.2426</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0806.2426</id><created>2008-06-15</created><updated>2010-09-13</updated><authors><author><keyname>Fougres</keyname><forenames>Anne-Laure</forenames><affiliation>ICJ</affiliation></author><author><keyname>Soulier</keyname><forenames>Philippe</forenames><affiliation>MODAL'X</affiliation></author></authors><title>Estimation of conditional laws given an extreme component</title><categories>math.ST stat.TH</categories><comments>32 pages, 5 figure</comments><proxy>ccsd</proxy><journal-ref>Extremes 15, 1 (2012) 1-34</journal-ref><doi>10.1007/s10687-010-0122-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $(X,Y)$ be a bivariate random vector. The estimation of a probability of
the form $P(Y\leq y \mid X &gt;t) $ is challenging when $t$ is large, and a
fruitful approach consists in studying, if it exists, the limiting conditional
distribution of the random vector $(X,Y)$, suitably normalized, given that $X$
is large. There already exists a wide literature on bivariate models for which
this limiting distribution exists. In this paper, a statistical analysis of
this problem is done. Estimators of the limiting distribution (which is assumed
to exist) and the normalizing functions are provided, as well as an estimator
of the conditional quantile function when the conditioning event is extreme.
Consistency of the estimators is proved and a functional central limit theorem
for the estimator of the limiting distribution is obtained. The small sample
behavior of the estimator of the conditional quantile function is illustrated
through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0806.2933</identifier>
 <datestamp>2010-11-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0806.2933</id><created>2008-06-18</created><updated>2010-11-11</updated><authors><author><keyname>Saksman</keyname><forenames>Eero</forenames></author><author><keyname>Vihola</keyname><forenames>Matti</forenames></author></authors><title>On the ergodicity of the adaptive Metropolis algorithm on unbounded
  domains</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AAP682 the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP682</report-no><journal-ref>Annals of Applied Probability 2010, Vol. 20, No. 6, 2178-2203</journal-ref><doi>10.1214/10-AAP682</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes sufficient conditions to ensure the correct ergodicity
of the Adaptive Metropolis (AM) algorithm of Haario, Saksman and Tamminen
[Bernoulli 7 (2001) 223--242] for target distributions with a noncompact
support. The conditions ensuring a strong law of large numbers require that the
tails of the target density decay super-exponentially and have regular
contours. The result is based on the ergodicity of an auxiliary process that is
sequentially constrained to feasible adaptation sets, independent estimates of
the growth rate of the AM chain and the corresponding geometric drift
constants. The ergodicity result of the constrained process is obtained through
a modification of the approach due to Andrieu and Moulines [Ann. Appl. Probab.
16 (2006) 1462--1505].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0806.3286</identifier>
 <datestamp>2010-10-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0806.3286</id><created>2008-06-19</created><updated>2010-10-07</updated><authors><author><keyname>Chipman</keyname><forenames>Hugh A.</forenames></author><author><keyname>George</keyname><forenames>Edward I.</forenames></author><author><keyname>McCulloch</keyname><forenames>Robert E.</forenames></author></authors><title>BART: Bayesian additive regression trees</title><categories>stat.ME stat.AP stat.ML</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOAS285 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS285</report-no><journal-ref>Annals of Applied Statistics 2010, Vol. 4, No. 1, 266-298</journal-ref><doi>10.1214/09-AOAS285</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a Bayesian "sum-of-trees" model where each tree is constrained by
a regularization prior to be a weak learner, and fitting and inference are
accomplished via an iterative Bayesian backfitting MCMC algorithm that
generates samples from a posterior. Effectively, BART is a nonparametric
Bayesian regression approach which uses dimensionally adaptive random basis
elements. Motivated by ensemble methods in general, and boosting algorithms in
particular, BART is defined by a statistical model: a prior and a likelihood.
This approach enables full posterior inference including point and interval
estimates of the unknown regression function as well as the marginal effects of
potential predictors. By keeping track of predictor inclusion frequencies, BART
can also be used for model-free variable selection. BART's many features are
illustrated with a bake-off against competing methods on 42 different data
sets, with a simulation experiment and on a drug discovery classification
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0806.3371</identifier>
 <datestamp>2010-10-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0806.3371</id><created>2008-06-20</created><authors><author><keyname>Comte</keyname><forenames>Fabienne</forenames><affiliation>MAP5</affiliation></author><author><keyname>Genon-Catalot</keyname><forenames>Valentine</forenames><affiliation>MAP5</affiliation></author></authors><title>Nonparametric adaptive estimation for pure jump L\'evy processes</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00289364</proxy><report-no>MAP5 2008-12</report-no><journal-ref>Annales de l'Institut Henri Poincare (B) Probability and
  Statistics 46, 3 (2010) 595-617</journal-ref><doi>10.1214/09-AIHP323</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with nonparametric estimation of the L\'evy density
of a pure jump L\'evy process. The sample path is observed at $n$ discrete
instants with fixed sampling interval. We construct a collection of estimators
obtained by deconvolution methods and deduced from appropriate estimators of
the characteristic function and its first derivative. We obtain a bound for the
${\mathbb L}^2$-risk, under general assumptions on the model. Then we propose a
penalty function that allows to build an adaptive estimator. The risk bound for
the adaptive estimator is obtained under additional assumptions on the L\'evy
density. Examples of models fitting in our framework are described and rates of
convergence of the estimator are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0806.3474</identifier>
 <datestamp>2013-05-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0806.3474</id><created>2008-06-20</created><updated>2009-09-29</updated><authors><author><keyname>Ensslin</keyname><forenames>Torsten A.</forenames></author><author><keyname>Frommert</keyname><forenames>Mona</forenames></author><author><keyname>Kitaura</keyname><forenames>Francisco S.</forenames></author></authors><title>Information field theory for cosmological perturbation reconstruction
  and non-linear signal analysis</title><categories>astro-ph cs.IT hep-th math.IT physics.data-an stat.CO</categories><comments>38 pages, 6 figures, LaTeX; version accepted by PRD</comments><report-no>J-MPA2270e</report-no><doi>10.1103/PhysRevD.80.105005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop information field theory (IFT) as a means of Bayesian inference on
spatially distributed signals, the information fields. A didactical approach is
attempted. Starting from general considerations on the nature of measurements,
signals, noise, and their relation to a physical reality, we derive the
information Hamiltonian, the source field, propagator, and interaction terms.
Free IFT reproduces the well known Wiener-filter theory. Interacting IFT can be
diagrammatically expanded, for which we provide the Feynman rules in position-,
Fourier-, and spherical harmonics space, and the Boltzmann-Shannon information
measure. The theory should be applicable in many fields. However, here, two
cosmological signal recovery problems are discussed in their IFT-formulation.
1) Reconstruction of the cosmic large-scale structure matter distribution from
discrete galaxy counts in incomplete galaxy surveys within a simple model of
galaxy formation. We show that a Gaussian signal, which should resemble the
initial density perturbations of the Universe, observed with a strongly
non-linear, incomplete and Poissonian-noise affected response, as the processes
of structure and galaxy formation and observations provide, can be
reconstructed thanks to the virtue of a response-renormalization flow equation.
2) We design a filter to detect local non-linearities in the cosmic microwave
background, which are predicted from some Early-Universe inflationary
scenarios, and expected due to measurement imperfections. This filter is the
optimal Bayes' estimator up to linear order in the non-linearity parameter and
can be used even to construct sky maps of non-linearities in the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0806.3769</identifier>
 <datestamp>2011-08-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0806.3769</id><created>2008-06-23</created><updated>2011-08-04</updated><authors><author><keyname>Melo</keyname><forenames>Tatiane F. N.</forenames></author><author><keyname>Ferrari</keyname><forenames>Silvia L. P.</forenames></author><author><keyname>Cribari-Neto</keyname><forenames>Francisco</forenames></author></authors><title>Improved testing inference in mixed linear models</title><categories>stat.ME math.ST stat.TH</categories><comments>17 pages, 1 figure</comments><journal-ref>Computational Statistics and Data Analysis, 53, (2009), 2573-2582</journal-ref><doi>10.1016/j.csda.2008.12.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixed linear models are commonly used in repeated measures studies. They
account for the dependence amongst observations obtained from the same
experimental unit. Oftentimes, the number of observations is small, and it is
thus important to use inference strategies that incorporate small sample
corrections. In this paper, we develop modified versions of the likelihood
ratio test for fixed effects inference in mixed linear models. In particular,
we derive a Bartlett correction to such a test and also to a test obtained from
a modified profile likelihood function. Our results generalize those in Zucker
et al. (Journal of the Royal Statistical Society B, 2000, 62, 827-838) by
allowing the parameter of interest to be vector-valued. Additionally, our
Bartlett corrections allow for random effects nonlinear covariance matrix
structure. We report numerical evidence which shows that the proposed tests
display superior finite sample behavior relative to the standard likelihood
ratio test. An application is also presented and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0806.4048</identifier>
 <datestamp>2011-08-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0806.4048</id><created>2008-06-25</created><updated>2008-12-25</updated><authors><author><keyname>Sumi</keyname><forenames>Toshio</forenames></author><author><keyname>Miyazaki</keyname><forenames>Mitsuhiro</forenames></author><author><keyname>Sakata</keyname><forenames>Toshio</forenames></author></authors><title>About the maximal rank of 3-tensors over the real and the complex number
  field</title><categories>math.RA math.ST stat.TH</categories><comments>13 pages, no figure v2: correction and improvement</comments><msc-class>15A69, 15A72, 14Q99, 14M12, 14M99</msc-class><journal-ref>Ann. Inst. Stat. Math. 62, 807-822, 2010</journal-ref><doi>10.1007/s10463-010-0294-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High dimensional array data, tensor data, is becoming important in recent
days. Then maximal rank of tensors is important in theory and applications. In
this paper we consider the maximal rank of 3 tensors. It can be attacked from
various viewpoints, however, we trace the method of Atkinson-Stephens(1979) and
Atkinson-Lloyd(1980). They treated the problem in the complex field, and we
will present various bounds over the real field by proving several lemmas and
propositions, which is real counterparts of their results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0806.4168</identifier>
 <datestamp>2012-07-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0806.4168</id><created>2008-06-25</created><authors><author><keyname>Slater</keyname><forenames>Paul B.</forenames></author></authors><title>Established Clustering Procedures for Network Analysis</title><categories>physics.soc-ph cs.SI physics.data-an stat.AP</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In light of the burgeoning interest in network analysis in the new millenium,
we bring to the attention of contemporary network theorists, a two-stage
double-standarization and hierarchical clustering (single-linkage-like)
procedure devised in 1974. In its many applications over the next
decade--primarily to the migration flows between geographic subdivisions within
nations--the presence was often revealed of ``hubs''. These are, typically,
``cosmopolitan/non-provincial'' areas--such as the French capital, Paris--which
send and receive people relatively broadly across their respective nations.
Additionally, this two-stage procedure--which ``might very well be the most
successful application of cluster analysis'' (R. C. Dubes)--has detected many
(physically or socially) isolated groups (regions) of areas, such as those
forming the southern islands, Shikoku and Kyushu, of Japan, the Italian islands
of Sardinia and Sicily, and the New England region of the United States.
Further, we discuss a (complementary) approach developed in 1976, involving the
application of the max-flow/min-cut theorem to raw/non-standardized flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0806.4730</identifier>
 <datestamp>2012-11-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0806.4730</id><created>2008-06-28</created><updated>2008-11-07</updated><authors><author><keyname>Chernozhukov</keyname><forenames>Victor</forenames></author><author><keyname>Fernandez-Val</keyname><forenames>Ivan</forenames></author><author><keyname>Galichon</keyname><forenames>Alfred</forenames></author></authors><title>Improving Point and Interval Estimates of Monotone Functions by
  Rearrangement</title><categories>math.ST math.FA stat.ME stat.TH</categories><comments>24 pages, 4 figures, 3 tables</comments><journal-ref>Biometrika (2009) 96 (3): 559-575</journal-ref><doi>10.1093/biomet/asp030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that a target function is monotonic, namely, weakly increasing, and
an available original estimate of this target function is not weakly
increasing. Rearrangements, univariate and multivariate, transform the original
estimate to a monotonic estimate that always lies closer in common metrics to
the target function. Furthermore, suppose an original simultaneous confidence
interval, which covers the target function with probability at least
$1-\alpha$, is defined by an upper and lower end-point functions that are not
weakly increasing. Then the rearranged confidence interval, defined by the
rearranged upper and lower end-point functions, is shorter in length in common
norms than the original interval and also covers the target function with
probability at least $1-\alpha$. We demonstrate the utility of the improved
point and interval estimates with an age-height growth chart example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0806.4864</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0806.4864</id><created>2008-06-30</created><updated>2011-06-21</updated><authors><author><keyname>Bouzebda</keyname><forenames>Salim</forenames><affiliation>LSTA</affiliation></author><author><keyname>Keziou</keyname><forenames>Amor</forenames></author></authors><title>New estimates and tests of independence in some copula models</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00292051</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce new estimates and tests of independence in copula models with
unknown margins using $\phi$-divergences and the duality technique. The
asymptotic laws of the estimates and the test statistics are established both
when the parameter is an interior or a boundary value of the parameter space.
Simulation results show that the choice of $\chi^2$-divergence has good
properties in terms of efficiency-robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0807.0053</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0807.0053</id><created>2008-06-30</created><authors><author><keyname>Shimodaira</keyname><forenames>Hidetoshi</forenames></author></authors><title>Frequentist and Bayesian measures of confidence via multiscale bootstrap
  for testing three regions</title><categories>stat.ME math.ST stat.TH</categories><journal-ref>Annals of the Institute of Statistical Mathematics 2010, Vol. 62,
  pp. 189-208</journal-ref><doi>10.1007/s10463-009-0247-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new computation method of frequentist $p$-values and Bayesian posterior
probabilities based on the bootstrap probability is discussed for the
multivariate normal model with unknown expectation parameter vector. The null
hypothesis is represented as an arbitrary-shaped region. We introduce new
parametric models for the scaling-law of bootstrap probability so that the
multiscale bootstrap method, which was designed for one-sided test, can also
computes confidence measures of two-sided test, extending applicability to a
wider class of hypotheses. Parameter estimation is improved by the two-step
multiscale bootstrap and also by including higher-order terms. Model selection
is important not only as a motivating application of our method, but also as an
essential ingredient in the method. A compromise between frequentist and
Bayesian is attempted by showing that the Bayesian posterior probability with
an noninformative prior is interpreted as a frequentist $p$-value of
``zero-sided'' test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0807.1208</identifier>
 <datestamp>2010-06-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0807.1208</id><created>2008-07-08</created><updated>2010-06-18</updated><authors><author><keyname>Chronopoulou</keyname><forenames>Alexandra</forenames><affiliation>LPP</affiliation></author><author><keyname>Viens</keyname><forenames>Frederi</forenames><affiliation>LPP</affiliation></author><author><keyname>Tudor</keyname><forenames>Ciprian</forenames><affiliation>LPP</affiliation></author></authors><title>Self-similarity parameter estimation and reproduction property for
  non-Gaussian Hermite processes</title><categories>math.PR math.ST stat.TH</categories><comments>To appear in "Communications on Stochastic Analysis"</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the class of all the Hermite processes $(Z_{t}^{(q,H)})_{t\in
\lbrack 0,1]}$ of order $q\in \mathbf{N}^{\ast}$ and with Hurst parameter $%
H\in (\frac{1}{2},1)$. The process $Z^{(q,H)}$ is $H$-selfsimilar, it has
stationary increments and it exhibits long-range dependence identical to that
of fractional Brownian motion (fBm). For $q=1$, $Z^{(1,H)}$ is fBm, which is
Gaussian; for $q=2$, $Z^{(2,H)}$ is the Rosenblatt process, which lives in the
second Wiener chaos; for any $q&gt;2$, $Z^{(q,H)}$ is a process in the $q$th
Wiener chaos. We study the variations of $Z^{(q,H)}$ for any $q$, by using
multiple Wiener -It\^{o} stochastic integrals and Malliavin calculus. We prove
a reproduction property for this class of processes in the sense that the terms
appearing in the chaotic decomposition of their variations give rise to other
Hermite processes of different orders and with different Hurst parameters. We
apply our results to construct a strongly consistent estimator for the
self-similarity parameter $H$ from discrete observations of $Z^{(q,H)}$; the
asymptotics of this estimator, after appropriate normalization, are proved to
be distributed like a Rosenblatt random variable (value at time $1$ of a
Rosenblatt process).with self-similarity parameter $1+2(H-1)/q$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0807.1550</identifier>
 <datestamp>2012-07-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0807.1550</id><created>2008-07-10</created><updated>2008-07-23</updated><authors><author><keyname>Slater</keyname><forenames>Paul B.</forenames></author></authors><title>Discernment of Hubs and Clusters in Socioeconomic Networks</title><categories>physics.soc-ph cs.SI physics.data-an stat.AP</categories><comments>17 pages, small mathematical expression for the probability 0.973469
  now correctly written (mid. p. 9)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interest in the analysis of networks has grown rapidly in the new millennium.
Consequently, we promote renewed attention to a certain methodological approach
introduced in 1974. Over the succeeding decade, this
two-stage--double-standardization and hierarchical clustering
(single-linkage-like)--procedure was applied to a wide variety of weighted,
directed networks of a socioeconomic nature, frequently revealing the presence
of ``hubs''. These were, typically--in the numerous instances studied of
migration flows between geographic subdivisions within
nations--``cosmopolitan/non-provincial'' areas, a prototypical example being
the French capital, Paris. Such locations emit and absorb people broadly across
their respective nations. Additionally, the two-stage procedure--which ``might
very well be the most successful application of cluster analysis'' (R. C.
Dubes, 1985)--detected many (physically or socially) isolated, functional
groups (regions) of areas, such as the southern islands, Shikoku and Kyushu, of
Japan, the Italian islands of Sardinia and Sicily, and the New England region
of the United States. Further, we discuss a (complementary) approach developed
in 1976, in which the max-flow/min-cut theorem was applied to
raw/non-standardized (interindustry, as well as migration) flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0807.1574</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0807.1574</id><created>2008-07-10</created><updated>2008-10-15</updated><authors><author><keyname>Kabaila</keyname><forenames>Paul</forenames></author><author><keyname>Giri</keyname><forenames>Khageswor</forenames></author></authors><title>Large-Sample Confidence Intervals for the Treatment Difference in a
  Two-Period Crossover Trial, Utilizing Prior Information</title><categories>stat.ME math.ST stat.TH</categories><comments>This version is the same as v1. This paper has been accepted for
  publication in Statistics and Probability Letters</comments><journal-ref>Statistics and Probability Letters, 79, 652-658 (2009)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a two-treatment, two-period crossover trial, with responses that are
continuous random variables. We find a large-sample frequentist 1-alpha
confidence interval for the treatment difference that utilizes the uncertain
prior information that there is no differential carryover effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0807.1750</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0807.1750</id><created>2008-07-10</created><updated>2012-07-13</updated><authors><author><keyname>Landim</keyname><forenames>C.</forenames></author><author><keyname>Portugal</keyname><forenames>R. D.</forenames></author><author><keyname>Svaiter</keyname><forenames>B. F.</forenames></author></authors><title>A Markovian growth dynamics on rooted binary trees evolving according to
  the Gompertz curve</title><categories>q-bio.CB math-ph math.MP q-bio.QM stat.OT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by biological dynamics, we consider a growth Markov process taking
values on the space of rooted binary trees, similar to the Aldous-Shields
model. Fix $n\ge 1$ and $\beta&gt;0$. We start at time 0 with the tree composed of
a root only. At any time, each node with no descendants, independently from the
other nodes, produces two successors at rate $\beta(n-k)/n$, where $k$ is the
distance from the node to the root. Denote by $Z_n(t)$ the number of nodes with
no descendants at time $t$ and let $T_n = \beta^{-1} n \ln(n /\ln 4) + (\ln
2)/(2 \beta)$. We prove that $2^{-n} Z_n(T_n + n \tau)$, $\tau\in\bb R$,
converges to the Gompertz curve $\exp (- (\ln 2) e^{-\beta \tau})$. We also
prove a central limit theorem for the martingale associated to $Z_n(t)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0807.2153</identifier>
 <datestamp>2011-06-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0807.2153</id><created>2008-07-14</created><updated>2011-06-21</updated><authors><author><keyname>Bouzebda</keyname><forenames>Salim</forenames><affiliation>LSTA</affiliation></author><author><keyname>Elhattab</keyname><forenames>Issam</forenames><affiliation>LSTA</affiliation></author></authors><title>Uniform-in-bandwidth consistency for kernel-type estimators of Shannon's
  entropy</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish uniform-in-bandwidth consistency for kernel-type estimators of
the differential entropy. We consider two kernel-type estimators of Shannon's
entropy. As a consequence, an asymptotic 100% confidence interval of entropy is
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0807.2245</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0807.2245</id><created>2008-07-14</created><updated>2009-02-21</updated><authors><author><keyname>Duembgen</keyname><forenames>Lutz</forenames><affiliation>University of Bern</affiliation></author><author><keyname>van de Geer</keyname><forenames>Sara</forenames><affiliation>ETH, Zurich</affiliation></author><author><keyname>Veraar</keyname><forenames>Mark</forenames><affiliation>Delft University of Technology</affiliation></author><author><keyname>Wellner</keyname><forenames>Jon A.</forenames><affiliation>University of Washington, Seattle</affiliation></author></authors><title>Nemirovski's Inequalities Revisited</title><categories>math.ST stat.TH</categories><comments>23 pages, 1 figure. Revision for American Mathematical Monthly,
  February 2009. Mark Veraar added as co-author</comments><msc-class>60B11, 60E15 (Primary), 60G50 (Secondary)</msc-class><journal-ref>The American Mathematical Monthly 117(2) (2011)</journal-ref><doi>10.1524/stnd.2011.1073</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important tool for statistical research are moment inequalities for sums
of independent random vectors. Nemirovski and coworkers (1983, 2000) derived
one particular type of such inequalities: For certain Banach spaces
$(\B,\|\cdot\|)$ there exists a constant $K = K(\B,\|\cdot\|)$ such that for
arbitrary independent and centered random vectors $X_1, X_2, ..., X_n \in \B$,
their sum $S_n$ satisfies the inequality $ E \|S_n \|^2 \le K \sum_{i=1}^n E
\|X_i\|^2$. We present and compare three different approaches to obtain such
inequalities: Nemirovski's results are based on deterministic inequalities for
norms. Another possible vehicle are type and cotype inequalities, a tool from
probability theory on Banach spaces. Finally, we use a truncation argument plus
Bernstein's inequality to obtain another version of the moment inequality
above. Interestingly, all three approaches have their own merits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0807.2832</identifier>
 <datestamp>2010-11-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0807.2832</id><created>2008-07-17</created><updated>2010-02-18</updated><authors><author><keyname>Spiliopoulos</keyname><forenames>Konstantinos</forenames></author></authors><title>Method of Moments Estimation of Ornstein-Uhlenbeck Processes Driven by
  General L\'{e}vy Process</title><categories>math.PR math.ST stat.TH</categories><comments>15 pages, 4 Postscript figures; corrected minor typos, final
  published version</comments><journal-ref>ANNALES de l'I.S.U.P. Vol 53 - Fascicule 2-3, 2009, pp. 3-19</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ornstein-Uhlenbeck processes driven by general L\'{e}vy process are
considered in this paper. We derive strongly consistent estimators for the
moments of the underlying L\'{e}vy process and for the mean reverting parameter
of the Ornstein-Uhlenbeck process. Moreover, we prove that the estimators are
asymptotically normal. Finally, we test the empirical performance of our
estimators in a simulation study and we fit the model to real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0807.2900</identifier>
 <datestamp>2011-02-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0807.2900</id><created>2008-07-18</created><authors><author><keyname>Richards</keyname><forenames>Joseph W.</forenames></author><author><keyname>Freeman</keyname><forenames>Peter E.</forenames></author><author><keyname>Lee</keyname><forenames>Ann B.</forenames></author><author><keyname>Schafer</keyname><forenames>Chad M.</forenames></author></authors><title>Exploiting Low-Dimensional Structure in Astronomical Spectra</title><categories>astro-ph stat.AP</categories><comments>24 pages, 8 figures</comments><journal-ref>Astrophys.J.691:32-42,2009</journal-ref><doi>10.1088/0004-637X/691/1/32</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dimension-reduction techniques can greatly improve statistical inference in
astronomy. A standard approach is to use Principal Components Analysis (PCA).
In this work we apply a recently-developed technique, diffusion maps, to
astronomical spectra for data parameterization and dimensionality reduction,
and develop a robust, eigenmode-based framework for regression. We show how our
framework provides a computationally efficient means by which to predict
redshifts of galaxies, and thus could inform more expensive redshift estimators
such as template cross-correlation. It also provides a natural means by which
to identify outliers (e.g., misclassified spectra, spectra with anomalous
features). We analyze 3835 SDSS spectra and show how our framework yields a
more than 95% reduction in dimensionality. Finally, we show that the prediction
error of the diffusion map-based regression approach is markedly smaller than
that of a similar approach based on PCA, clearly demonstrating the superiority
of diffusion maps over PCA for this regression task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0807.3677</identifier>
 <datestamp>2012-02-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0807.3677</id><created>2008-07-23</created><updated>2012-02-15</updated><authors><author><keyname>Taylor</keyname><forenames>Lorna</forenames></author><author><keyname>Trenkel</keyname><forenames>Verena M.</forenames></author><author><keyname>Kupca</keyname><forenames>Vojtech</forenames></author><author><keyname>Stefansson</keyname><forenames>Gunnar</forenames></author></authors><title>A bootstrap method for estimating bias and variance in statistical
  multispecies models using highly disparate data sets</title><categories>stat.AP</categories><comments>Submitted for formal publication. Withdrawn due to journal
  requirments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical multispecies models of multiarea marine ecosystems use a variety
of data sources to estimate parameters using composite or weighted likelihood
functions with associated weighting issues and questions on how to obtain
variance estimates. Regardless of the method used to obtain point estimates, a
method is needed for variance estimation. A bootstrap technique is introduced
for the evaluation of uncertainty in such models, taking into account inherent
spatial and temporal correlations in the data sets thus avoiding many
model--specification issues, which are commonly transferred as assumptions from
a likelihood estimation procedure into Hessian--based variance estimation
procedures. The technique is demonstrated on a real data set and used to look
for estimation bias and the effects of different aggregation levels in
population dynamics models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0807.4003</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0807.4003</id><created>2008-07-25</created><updated>2013-12-09</updated><authors><author><keyname>Gelman</keyname><forenames>Andrew</forenames></author><author><keyname>Cai</keyname><forenames>Cexun Jeffrey</forenames></author></authors><title>Should the democrats move to the left on economic policy?</title><categories>stat.AP</categories><comments>Published in at http://dx.doi.org/10.1214/07-AOAS150 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org). With Corrections</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS150</report-no><journal-ref>Annals of Applied Statistics 2008, Vol. 2, No. 2, 536-549</journal-ref><doi>10.1214/07-AOAS150</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Could John Kerry have gained votes in the 2004 Presidential election by more
clearly distinguishing himself from George Bush on economic policy? At first
thought, the logic of political preferences would suggest not: the Republicans
are to the right of most Americans on economic policy, and so in a
one-dimensional space with party positions measured with no error, the optimal
strategy for the Democrats would be to stand infinitesimally to the left of the
Republicans. The median voter theorem suggests that each party should keep its
policy positions just barely distinguishable from the opposition. In a
multidimensional setting, however, or when voters vary in their perceptions of
the parties' positions, a party can benefit from putting some daylight between
itself and the other party on an issue where it has a public-opinion advantage
(such as economic policy for the Democrats). We set up a plausible theoretical
model in which the Democrats could achieve a net gain in votes by moving to the
left on economic policy, given the parties' positions on a range of issue
dimensions. We then evaluate this model based on survey data on voters'
perceptions of their own positions and those of the candidates in 2004. Under
our model, it turns out to be optimal for the Democrats to move slightly to the
right but staying clearly to the left of the Republicans' current position on
economic issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0807.5059</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0807.5059</id><created>2008-07-31</created><authors><author><keyname>Baldi</keyname><forenames>P.</forenames></author><author><keyname>Kerkyacharian</keyname><forenames>G.</forenames></author><author><keyname>Marinucci</keyname><forenames>D.</forenames></author><author><keyname>Picard</keyname><forenames>D.</forenames></author></authors><title>Adaptive density estimation for directional data using needlets</title><categories>math.ST astro-ph stat.TH</categories><comments>30 pages, 9 figures</comments><msc-class>62G07; 62G20; 65T60</msc-class><journal-ref>Annals of Statistics, Vol. 37, (2009), pp. 3362-3395</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with density estimation of directional data on the
sphere. We introduce a procedure based on thresholding on a new type of
spherical wavelets called {\it needlets}. We establish a minimax result and
prove its optimality. We are motivated by astrophysical applications, in
particular in connection with the analysis of ultra high energy cosmic rays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0807.5096</identifier>
 <datestamp>2011-08-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0807.5096</id><created>2008-07-31</created><authors><author><keyname>Fa</keyname><forenames>Gilles</forenames></author></authors><title>Moment bounds for non-linear functionals of the periodogram</title><categories>math.ST stat.TH</categories><doi>10.1016/j.spa.2010.02.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove the validity of the Edgeworth expansion of the
Discrete Fourier transforms of some linear time series. This result is applied
to approach moments of non linear functionals of the periodogram. As an
illustration, we give an expression of the mean square error of the Geweke and
Porter-Hudak estimator of the long memory parameter. We prove that this
estimator is rate optimal, extending the result of Giraitis, Robinson, Samarov
(1997) from Gaussian to linear processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0808.0383</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0808.0383</id><created>2008-08-04</created><updated>2012-12-18</updated><authors><author><keyname>Young</keyname><forenames>Derek S.</forenames></author></authors><title>An Overview of Mixture Models</title><categories>math.ST stat.TH</categories><comments>Portions of this review article need to be enhanced with further
  explanation. Some of this material will be incorporated into a future
  manuscript</comments><proxy>vtex</proxy><report-no>IMS-SS-SS_2008_38</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn. With the advancement of statistical theory and
computing power, data sets are providing a greater amount of insight into the
problems of today. Statisticians have an ever increasing number of tools to
attack these problems, some of which can be implemented in the area of mixture
modeling. There is a great deal of literature on mixture models and this work
attempts to provide a general overview of the subject, including the discussion
of relevant issues and algorithms. The reader can hope to gain a broad
understanding of concepts in mixture modeling and find the references cited
within as a valuable resource for the next stage of their research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0808.0711</identifier>
 <datestamp>2011-03-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0808.0711</id><created>2008-08-05</created><updated>2011-03-07</updated><authors><author><keyname>Obozinski</keyname><forenames>Guillaume</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Support union recovery in high-dimensional multivariate regression</title><categories>stat.ML math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS776 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS776</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 1, 1-47</journal-ref><doi>10.1214/09-AOS776</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multivariate regression, a $K$-dimensional response vector is regressed
upon a common set of $p$ covariates, with a matrix $B^*\in\mathbb{R}^{p\times
K}$ of regression coefficients. We study the behavior of the multivariate group
Lasso, in which block regularization based on the $\ell_1/\ell_2$ norm is used
for support union recovery, or recovery of the set of $s$ rows for which $B^*$
is nonzero. Under high-dimensional scaling, we show that the multivariate group
Lasso exhibits a threshold for the recovery of the exact row pattern with high
probability over the random design and noise that is specified by the sample
complexity parameter $\theta(n,p,s):=n/[2\psi(B^*)\log(p-s)]$. Here $n$ is the
sample size, and $\psi(B^*)$ is a sparsity-overlap function measuring a
combination of the sparsities and overlaps of the $K$-regression coefficient
vectors that constitute the model. We prove that the multivariate group Lasso
succeeds for problem sequences $(n,p,s)$ such that $\theta(n,p,s)$ exceeds a
critical level $\theta_u$, and fails for sequences such that $\theta(n,p,s)$
lies below a critical level $\theta_{\ell}$. For the special case of the
standard Gaussian ensemble, we show that $\theta_{\ell}=\theta_u$ so that the
characterization is sharp. The sparsity-overlap function $\psi(B^*)$ reveals
that, if the design is uncorrelated on the active rows, $\ell_1/\ell_2$
regularization for multivariate regression never harms performance relative to
an ordinary Lasso approach and can yield substantial improvements in sample
complexity (up to a factor of $K$) when the coefficient vectors are suitably
orthogonal. For more general designs, it is possible for the ordinary Lasso to
outperform the multivariate group Lasso. We complement our analysis with
simulations that demonstrate the sharpness of our theoretical results, even for
relatively small problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0808.1001</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0808.1001</id><created>2008-08-07</created><updated>2012-03-28</updated><authors><author><keyname>Ballas</keyname><forenames>Dimitris</forenames></author><author><keyname>Tranmer</keyname><forenames>Mark</forenames></author></authors><title>Happy places or happy people? A multi-level modelling approach to the
  analysis of happiness and well-being</title><categories>stat.AP stat.ME</categories><comments>This paper has been withdrawn by the authors as it presented work in
  progress at the time. A significantly revised and improved version of this
  work has now been published in 2012 in the journal International Regional
  Science Review, vol 35(1), pp 70-102</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to enhance our understanding of substantive questions
regarding self-reported happiness and well-being through the specification and
use of multi-level models. To date, there have been numerous quantitative
research studies of the happiness of individuals, based on single-level
regression models, where typically a happiness index is related to a set of
explanatory variables. There are also several single-level studies comparing
aggregate happiness levels between countries. Nevertheless, there have been
very few studies that attempt to simultaneously take into account variations in
happiness and well-being at several different levels, such as individual,
household, and area. Here, multilevel models are used with data from the
British Household Panel Survey to assess the nature and extent of variations in
happiness and well-being to determine the relative importance of the area
(district, region), household and individual characteristics on these outcomes.
Moreover, having taken into account the characteristics at these different
levels in the multilevel models, the paper shows how it is possible to identify
any areas that are associated with especially positive or negative feelings of
happiness and well-being.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0808.1416</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0808.1416</id><created>2008-08-10</created><updated>2013-01-19</updated><authors><author><keyname>Birg</keyname><forenames>Lucien</forenames></author></authors><title>Model selection for density estimation with L2-loss</title><categories>math.ST stat.TH</categories><comments>37 pages. Minor changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider here estimation of an unknown probability density s belonging to
L2(mu) where mu is a probability measure. We have at hand n i.i.d. observations
with density s and use the squared L2-norm as our loss function. The purpose of
this paper is to provide an abstract but completely general method for
estimating s by model selection, allowing to handle arbitrary families of
finite-dimensional (possibly non-linear) models and any density s belonging to
L2(mu). We shall, in particular, consider the cases of unbounded densities and
bounded densities with unknown bound and investigate how the L-infinity-norm of
s may influence the risk. We shall also provide applications to adaptive
estimation and aggregation of preliminary estimators. Although of a purely
theoretical nature, our method leads to results that cannot presently be
reached by more concrete methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0808.2902</identifier>
 <datestamp>2012-01-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0808.2902</id><created>2008-08-21</created><updated>2012-01-09</updated><authors><author><keyname>Robert</keyname><forenames>Christian</forenames></author><author><keyname>Casella</keyname><forenames>George</forenames></author></authors><title>A Short History of Markov Chain Monte Carlo: Subjective Recollections
  from Incomplete Data</title><categories>stat.CO stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/10-STS351 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS351</report-no><journal-ref>Statistical Science 2011, Vol. 26, No. 1, 102-115</journal-ref><doi>10.1214/10-STS351</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We attempt to trace the history and development of Markov chain Monte Carlo
(MCMC) from its early inception in the late 1940s through its use today. We see
how the earlier stages of Monte Carlo (MC, not MCMC) research have led to the
algorithms currently in use. More importantly, we see how the development of
this methodology has not only changed our solutions to problems, but has
changed the way we think about problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0808.3495</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0808.3495</id><created>2008-08-26</created><updated>2014-04-22</updated><authors><author><keyname>Vlasiou</keyname><forenames>Maria</forenames></author><author><keyname>Palmowski</keyname><forenames>Zbigniew</forenames></author></authors><title>Tail asymptotics for a random sign Lindley recursion</title><categories>math.PR math.ST stat.TH</categories><comments>12 pages, 22 references</comments><msc-class>60F10</msc-class><journal-ref>Journal of Applied Probability, 47(1), 72-83, 2010</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We investigate the tail behaviour of the steady state distribution of a
stochastic recursion that generalises Lindley's recursion. This recursion
arises in queuing systems with dependent interarrival and service times, and
includes alternating service systems and carousel storage systems as special
cases. We obtain precise tail asymptotics in three qualitatively different
cases, and compare these with existing results for Lindley's recursion and for
alternating service systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.0490</identifier>
 <datestamp>2011-05-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.0490</id><created>2008-09-02</created><updated>2011-05-09</updated><authors><author><keyname>Gorban</keyname><forenames>A. N.</forenames></author><author><keyname>Zinovyev</keyname><forenames>A. Y.</forenames></author></authors><title>Principal Graphs and Manifolds</title><categories>cs.LG cs.NE stat.ML</categories><comments>36 pages, 6 figures, minor corrections</comments><journal-ref>Handbook of Research on Machine Learning Applications and Trends:
  Algorithms, Methods and Techniques, Ch. 2, Information Science Reference,
  2009. 28-59</journal-ref><doi>10.4018/978-1-60566-766-9</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In many physical, statistical, biological and other investigations it is
desirable to approximate a system of points by objects of lower dimension
and/or complexity. For this purpose, Karl Pearson invented principal component
analysis in 1901 and found 'lines and planes of closest fit to system of
points'. The famous k-means algorithm solves the approximation problem too, but
by finite sets instead of lines and planes. This chapter gives a brief
practical introduction into the methods of construction of general principal
objects, i.e. objects embedded in the 'middle' of the multidimensional data
set. As a basis, the unifying framework of mean squared distance approximation
of finite datasets is selected. Principal graphs and manifolds are constructed
as generalisations of principal components and k-means principal points. For
this purpose, the family of expectation/maximisation algorithms with nearest
generalisations is presented. Construction of principal graphs with controlled
complexity is based on the graph grammar approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.0492</identifier>
 <datestamp>2011-01-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.0492</id><created>2008-09-02</created><authors><author><keyname>Murtagh</keyname><forenames>Fionn</forenames></author></authors><title>From Data to the p-Adic or Ultrametric Model</title><categories>stat.ML stat.AP</categories><comments>15 pages, 6 figures. To appear in: Proceedings of Third International
  Conference on p-Adic Mathematical Physics: From Planck Scale Physics to
  Complex Systems to Biology, Steklov Mathematics Institute, Russian Academy of
  Sciences</comments><journal-ref>p-Adic Numbers, Ultrametric Analysis and Applications, 1, 58-68,
  2009</journal-ref><doi>10.1134/S2070046609010063</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We model anomaly and change in data by embedding the data in an ultrametric
space. Taking our initial data as cross-tabulation counts (or other input data
formats), Correspondence Analysis allows us to endow the information space with
a Euclidean metric. We then model anomaly or change by an induced ultrametric.
The induced ultrametric that we are particularly interested in takes a
sequential - e.g. temporal - ordering of the data into account. We apply this
work to the flow of narrative expressed in the film script of the Casablanca
movie; and to the evolution between 1988 and 2004 of the Colombian social
conflict and violence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.0660</identifier>
 <datestamp>2010-11-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.0660</id><created>2008-09-03</created><updated>2009-09-23</updated><authors><author><keyname>Chretien</keyname><forenames>Stephane</forenames></author></authors><title>An Alternating l1 approach to the compressed sensing problem</title><categories>stat.ME stat.CO</categories><comments>7 pages, 1 figure, presented at ICIAM 07</comments><journal-ref>IEEE Signal Processing Letters, Feb. 2010 Volume : 17 , Issue: 2
  On page(s): 181 - 184</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing is a new methodology for constructing sensors which allow
sparse signals to be efficiently recovered using only a small number of
observations. The recovery problem can often be stated as the one of finding
the solution of an underdetermined system of linear equations with the smallest
possible support. The most studied relaxation of this hard combinatorial
problem is the $l_1$-relaxation consisting of searching for solutions with
smallest $l_1$-norm. In this short note, based on the ideas of Lagrangian
duality, we introduce an alternating $l_1$ relaxation for the recovery problem
enjoying higher recovery rates in practice than the plain $l_1$ relaxation and
the recent reweighted $l_1$ method of Cand\`es, Wakin and Boyd.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.0686</identifier>
 <datestamp>2010-06-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.0686</id><created>2008-09-03</created><updated>2010-06-08</updated><authors><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Yukich</keyname><forenames>Joseph E.</forenames></author><author><keyname>Tong</keyname><forenames>Lang</forenames></author><author><keyname>Swami</keyname><forenames>Ananthram</forenames></author></authors><title>Energy Scaling Laws for Distributed Inference in Random Fusion Networks</title><categories>cs.IT cs.NI math.IT math.ST stat.TH</categories><comments>IEEE JSAC on Stochastic Geometry and Random Graphs for Wireless
  Networks</comments><journal-ref>vol. 27, no. 7, pp.1203-1217, Sept. 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The energy scaling laws of multihop data fusion networks for distributed
inference are considered. The fusion network consists of randomly located
sensors distributed i.i.d. according to a general spatial distribution in an
expanding region. Among the class of data fusion schemes that enable optimal
inference at the fusion center for Markov random field (MRF) hypotheses, the
scheme with minimum average energy consumption is bounded below by average
energy of fusion along the minimum spanning tree, and above by a suboptimal
scheme, referred to as Data Fusion for Markov Random Fields (DFMRF). Scaling
laws are derived for the optimal and suboptimal fusion policies. It is shown
that the average asymptotic energy of the DFMRF scheme is finite for a class of
MRF models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.0815</identifier>
 <datestamp>2011-06-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.0815</id><created>2008-09-04</created><updated>2011-05-29</updated><authors><author><keyname>Juditsky</keyname><forenames>Anatoli</forenames><affiliation>LJK</affiliation></author><author><keyname>Nemirovskii</keyname><forenames>Arkadii S.</forenames><affiliation>ISyE</affiliation></author><author><keyname>Tauvel</keyname><forenames>Claire</forenames><affiliation>LJK</affiliation></author></authors><title>Solving variational inequalities with Stochastic Mirror-Prox algorithm</title><categories>math.OC math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider iterative methods for stochastic variational
inequalities (s.v.i.) with monotone operators. Our basic assumption is that the
operator possesses both smooth and nonsmooth components. Further, only noisy
observations of the problem data are available. We develop a novel Stochastic
Mirror-Prox (SMP) algorithm for solving s.v.i. and show that with the
convenient stepsize strategy it attains the optimal rates of convergence with
respect to the problem parameters. We apply the SMP algorithm to Stochastic
composite minimization and describe particular applications to Stochastic
Semidefinite Feasability problem and Eigenvalue minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.0853</identifier>
 <datestamp>2011-01-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.0853</id><created>2008-09-04</created><updated>2009-04-22</updated><authors><author><keyname>Nguyen</keyname><forenames>XuanLong</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Estimating divergence functionals and the likelihood ratio by convex
  risk minimization</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>28 pages</comments><journal-ref>IEEE Transactions on Information Theory, 56(11), 5847--5861, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop and analyze $M$-estimation methods for divergence functionals and
the likelihood ratios of two probability distributions. Our method is based on
a non-asymptotic variational characterization of $f$-divergences, which allows
the problem of estimating divergences to be tackled via convex empirical risk
optimization. The resulting estimators are simple to implement, requiring only
the solution of standard convex programs. We present an analysis of consistency
and convergence for these estimators. Given conditions only on the ratios of
densities, we show that our estimators can achieve optimal minimax rates for
the likelihood ratio and the divergence functionals in certain regimes. We
derive an efficient optimization algorithm for computing our estimates, and
illustrate their convergence behavior and practical viability by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.0974</identifier>
 <datestamp>2010-03-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.0974</id><created>2008-09-05</created><updated>2009-01-26</updated><authors><author><keyname>Beran</keyname><forenames>Rudolf</forenames></author><author><keyname>Duembgen</keyname><forenames>Lutz</forenames></author></authors><title>Least Squares and Shrinkage Estimation under Bimonotonicity Constraints</title><categories>stat.CO stat.ME</categories><journal-ref>Statistics and Computing, Volume 20, Number 2 (2010), pp. 177-189</journal-ref><doi>10.1007/s11222-009-9124-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe active set type algorithms for minimization of a
smooth function under general order constraints, an important case being
functions on the set of bimonotone r-by-s matrices. These algorithms can be
used, for instance, to estimate a bimonotone regression function via least
squares or (a smooth approximation of) least absolute deviations. Another
application is shrinkage estimation in image denoising or, more generally,
regression problems with two ordinal factors after representing the data in a
suitable basis which is indexed by pairs (i,j) in {1,...,r}x{1,...,s}. Various
numerical examples illustrate our methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.1241</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.1241</id><created>2008-09-08</created><updated>2012-12-04</updated><authors><author><keyname>Chen</keyname><forenames>Xinjia</forenames></author></authors><title>A New Framework of Multistage Estimation</title><categories>math.ST cs.LG math.PR stat.ME stat.TH</categories><comments>254 pages, no figure; added more references; main results appeared in
  Proceedings of SPIE, Orlando, Florida, USA, April 2010 and 2011</comments><doi>10.1103/PhysRevE.79.026307</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have established a unified framework of multistage
parameter estimation. We demonstrate that a wide variety of statistical
problems such as fixed-sample-size interval estimation, point estimation with
error control, bounded-width confidence intervals, interval estimation
following hypothesis testing, construction of confidence sequences, can be cast
into the general framework of constructing sequential random intervals with
prescribed coverage probabilities. We have developed exact methods for the
construction of such sequential random intervals in the context of multistage
sampling. In particular, we have established inclusion principle and coverage
tuning techniques to control and adjust the coverage probabilities of
sequential random intervals. We have obtained concrete sampling schemes which
are unprecedentedly efficient in terms of sampling effort as compared to
existing procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.1431</identifier>
 <datestamp>2011-07-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.1431</id><created>2008-09-09</created><updated>2011-07-18</updated><authors><author><keyname>Griffiths</keyname><forenames>Robert C.</forenames></author><author><keyname>Span</keyname><forenames>Dario</forenames></author></authors><title>Multivariate Jacobi and Laguerre polynomials, infinite-dimensional
  extensions, and their probabilistic connections with multivariate Hahn and
  Meixner polynomials</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ305 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ305</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 3, 1095-1125</journal-ref><doi>10.3150/10-BEJ305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multivariate versions of classical orthogonal polynomials such as Jacobi,
Hahn, Laguerre and Meixner are reviewed and their connection explored by
adopting a probabilistic approach. Hahn and Meixner polynomials are interpreted
as posterior mixtures of Jacobi and Laguerre polynomials, respectively. By
using known properties of gamma point processes and related transformations, a
new infinite-dimensional version of Jacobi polynomials is constructed with
respect to the size-biased version of the Poisson--Dirichlet weight measure and
to the law of the gamma point process from which it is derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.1873</identifier>
 <datestamp>2011-04-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.1873</id><created>2008-09-10</created><authors><author><keyname>Barreto-Souza</keyname><forenames>Wagner</forenames></author><author><keyname>Cordeiro</keyname><forenames>Gauss M.</forenames></author><author><keyname>Simas</keyname><forenames>Alexandre B.</forenames></author></authors><title>Some results for beta Fr\'echet distribution</title><categories>stat.ME</categories><journal-ref>Communications in Statistics. Theory and Methods, v. 40, p.
  798-811, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nadarajah and Gupta (2004) introduced the beta Fr\'echet (BF) distribution,
which is a generalization of the exponentiated Fr\'echet (EF) and Fr\'echet
distributions, and obtained the probability density and cumulative distribution
functions. However, they do not investigated its moments and the order
statistics. In this paper the BF density function and the density function of
the order statistics are expressed as linear combinations of Fr\'echet density
functions. This is important to obtain some mathematical properties of the BF
distribution in terms of the corresponding properties of the Fr\'echet
distribution. We derive explicit expansions for the ordinary moments and
L-moments and obtain the order statistics and their moments. We also discuss
maximum likelihood estimation and calculate the information matrix which was
not known. The information matrix is easily numerically determined. Two
applications to real data sets are given to illustrate the potentiality of this
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.1889</identifier>
 <datestamp>2010-08-17</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.1889</id><created>2008-09-10</created><authors><author><keyname>Barreto-Souza</keyname><forenames>Wagner</forenames></author><author><keyname>Santos</keyname><forenames>Alessandro H. S.</forenames></author><author><keyname>Cordeiro</keyname><forenames>Gauss M.</forenames></author></authors><title>The Beta Generalized Exponential Distribution</title><categories>stat.ME stat.CO</categories><journal-ref>Journal of Statistical Computation and Simulation, 80 , 159 - 172.
  (2010)</journal-ref><doi>10.1080/00949650802552402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the beta generalized exponential distribution that includes the
beta exponential and generalized exponential distributions as special cases. We
provide a comprehensive mathematical treatment of this distribution. We derive
the moment generating function and the $r$th moment thus generalizing some
results in the literature. Expressions for the density, moment generating
function and $r$th moment of the order statistics also are obtained. We discuss
estimation of the parameters by maximum likelihood and provide the information
matrix. We observe in one application to real data set that this model is quite
flexible and can be used quite effectively in analyzing positive data in place
of the beta exponential and generalized exponential distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.1894</identifier>
 <datestamp>2010-08-17</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.1894</id><created>2008-09-10</created><authors><author><keyname>Barreto-Souza</keyname><forenames>Wagner</forenames></author><author><keyname>Cribari-Neto</keyname><forenames>Francisco</forenames></author></authors><title>A Generalization of the Exponential-Poisson Distribution</title><categories>stat.ME stat.AP</categories><journal-ref>Statistics &amp; Probability Letters, 79, 2493-2500. (2009)</journal-ref><doi>10.1016/j.spl.2009.09.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The two-parameter distribution known as exponential-Poisson (EP)
distribution, which has decreasing failure rate, was introduced by Kus (2007).
In this paper we generalize the EP distribution and show that the failure rate
of the new distribution can be decreasing or increasing. The failure rate can
also be upside-down bathtub shaped. A comprehensive mathematical treatment of
the new distribution is provided. We provide closed-form expressions for the
density, cumulative distribution, survival and failure rate functions; we also
obtain the density of the $i$th order statistic. We derive the $r$th raw moment
of the new distribution and also the moments of order statistics. Moreover, we
discuss estimation by maximum likelihood and obtain an expression for Fisher's
information matrix. Furthermore, expressions for the R\'enyi and Shannon
entropies are given and estimation of the stress-strength parameter is
discussed. Applications using two real data sets are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.2274</identifier>
 <datestamp>2010-06-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.2274</id><created>2008-09-12</created><updated>2009-07-05</updated><authors><author><keyname>Rokhlin</keyname><forenames>Vladimir</forenames></author><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author><author><keyname>Tygert</keyname><forenames>Mark</forenames></author></authors><title>A randomized algorithm for principal component analysis</title><categories>stat.CO</categories><comments>26 pages, 6 tables, 1 figure; to appear in the SIAM Journal on Matrix
  Analysis and Applications</comments><report-no>UCLA Computational and Applied Math Technical Report 08-60</report-no><journal-ref>A randomized algorithm for principal component analysis, SIAM
  Journal on Matrix Analysis and Applications, 31 (3): 1100-1124, 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal component analysis (PCA) requires the computation of a low-rank
approximation to a matrix containing the data being analyzed. In many
applications of PCA, the best possible accuracy of any rank-deficient
approximation is at most a few digits (measured in the spectral norm, relative
to the spectral norm of the matrix being approximated). In such circumstances,
efficient algorithms have not come with guarantees of good accuracy, unless one
or both dimensions of the matrix being approximated are small. We describe an
efficient algorithm for the low-rank approximation of matrices that produces
accuracy very close to the best possible, for matrices of arbitrary sizes. We
illustrate our theoretical results via several numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.2402</identifier>
 <datestamp>2010-10-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.2402</id><created>2008-09-15</created><updated>2010-10-11</updated><authors><author><keyname>Mendo</keyname><forenames>Luis</forenames></author><author><keyname>Hernando</keyname><forenames>Jos M.</forenames></author></authors><title>Estimation of a probability with optimum guaranteed confidence in
  inverse binomial sampling</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ219 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ219</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 2, 493-513</journal-ref><doi>10.3150/09-BEJ219</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequential estimation of a probability $p$ by means of inverse binomial
sampling is considered. For $\mu_1,\mu_2&gt;1$ given, the accuracy of an estimator
$\hat{p}$ is measured by the confidence level $P[p/\mu_2\leq\hat{p}\leq
p\mu_1]$. The confidence levels $c_0$ that can be guaranteed for $p$ unknown,
that is, such that $P[p/\mu_2\leq \hat{p}\leq p\mu_1]\geq c_0$ for all
$p\in(0,1)$, are investigated. It is shown that within the general class of
randomized or non-randomized estimators based on inverse binomial sampling,
there is a maximum $c_0$ that can be guaranteed for arbitrary $p$. A
non-randomized estimator is given that achieves this maximum guaranteed
confidence under mild conditions on $\mu_1$, $\mu_2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.2650</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.2650</id><created>2008-09-16</created><updated>2010-05-31</updated><authors><author><keyname>Juditsky</keyname><forenames>Anatoli</forenames><affiliation>LMC - IMAG</affiliation></author><author><keyname>Nemirovski</keyname><forenames>Arkadii S.</forenames><affiliation>ISyE</affiliation></author></authors><title>On Verifiable Sufficient Conditions for Sparse Signal Recovery via
  $\ell_1$ Minimization</title><categories>math.OC math.ST stat.TH</categories><proxy>ccsd</proxy><journal-ref>Mathematical Programming B 127, 1 (2011) 57-88</journal-ref><doi>10.1007/s10107-010-0417-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose novel necessary and sufficient conditions for a sensing matrix to
be "$s$-good" - to allow for exact $\ell_1$-recovery of sparse signals with $s$
nonzero entries when no measurement noise is present. Then we express the error
bounds for imperfect $\ell_1$-recovery (nonzero measurement noise, nearly
$s$-sparse signal, near-optimal solution of the optimization problem yielding
the $\ell_1$-recovery) in terms of the characteristics underlying these
conditions. Further, we demonstrate (and this is the principal result of the
paper) that these characteristics, although difficult to evaluate, lead to
verifiable sufficient conditions for exact sparse $\ell_1$-recovery and to
efficiently computable upper bounds on those $s$ for which a given sensing
matrix is $s$-good. We establish also instructive links between our approach
and the basic concepts of the Compressed Sensing theory, like Restricted
Isometry or Restricted Eigenvalue properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.2703</identifier>
 <datestamp>2010-08-17</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.2703</id><created>2008-09-16</created><authors><author><keyname>Barreto-Souza</keyname><forenames>Wagner</forenames></author><author><keyname>de Morais</keyname><forenames>Alice Lemos</forenames></author><author><keyname>Cordeiro</keyname><forenames>Gauss M.</forenames></author></authors><title>The Weibull-Geometric distribution</title><categories>stat.ME</categories><journal-ref>Journal of Statistical Computation and Simulation. (2010)</journal-ref><doi>10.1080/00949650903436554</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce, for the first time, the Weibull-Geometric
distribution which generalizes the exponential-geometric distribution proposed
by Adamidis and Loukas (1998). The hazard function of the last distribution is
monotone decreasing but the hazard function of the new distribution can take
more general forms. Unlike the Weibull distribution, the proposed distribution
is useful for modeling unimodal failure rates. We derive the cumulative
distribution and hazard functions, the density of the order statistics and
calculate expressions for its moments and for the moments of the order
statistics. We give expressions for the R\'enyi and Shannon entropies. The
maximum likelihood estimation procedure is discussed and an algorithm EM
(Dempster et al., 1977; McLachlan and Krishnan, 1997) is provided for
estimating the parameters. We obtain the information matrix and discuss
inference. Applications to real data sets are given to show the flexibility and
potentiality of the proposed distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.2768</identifier>
 <datestamp>2012-07-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.2768</id><created>2008-09-16</created><updated>2008-10-15</updated><authors><author><keyname>Slater</keyname><forenames>Paul B.</forenames></author></authors><title>Hubs and Clusters in the Evolving U. S. Internal Migration Network</title><categories>physics.soc-ph cs.SI physics.data-an stat.AP</categories><comments>38 pages, 16 figures, 2 tables. Additional analyses of the 1995-2000
  migration data and new figures are presented in Secs. V.C and V.D. To examine
  the four (searchable) master dendrograms (the first two [cardinal and
  ordinal] based on the doubly-stochastic table, and the next two, on its
  square), one must download the source</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most nations of the world periodically publish N x N origin-destination
tables, recording the number of people who lived in geographic subdivision i at
time t and j at t+1. We have developed and widely applied to such national
tables and other analogous (weighted, directed) socioeconomic networks, a
two-stage--double-standardization and (strong component) hierarchical
clustering--procedure. Previous applications of this methodology and related
analytical issues are discussed. Its use is illustrated in a large-scale study,
employing recorded United States internal migration flows between the 3,000+
county-level units of the nation for the periods 1965-1970 and 1995-2000.
Prominent, important features--such as ''cosmopolitan hubs'' and ``functional
regions''--are extracted from master dendrograms. The extent to which such
characteristics have varied over the intervening thirty years is evaluated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.3170</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.3170</id><created>2008-09-18</created><updated>2012-12-04</updated><authors><author><keyname>Chen</keyname><forenames>Xinjia</forenames></author></authors><title>A New Framework of Multistage Hypothesis Tests</title><categories>math.ST cs.LG math.PR stat.ME stat.TH</categories><comments>77 pages, no figure; added more references; in Proceedings of SPIE
  Conferences, Orlando, Florida, April 5-10, 2010 and April 25-29, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have established a general framework of multistage
hypothesis tests which applies to arbitrarily many mutually exclusive and
exhaustive composite hypotheses. Within the new framework, we have constructed
specific multistage tests which rigorously control the risk of committing
decision errors and are more efficient than previous tests in terms of average
sample number and the number of sampling operations. Without truncation, the
sample numbers of our testing plans are absolutely bounded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.3332</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.3332</id><created>2008-09-19</created><updated>2009-08-17</updated><authors><author><keyname>Kerkyacharian</keyname><forenames>Grard</forenames><affiliation>PMA</affiliation></author><author><keyname>Kyriazis</keyname><forenames>George</forenames><affiliation>PMA</affiliation></author><author><keyname>Pennec</keyname><forenames>Erwan Le</forenames><affiliation>PMA</affiliation></author><author><keyname>Petrushev</keyname><forenames>Pencho</forenames><affiliation>PMA</affiliation></author><author><keyname>Picard</keyname><forenames>Dominique</forenames><affiliation>PMA</affiliation></author></authors><title>Inversion of noisy Radon transform by SVD based needlet</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00321894</proxy><journal-ref>Applied and Computational Harmonic Analysis 28, 1 (2010) 24-45</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A linear method for inverting noisy observations of the Radon transform is
developed based on decomposition systems (needlets) with rapidly decaying
elements induced by the Radon transform SVD basis. Upper bounds of the risk of
the estimator are established in $L^p$ ($1\le p\le \infty$) norms for functions
with Besov space smoothness. A practical implementation of the method is given
and several examples are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.3373</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.3373</id><created>2008-09-19</created><authors><author><keyname>Bailer-Jones</keyname><forenames>C. A. L.</forenames><affiliation>MPIA, Heidelberg</affiliation></author><author><keyname>Smith</keyname><forenames>K. W.</forenames><affiliation>MPIA, Heidelberg</affiliation></author><author><keyname>Tiede</keyname><forenames>C.</forenames><affiliation>MPIA, Heidelberg</affiliation></author><author><keyname>Sordo</keyname><forenames>R.</forenames><affiliation>INAF, Padua</affiliation></author><author><keyname>Vallenari</keyname><forenames>A.</forenames><affiliation>INAF, Padua</affiliation></author></authors><title>Finding rare objects and building pure samples: Probabilistic quasar
  classification from low resolution Gaia spectra</title><categories>astro-ph physics.data-an stat.ML</categories><comments>MNRAS accepted</comments><journal-ref>MNRAS 391, 1838 (2008)</journal-ref><doi>10.1111/j.1365-2966.2008.13983.x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop and demonstrate a probabilistic method for classifying rare
objects in surveys with the particular goal of building very pure samples. It
works by modifying the output probabilities from a classifier so as to
accommodate our expectation (priors) concerning the relative frequencies of
different classes of objects. We demonstrate our method using the Discrete
Source Classifier, a supervised classifier currently based on Support Vector
Machines, which we are developing in preparation for the Gaia data analysis.
DSC classifies objects using their very low resolution optical spectra. We look
in detail at the problem of quasar classification, because identification of a
pure quasar sample is necessary to define the Gaia astrometric reference frame.
By varying a posterior probability threshold in DSC we can trade off sample
completeness and contamination. We show, using our simulated data, that it is
possible to achieve a pure sample of quasars (upper limit on contamination of 1
in 40,000) with a completeness of 65% at magnitudes of G=18.5, and 50% at
G=20.0, even when quasars have a frequency of only 1 in every 2000 objects. The
star sample completeness is simultaneously 99% with a contamination of 0.7%.
Including parallax and proper motion in the classifier barely changes the
results. We further show that not accounting for class priors in the target
population leads to serious misclassifications and poor predictions for sample
completeness and contamination. (Truncated)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.3650</identifier>
 <datestamp>2011-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.3650</id><created>2008-09-22</created><updated>2009-09-23</updated><authors><author><keyname>Dobigeon</keyname><forenames>Nicolas</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames></author></authors><title>Hierarchical Bayesian sparse image reconstruction with application to
  MRFM</title><categories>physics.data-an cs.IT math.IT stat.ME</categories><comments>v2: final version; IEEE Trans. Image Processing, 2009</comments><journal-ref>IEEE Trans. Image Processing, vol. 18, no. 9, pp. 2059-2070, Sept.
  2009</journal-ref><doi>10.1109/TIP.2009.2024067</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a hierarchical Bayesian model to reconstruct sparse
images when the observations are obtained from linear transformations and
corrupted by an additive white Gaussian noise. Our hierarchical Bayes model is
well suited to such naturally sparse image applications as it seamlessly
accounts for properties such as sparsity and positivity of the image via
appropriate Bayes priors. We propose a prior that is based on a weighted
mixture of a positive exponential distribution and a mass at zero. The prior
has hyperparameters that are tuned automatically by marginalization over the
hierarchical Bayesian model. To overcome the complexity of the posterior
distribution, a Gibbs sampling strategy is proposed. The Gibbs samples can be
used to estimate the image to be recovered, e.g. by maximizing the estimated
posterior distribution. In our fully Bayesian approach the posteriors of all
the parameters are available. Thus our algorithm provides more information than
other previously proposed sparse reconstruction methods that only give a point
estimate. The performance of our hierarchical Bayesian sparse reconstruction
method is illustrated on synthetic and real data collected from a tobacco virus
sample using a prototype MRFM instrument.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.3918</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.3918</id><created>2008-09-23</created><updated>2008-09-24</updated><authors><author><keyname>ukovi</keyname><forenames>Milan</forenames></author><author><keyname>Hristopulos</keyname><forenames>Dionissios T.</forenames></author></authors><title>Multilevel Discretized Random Field Models with "Spin" Correlations for
  the Simulation of Environmental Spatial Data</title><categories>stat.AP math.PR</categories><comments>20 pages, 8 figures. Presented at the Sigma Phi 2008 conference,
  http://www2.polito.it/eventi/sigmaphi2008/</comments><journal-ref>J. Stat. Mech. (2009) P02023</journal-ref><doi>10.1088/1742-5468/2009/02/P02023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A problem of practical significance is the analysis of large, spatially
distributed data sets. The problem is more challenging for variables that
follow non-Gaussian distributions. We show that the spatial correlations
between variables can be captured by interactions between "spins". The spins
represent multilevel discretizations of the initial field with respect to a
number of pre-defined thresholds. The spatial dependence between the "spins" is
imposed by means of short-range interactions. We present two approaches,
inspired by the Ising and Potts models, that generate conditional simulations
from samples with missing data. The simulations of the "spin system" are forced
to respect locally the sample values and the system statistics globally. We
compare the two approaches in terms of their ability to reproduce the sample
statistical properties, to predict data at unsampled locations, as well as in
terms of their computational complexity. We discuss the impact of relevant
simulation parameters, such as the domain size, the number of discretization
levels, and the initial conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.4178</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.4178</id><created>2008-09-24</created><updated>2009-02-23</updated><authors><author><keyname>Blum</keyname><forenames>M. G. B.</forenames></author><author><keyname>Francois</keyname><forenames>O.</forenames></author></authors><title>Non-linear regression models for Approximate Bayesian Computation</title><categories>stat.CO stat.ML</categories><comments>4 figures; version 3 minor changes; to appear in Statistics and
  Computing</comments><journal-ref>Statistics and Computing, 20: 63-73 (2010)</journal-ref><doi>10.1007/s11222-009-9116-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate Bayesian inference on the basis of summary statistics is
well-suited to complex problems for which the likelihood is either
mathematically or computationally intractable. However the methods that use
rejection suffer from the curse of dimensionality when the number of summary
statistics is increased. Here we propose a machine-learning approach to the
estimation of the posterior density by introducing two innovations. The new
method fits a nonlinear conditional heteroscedastic regression of the parameter
on the summary statistics, and then adaptively improves estimation using
importance sampling. The new algorithm is compared to the state-of-the-art
approximate Bayesian methods, and achieves considerable reduction of the
computational burden in two examples of inference in statistical genetics and
in a queueing model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.4297</identifier>
 <datestamp>2010-11-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.4297</id><created>2008-09-24</created><updated>2010-11-26</updated><authors><author><keyname>Rudloff</keyname><forenames>Birgit</forenames></author><author><keyname>Karatzas</keyname><forenames>Ioannis</forenames></author></authors><title>Testing composite hypotheses via convex duality</title><categories>math.PR math.OC math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ249 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ249</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 4, 1224-1239</journal-ref><doi>10.3150/10-BEJ249</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of testing composite hypotheses versus composite
alternatives, using a convex duality approach. In contrast to classical results
obtained by Krafft and Witting (Z. Wahrsch. Verw. Gebiete 7 (1967) 289--302),
where sufficient optimality conditions are derived via Lagrange duality, we
obtain necessary and sufficient optimality conditions via Fenchel duality under
compactness assumptions. This approach also differs from the methodology
developed in Cvitani\'{c} and Karatzas (Bernoulli 7 (2001) 79--97).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.4560</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.4560</id><created>2008-09-26</created><authors><author><keyname>Hashorva</keyname><forenames>Enkelejd</forenames></author></authors><title>Boundary non-crossings of Brownian pillow</title><categories>math.PR math.ST stat.TH</categories><comments>14 pages</comments><msc-class>60F10, 60G15</msc-class><doi>10.1007/s10959-008-0191-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let B_0(s,t) be a Brownian pillow with continuous sample paths, and let
h,u:[0,1]^2\to R be two measurable functions. In this paper we derive upper and
lower bounds for the boundary non-crossing probability
\psi(u;h):=P{B_0(s,t)+h(s,t) \le u(s,t), \forall s,t\in [0,1]}. Further we
investigate the asymptotic behaviour of $\psi(u;\gamma h)$ with $\gamma$
tending to infinity, and solve a related minimisation problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0809.4627</identifier>
 <datestamp>2011-09-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0809.4627</id><created>2008-09-26</created><updated>2011-08-27</updated><authors><author><keyname>Zhu</keyname><forenames>Mingfu</forenames></author><author><keyname>Jiang</keyname><forenames>Guangran</forenames></author><author><keyname>Gao</keyname><forenames>Shuhong</forenames></author></authors><title>Solving the 100 Swiss Francs Problem</title><categories>stat.CO math.AG math.ST stat.TH</categories><msc-class>65H10 (Primary), 62P10, 62F30 (Secondary)</msc-class><doi>10.1007/s11786-011-0068-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sturmfels offered 100 Swiss Francs in 2005 to a conjecture, which deals with
a special case of the maximum likelihood estimation for a latent class model.
This paper confirms the conjecture positively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0810.0392</identifier>
 <datestamp>2010-11-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0810.0392</id><created>2008-10-02</created><updated>2010-11-29</updated><authors><author><keyname>MacPhee</keyname><forenames>Iain M.</forenames></author><author><keyname>Menshikov</keyname><forenames>Mikhail V.</forenames></author><author><keyname>Volkov</keyname><forenames>Stanislav</forenames></author><author><keyname>Wade</keyname><forenames>Andrew R.</forenames></author></authors><title>Passage-time moments and hybrid zones for the exclusion-voter model</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ243 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ243</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 4, 1312-1342</journal-ref><doi>10.3150/09-BEJ243</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the non-equilibrium dynamics of a one-dimensional interacting
particle system that is a mixture of the voter model and the exclusion process.
With the process started from a finite perturbation of the ground state
Heaviside configuration consisting of 1's to the left of the origin and 0's
elsewhere, we study the relaxation time $\tau$, that is, the first hitting time
of the ground state configuration (up to translation). We give conditions for
$\tau$ to be finite and for certain moments of $\tau$ to be finite or infinite,
and prove a result that approaches a conjecture of Belitsky et al. (Bernoulli 7
(2001) 119--144). Ours are the first non-existence-of-moments results for
$\tau$ for the mixture model. Moreover, we give almost sure asymptotics for the
evolution of the size of the hybrid (disordered) region. Most of our results
pertain to the discrete-time setting, but several transfer to continuous-time.
As well as the mixture process, some of our results also cover pure exclusion.
We state several significant open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0810.0866</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0810.0866</id><created>2008-10-05</created><updated>2012-09-16</updated><authors><author><keyname>Zhang</keyname><forenames>Zuhe</forenames></author></authors><title>The enumeration of independent sets on some lattices</title><categories>math.CO math-ph math.MP math.ST stat.TH</categories><msc-class>05A15, 05A16</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, firstly we show that the entropy constants of the number of
independent sets on certain plane lattices are the same as the entropy
constants of the corresponding cylindrical and toroidal lattices. Secondly, we
consider three more complex lattices which can not be handled by a single
transfer matrix as in the plane quadratic lattice case. By introducing the
concept of transfer multiplicity, we obtain the lower and upper bounds of the
entropy constants of crossed quadratic lattice, generalized aztec diamond
lattice and 8-8-4 lattice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0810.0896</identifier>
 <datestamp>2010-08-31</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0810.0896</id><created>2008-10-06</created><updated>2010-05-31</updated><authors><author><keyname>Blum</keyname><forenames>Michael G. B.</forenames><affiliation>TIMC</affiliation></author><author><keyname>Tran</keyname><forenames>Viet Chi</forenames><affiliation>LPP, CMAP</affiliation></author></authors><title>HIV with contact-tracing: a case study in Approximate Bayesian
  Computation</title><categories>stat.AP stat.CO</categories><proxy>ccsd</proxy><journal-ref>Biostatistics 11, 4 (2010) 644-660</journal-ref><doi>10.1093/biostatistics/kxq022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Missing data is a recurrent issue in epidemiology where the infection process
may be partially observed. Approximate Bayesian Computation, an alternative to
data imputation methods such as Markov Chain Monte Carlo integration, is
proposed for making inference in epidemiological models. It is a
likelihood-free method that relies exclusively on numerical simulations. ABC
consists in computing a distance between simulated and observed summary
statistics and weighting the simulations according to this distance. We propose
an original extension of ABC to path-valued summary statistics, corresponding
to the cumulated number of detections as a function of time. For a standard
compartmental model with Suceptible, Infectious and Recovered individuals
(SIR), we show that the posterior distributions obtained with ABC and MCMC are
similar. In a refined SIR model well-suited to the HIV contact-tracing data in
Cuba, we perform a comparison between ABC with full and binned detection times.
For the Cuban data, we evaluate the efficiency of the detection system and
predict the evolution of the HIV-AIDS disease. In particular, the percentage of
undetected infectious individuals is found to be of the order of 40%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0810.0901</identifier>
 <datestamp>2010-08-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0810.0901</id><created>2008-10-06</created><updated>2010-08-12</updated><authors><author><keyname>Seeger</keyname><forenames>Matthias W.</forenames></author><author><keyname>Nickisch</keyname><forenames>Hannes</forenames></author></authors><title>Large Scale Variational Inference and Experimental Design for Sparse
  Generalized Linear Models</title><categories>stat.ML</categories><comments>34 pages, 6 figures, technical report (submitted)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many problems of low-level computer vision and image processing, such as
denoising, deconvolution, tomographic reconstruction or super-resolution, can
be addressed by maximizing the posterior distribution of a sparse linear model
(SLM). We show how higher-order Bayesian decision-making problems, such as
optimizing image acquisition in magnetic resonance scanners, can be addressed
by querying the SLM posterior covariance, unrelated to the density's mode. We
propose a scalable algorithmic framework, with which SLM posteriors over full,
high-resolution images can be approximated for the first time, solving a
variational optimization problem which is convex iff posterior mode finding is
convex. These methods successfully drive the optimization of sampling
trajectories for real-world magnetic resonance imaging through Bayesian
experimental design, which has not been attempted before. Our methodology
provides new insight into similarities and differences between sparse
reconstruction and approximate Bayesian inference, and has important
implications for compressive sensing of real-world images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0810.1547</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0810.1547</id><created>2008-10-08</created><authors><author><keyname>Hashorva</keyname><forenames>Enkelejd</forenames></author></authors><title>Conditional Limit Results for Type I Polar Distributions</title><categories>math.ST stat.TH</categories><comments>14 pages, paper submitted to Extremes in 2007</comments><msc-class>60F05</msc-class><doi>10.1007/s10687-008-0078-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let (S_1,S_2)=(R \cos(\Theta), R \sin (\Theta)) be a bivariate random vector
with associated random radius R which has distribution function $F$ being
further independent of the random angle \Theta. In this paper we investigate
the asymptotic behaviour of the conditional survivor probability
\Psi_{\rho,u}(y):=\pk{\rho S_1+ \sqrt{1- \rho^2} S_2&gt; y \lvert S_1&gt; u}, \rho
\in (-1,1),\in R when u approaches the upper endpoint of F. On the density
function of \Theta we require a certain local asymptotic behaviour at 0,
whereas for F we require that it belongs to the Gumbel max-domain of
attraction. The main result of this contribution is an asymptotic expansion of
\Psi_{\rho,u}, which is then utilised to construct two estimators for the
conditional distribution function 1- \Psi_{\rho,u}. Further, we allow \Theta to
depend on u.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0810.1793</identifier>
 <datestamp>2010-02-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0810.1793</id><created>2008-10-10</created><authors><author><keyname>Hara</keyname><forenames>Hisayuki</forenames></author><author><keyname>Takemura</keyname><forenames>Akimichi</forenames></author><author><keyname>Yoshida</keyname><forenames>Ruriko</forenames></author></authors><title>On connectivity of fibers with positive marginals in multiple logistic
  regression</title><categories>math.ST math.CO stat.TH</categories><comments>26 pages</comments><msc-class>62H17; 62E15</msc-class><journal-ref>J. Multivariate Anal., Vol.101 (2010), 909-925</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider exact tests of a multiple logistic regression,
where the levels of covariates are equally spaced, via Markov beses. In usual
application of multiple logistic regression, the sample size is positive for
each combination of levels of the covariates. In this case we do not need a
whole Markov basis, which guarantees connectivity of all fibers. We first give
an explicit Markov basis for multiple Poisson regression. By the Lawrence
lifting of this basis, in the case of bivariate logistic regression, we show a
simple subset of the Markov basis which connects all fibers with a positive
sample size for each combination of levels of covariates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0810.2688</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0810.2688</id><created>2008-10-15</created><authors><author><keyname>Barczy</keyname><forenames>Matyas</forenames></author><author><keyname>Pap</keyname><forenames>Gyula</forenames></author></authors><title>Asymptotic behavior of maximum likelihood estimator for time
  inhomogeneous diffusion processes</title><categories>math.ST math.PR stat.TH</categories><comments>35 pages</comments><msc-class>62M05 (Primary), 62F12, 60J60 (Secondary)</msc-class><journal-ref>Journal of Statistical Planning and Inference 140 (2010) 1576-1593</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study asymptotic behavior of maximum likelihood estimator for a time
inhomogeneous diffusion process given by a SDE $dX_t=\alpha b(t)X_t dt +
\sigma(t) dB_t$, $t\in[0,T)$, with a parameter $\alpha\in R$, where
$T\in(0,\infty]$ and $(B_t)_{t\in[0,T)}$ is a standard Wiener process. We
formulate sufficient conditions under which the MLE of $\alpha$ normalized by
Fisher information converges to the limit distribution of Dickey-Fuller
statistics. Next we study a SDE $dY_t=\alpha b(t)a(Y_t) dt + \sigma(t) dB_t$,
$t\in[0,T)$, with a perturbed drift satisfying $a(x)=x+O(1+|x|^\gamma)$ with
some $\gamma\in[0,1)$. We give again sufficient conditions under which the MLE
of $\alpha$ normalized by Fisher information converges to the limit
distribution of Dickey-Fuller statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0810.2930</identifier>
 <datestamp>2011-04-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0810.2930</id><created>2008-10-16</created><updated>2011-03-29</updated><authors><author><keyname>Barczy</keyname><forenames>Matyas</forenames></author><author><keyname>Pap</keyname><forenames>Gyula</forenames></author></authors><title>Explicit formulas for Laplace transforms of certain functionals of some
  time inhomogeneous diffusions</title><categories>math.PR math.ST stat.TH</categories><comments>27 pages, references are updated</comments><msc-class>60E10 (Primary), 60J60, 62F12 (Secondary)</msc-class><journal-ref>Journal of Mathematical Analysis and Applications, 380(2), 2011,
  405-424</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a process $(X_t)_{t\in[0,T)}$ given by the SDE $dX_t = \alpha
b(t)X_t dt + \sigma(t) dB_t$, $t\in[0,T)$, with initial condition $X_0=0$,
where $T\in(0,\infty]$, $\alpha\in R$, $(B_t)_{t\in[0,T)}$ is a standard Wiener
process, $b:[0,T)\to R\setminus\{0\}$ and $\sigma:[0,T)\to(0,\infty)$ are
continuously differentiable functions. Assuming that $b$ and $\sigma$ satisfy a
certain differential equation we derive an explicit formula for the joint
Laplace transform of $\int_0^t\frac{b(s)^2}{\sigma(s)^2}(X_s)^2 ds$ and
$(X_t)^2$ for all $t\in[0,T)$. As an application, we study asymptotic behavior
of the maximum likelihood estimator of $\alpha$ for $\sign(\alpha-K)=\sign(K)$,
$K\ne0$, and for $\alpha=K$, $K\ne0$. As an example, we examine the so-called
$\alpha$-Wiener bridges given by SDE $dX_t = -\frac{\alpha}{T-t}X_t dt + dB_t$,
$t\in[0,T)$, with initial condition $X_0=0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0810.3121</identifier>
 <datestamp>2014-07-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0810.3121</id><created>2008-10-17</created><updated>2014-07-16</updated><authors><author><keyname>Li</keyname><forenames>Jiexiang</forenames></author></authors><title>Asymptotic normality for deconvolution kernel density estimators from
  random fields</title><categories>math.ST stat.TH</categories><comments>This paper need significant enhancement. After necessary enhancement,
  the paper will be submitted to a journal for publication!</comments><proxy>vtex</proxy><report-no>IMS-EJS-EJS_2008_313</report-no><msc-class>62G07, 62G20 (Primary) 62M40 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper discusses the estimation of a continuous density function of the
target random field $X_{\bf{i}}$, $\bf{i}\in \mathbb {Z}^N$ which is
contaminated by measurement errors. In particular, the observed random field
$Y_{\bf{i}}$, $\bf{i}\in \mathbb {Z}^N$ is such that
$Y_{\bf{i}}=X_{\bf{i}}+\epsilon_{\bf{i}}$, where the random error
$\epsilon_{\bf{i}}$ is from a known distribution and independent of the target
random field. Compared to the existing results, the paper is improved in two
directions. First, the random vectors in contrast to univariate random
variables are investigated. Second, a random field with a certain spatial
interactions instead of i. i. d. random variables is studied. Asymptotic
normality of the proposed estimator is established under appropriate
conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0810.3177</identifier>
 <datestamp>2010-04-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0810.3177</id><created>2008-10-17</created><authors><author><keyname>Ambroise</keyname><forenames>Christophe</forenames></author><author><keyname>Chiquet</keyname><forenames>Julien</forenames></author><author><keyname>Matias</keyname><forenames>Catherine</forenames></author></authors><title>Inferring sparse Gaussian graphical models with latent structure</title><categories>stat.ME stat.AP</categories><comments>35 pages, 15 figures</comments><journal-ref>Electron. J. Statist. Volume 3 (2009), 205-238.</journal-ref><doi>10.1214/08-EJS314</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our concern is selecting the concentration matrix's nonzero coefficients for
a sparse Gaussian graphical model in a high-dimensional setting. This
corresponds to estimating the graph of conditional dependencies between the
variables. We describe a novel framework taking into account a latent structure
on the concentration matrix. This latent structure is used to drive a penalty
matrix and thus to recover a graphical model with a constrained topology. Our
method uses an $\ell_1$ penalized likelihood criterion. Inference of the graph
of conditional dependencies between the variates and of the hidden variables is
performed simultaneously in an iterative \textsc{em}-like algorithm. The
performances of our method is illustrated on synthetic as well as real data,
the latter concerning breast cancer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0810.3946</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0810.3946</id><created>2008-10-21</created><updated>2011-06-12</updated><authors><author><keyname>Chen</keyname><forenames>Xinjia</forenames></author></authors><title>Multistage Hypothesis Tests for the Mean of a Normal Distribution</title><categories>math.ST math.PR stat.ME stat.TH</categories><comments>34 pages, 28 figures, added adaptive scanning algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have developed new multistage tests which guarantee
prescribed level of power and are more efficient than previous tests in terms
of average sampling number and the number of sampling operations. Without
truncation, the maximum sampling numbers of our testing plans are absolutely
bounded. Based on geometrical arguments, we have derived extremely tight bounds
for the operating characteristic function. To reduce the computational
complexity for the relevant integrals, we propose adaptive scanning algorithms
which are not only useful for present hypothesis testing problem but also for
other problem areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0810.4727</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0810.4727</id><created>2008-10-26</created><updated>2008-11-10</updated><authors><author><keyname>Chen</keyname><forenames>Xinjia</forenames></author></authors><title>Robust Estimation of Mean Values</title><categories>math.ST cs.SY math.PR stat.CO stat.TH</categories><comments>12 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a computational approach for estimating the mean
value of a quantity in the presence of uncertainty. We demonstrate that, under
some mild assumptions, the upper and lower bounds of the mean value are
efficiently computable via a sample reuse technique, of which the computational
complexity is shown to posses a Poisson distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0810.5302</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0810.5302</id><created>2008-10-29</created><updated>2012-11-15</updated><authors><author><keyname>Leonenko</keyname><forenames>Nikolai</forenames></author><author><keyname>Pronzato</keyname><forenames>Luc</forenames></author><author><keyname>Savani</keyname><forenames>Vippal</forenames></author></authors><title>A class of R\'{e}nyi information estimators for multidimensional
  densities</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/07-AOS539 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS539</report-no><journal-ref>Annals of Statistics 2008, Vol. 36, No. 5, 2153-2182</journal-ref><doi>10.1214/07-AOS539</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A class of estimators of the R\'{e}nyi and Tsallis entropies of an unknown
distribution $f$ in $\mathbb{R}^m$ is presented. These estimators are based on
the $k$th nearest-neighbor distances computed from a sample of $N$ i.i.d.
vectors with distribution $f$. We show that entropies of any order $q$,
including Shannon's entropy, can be estimated consistently with minimal
assumptions on $f$. Moreover, we show that it is straightforward to extend the
nearest-neighbor method to estimate the statistical distance between two
distributions using one i.i.d. sample from each. (Wit Correction.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0810.5551</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0810.5551</id><created>2008-10-30</created><updated>2008-11-10</updated><authors><author><keyname>Chen</keyname><forenames>Xinjia</forenames></author></authors><title>A Theory of Truncated Inverse Sampling</title><categories>math.ST cs.LG math.PR stat.ME stat.TH</categories><comments>31 pages, no figure, revised proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have established a new framework of truncated inverse
sampling for estimating mean values of non-negative random variables such as
binomial, Poisson, hyper-geometrical, and bounded variables. We have derived
explicit formulas and computational methods for designing sampling schemes to
ensure prescribed levels of precision and confidence for point estimators.
Moreover, we have developed interval estimation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0810.5706</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0810.5706</id><created>2008-10-31</created><updated>2009-03-24</updated><authors><author><keyname>Hashorva</keyname><forenames>Enkelejd</forenames></author></authors><title>Conditional Limits of W_p scale Mixture Distributions</title><categories>math.PR math.ST stat.TH</categories><comments>14 pages</comments><msc-class>60F05, 60G70, 62E20, 62H05</msc-class><doi>10.1016/j.jspi.2009.04.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce the class of W_p scale mixture random vectors with
a particular radial decomposition and a independent splitting property
specified by some random variable W_p, and a positive constant p. We derive
several conditional limit results assuming that the distribution of the random
radius is in the max-domain of attraction of a univariate extreme value
distribution and W_p has a certain tail asymptotic behaviour. As an application
we obtain the joint asymptotic distribution of concomitants of order statics
considering certain bivariate W_p scale miture samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.0072</identifier>
 <datestamp>2011-07-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.0072</id><created>2008-11-01</created><updated>2011-07-04</updated><authors><author><keyname>Alquier</keyname><forenames>Pierre</forenames><affiliation>LPMA, CREST</affiliation></author><author><keyname>Hebiri</keyname><forenames>Mohamed</forenames><affiliation>LPMA</affiliation></author></authors><title>Generalization of l1 constraints for high dimensional regression
  problems</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the high dimensional linear regression
$Y\sim\mathcal{N}(X\beta^{*},\sigma^{2}I_{n})$, where
$\beta^{*}\in\mathds{R}^{p}$ is the parameter of interest. In this setting,
several estimators such as the LASSO and the Dantzig Selector are known to
satisfy interesting properties whenever the vector $\beta^{*}$ is sparse.
Interestingly both of the LASSO and the Dantzig Selector can be seen as
orthogonal projections of 0 into
$\mathcal{DC}(s)=\{\beta\in\mathds{R}^{p},\|X'(Y-X\beta)\|_{\infty}\leq s\}$ -
using an $\ell_{1}$ distance for the Dantzig Selector and $\ell_{2}$ for the
LASSO. For a well chosen $s&gt;0$, this set is actually a confidence region for
$\beta^{*}$. In this paper, we investigate the properties of estimators defined
as projections on $\mathcal{DC}(s)$ using general distances. We prove that the
obtained estimators satisfy oracle properties close to the one of the LASSO and
Dantzig Selector. On top of that, it turns out that these estimators can be
tuned to exploit a different sparsity or/and slightly different estimation
objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.0643</identifier>
 <datestamp>2012-08-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.0643</id><created>2008-11-04</created><updated>2012-08-01</updated><authors><author><keyname>Foondun</keyname><forenames>Mohammud</forenames></author><author><keyname>Khoshnevisan</keyname><forenames>Davar</forenames></author></authors><title>An asymptotic theory for randomly forced discrete nonlinear heat
  equations</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/11-BEJ357 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ357</report-no><journal-ref>Bernoulli 2012, Vol. 18, No. 3, 1042-1060</journal-ref><doi>10.3150/11-BEJ357</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study discrete nonlinear parabolic stochastic heat equations of the form,
$u_{n+1}(x)-u_n(x)=(\mathcal {L}u_n)(x)+\sigma(u_n(x))\xi_n(x)$, for $n\in
{\mathbf{Z}}_+$ and $x\in {\mathbf{Z}}^d$, where $\boldsymbol
\xi:=\{\xi_n(x)\}_{n\ge 0,x\in {\mathbf{Z}}^d}$ denotes random forcing and
$\mathcal {L}$ the generator of a random walk on ${\mathbf{Z}}^d$. Under mild
conditions, we prove that the preceding stochastic PDE has a unique solution
that grows at most exponentially in time. And that, under natural conditions,
it is "weakly intermittent." Along the way, we establish a comparison principle
as well as a finite support property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.0662</identifier>
 <datestamp>2013-09-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.0662</id><created>2008-11-05</created><authors><author><keyname>Hashorva</keyname><forenames>Enkelejd</forenames></author></authors><title>Asymptotics for Kotz Type III Elliptical Distributions</title><categories>math.PR math.ST stat.TH</categories><comments>10 pages</comments><msc-class>60F05, 60G70</msc-class><journal-ref>Statistics and Probability Letters 79 (2009) 927?--935</journal-ref><doi>10.1016/j.spl.2008.11.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we derive the tail asymptotics of a Kotz Type III elliptical
random vector. As an application of our asymptotic expansion we derive an
approximation for the conditional excess distribution. Furthermore, we discuss
the asymptotic dependence of Kotz Type III triangular arrays and provide some
details on the estimation of conditional excess distribution and survivor
function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.0697</identifier>
 <datestamp>2012-11-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.0697</id><created>2008-11-05</created><updated>2012-11-15</updated><authors><author><keyname>Chan</keyname><forenames>Ngai Hang</forenames></author><author><keyname>Ling</keyname><forenames>Shiqing</forenames></author></authors><title>Residual empirical processes for long and short memory time series</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/07-AOS543 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS543</report-no><journal-ref>Annals of Statistics 2008, Vol. 36, No. 5, 2453-2470</journal-ref><doi>10.1214/07-AOS543</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the residual empirical process of long- and short-memory
time series regression models and establishes its uniform expansion under a
general framework. The results are applied to the stochastic regression models
and unstable autoregressive models. For the long-memory noise, it is shown that
the limit distribution of the Kolmogorov-Smirnov test statistic studied in Ho
and Hsing [Ann. Statist. 24 (1996) 992-1024] does not hold when the stochastic
regression model includes an unknown intercept or when the characteristic
polynomial of the unstable autoregressive model has a unit root. To this end,
two new statistics are proposed to test for the distribution of the long-memory
noises of stochastic regression models and unstable autoregressive models.
(With Correction.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.0802</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.0802</id><created>2008-11-05</created><updated>2014-10-01</updated><authors><author><keyname>Celisse</keyname><forenames>Alain</forenames></author></authors><title>Optimal cross-validation in density estimation with the $L^2$-loss</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/14-AOS1240 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>ccsd</proxy><report-no>IMS-AOS-AOS1240</report-no><msc-class>62G09 (Primary) 62G07, 62E17 (Secondary)</msc-class><journal-ref>Annals of Statistics 2014, Vol. 42, No. 5, 1879-1910</journal-ref><doi>10.1214/14-AOS1240</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the performance of cross-validation (CV) in the density estimation
framework with two purposes: (i) risk estimation and (ii) model selection. The
main focus is given to the so-called leave-$p$-out CV procedure (Lpo), where
$p$ denotes the cardinality of the test set. Closed-form expressions are
settled for the Lpo estimator of the risk of projection estimators. These
expressions provide a great improvement upon $V$-fold cross-validation in terms
of variability and computational complexity. From a theoretical point of view,
closed-form expressions also enable to study the Lpo performance in terms of
risk estimation. The optimality of leave-one-out (Loo), that is Lpo with $p=1$,
is proved among CV procedures used for risk estimation. Two model selection
frameworks are also considered: estimation, as opposed to identification. For
estimation with finite sample size $n$, optimality is achieved for $p$ large
enough [with $p/n=o(1)$] to balance the overfitting resulting from the
structure of the model collection. For identification, model selection
consistency is settled for Lpo as long as $p/n$ is conveniently related to the
rate of convergence of the best estimator in the collection: (i) $p/n\to1$ as
$n\to+\infty$ with a parametric rate, and (ii) $p/n=o(1)$ with some
nonparametric estimators. These theoretical results are validated by simulation
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.1729</identifier>
 <datestamp>2010-02-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.1729</id><created>2008-11-11</created><updated>2010-02-25</updated><authors><author><keyname>Flegal</keyname><forenames>James M.</forenames></author><author><keyname>Jones</keyname><forenames>Galin L.</forenames></author></authors><title>Batch means and spectral variance estimators in Markov chain Monte Carlo</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS735 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><report-no>IMS-AOS-AOS735</report-no><msc-class>60J22 (Primary) 62M15 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 1034-1070</journal-ref><doi>10.1214/09-AOS735</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Calculating a Monte Carlo standard error (MCSE) is an important step in the
statistical analysis of the simulation output obtained from a Markov chain
Monte Carlo experiment. An MCSE is usually based on an estimate of the variance
of the asymptotic normal distribution. We consider spectral and batch means
methods for estimating this variance. In particular, we establish conditions
which guarantee that these estimators are strongly consistent as the simulation
effort increases. In addition, for the batch means and overlapping batch means
methods we establish conditions ensuring consistency in the mean-square sense
which in turn allows us to calculate the optimal batch size up to a constant of
proportionality. Finally, we examine the empirical finite-sample properties of
spectral variance and batch means estimators and provide recommendations for
practitioners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.1888</identifier>
 <datestamp>2011-08-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.1888</id><created>2008-11-12</created><updated>2009-06-04</updated><authors><author><keyname>Dehling</keyname><forenames>Herold</forenames></author><author><keyname>Wendler</keyname><forenames>Martin</forenames></author></authors><title>Central Limit Theorem and the Bootstrap for U-Statistics of Strongly
  Mixing Data</title><categories>math.PR math.ST stat.TH</categories><comments>20 pages, typos corrected</comments><report-no>J. Multivariate Anal. 101 (2010), no. 1, 126-137</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The asymptotic normality of U-statistics has so far been proved for iid data
and under various mixing conditions such as absolute regularity, but not for
strong mixing. We use a coupling technique introduced in 1983 by Bradley to
prove a new generalized covariance inequality similar to Yoshihara's. It
follows from the Hoeffding-decomposition and this inequality that U-statistics
of strongly mixing observations converge to a normal limit if the kernel of the
U-statistic fulfills some moment and continuity conditions.
  The validity of the bootstrap for U-statistics has until now only been
established in the case of iid data (see Bickel and Freedman). For mixing data,
Politis and Romano proposed the circular block bootstrap, which leads to a
consistent estimation of the sample mean's distribution. We extend these
results to U-statistics of weakly dependent data and prove a CLT for the
circular block bootstrap version of U-statistics under absolute regularity and
strong mixing. We also calculate a rate of convergence for the bootstrap
variance estimator of a U-statistic and give some simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.2097</identifier>
 <datestamp>2012-11-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.2097</id><created>2008-11-13</created><updated>2009-04-22</updated><authors><author><keyname>Aletti</keyname><forenames>G.</forenames></author><author><keyname>May</keyname><forenames>C.</forenames></author><author><keyname>Secchi</keyname><forenames>P.</forenames></author></authors><title>A Central Limit Theorem, and related results, for a two-color randomly
  reinforced urn</title><categories>math.PR math.ST stat.TH</categories><comments>typos corrected</comments><msc-class>60F05</msc-class><journal-ref>Adv. in Appl. Probab. Volume 41, Number 3 (2009), 829-844</journal-ref><doi>10.1239/aap/1253281065</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a Central Limit Theorem for the sequence of random compositions of a
two-color randomly reinforced urn. As a consequence, we are able to show that
the distribution of the urn limit composition has no point masses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.2501</identifier>
 <datestamp>2012-01-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.2501</id><created>2008-11-16</created><updated>2009-10-02</updated><authors><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author><author><keyname>Zhou</keyname><forenames>Shuheng</forenames></author></authors><title>A statistical framework for differential privacy</title><categories>math.ST stat.TH</categories><comments>42 pages, 1 figure</comments><journal-ref>Journal of the American Statistical Association (2010) Volume 105,
  No. 489, pp 375--389</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One goal of statistical privacy research is to construct a data release
mechanism that protects individual privacy while preserving information
content. An example is a {\em random mechanism} that takes an input database
$X$ and outputs a random database $Z$ according to a distribution
$Q_n(\cdot|X)$. {\em Differential privacy} is a particular privacy requirement
developed by computer scientists in which $Q_n(\cdot |X)$ is required to be
insensitive to changes in one data point in $X$. This makes it difficult to
infer from $Z$ whether a given individual is in the original database $X$. We
consider differential privacy from a statistical perspective. We consider
several data release mechanisms that satisfy the differential privacy
requirement. We show that it is useful to compare these schemes by computing
the rate of convergence of distributions and densities constructed from the
released data. We study a general privacy method, called the exponential
mechanism, introduced by McSherry and Talwar (2007). We show that the accuracy
of this method is intimately linked to the rate at which the probability that
the empirical distribution concentrates in a small ball around the true
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.2664</identifier>
 <datestamp>2010-08-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.2664</id><created>2008-11-17</created><updated>2010-08-12</updated><authors><author><keyname>Bardet</keyname><forenames>Jean-Marc</forenames><affiliation>SAMM</affiliation></author><author><keyname>Tudor</keyname><forenames>Ciprian</forenames><affiliation>SAMM, LPP</affiliation></author></authors><title>A wavelet analysis of the Rosenblatt process: chaos expansion and
  estimation of the self-similarity parameter</title><categories>math.ST math.PR stat.TH</categories><proxy>ccsd</proxy><journal-ref>Stochastic Analysis and Applications (2010) 1-25</journal-ref><doi>10.1016/j.spa.2010.08.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By using chaos expansion into multiple stochastic integrals, we make a
wavelet analysis of two self-similar stochastic processes: the fractional
Brownian motion and the Rosenblatt process. We study the asymptotic behavior of
the statistic based on the wavelet coefficients of these processes. Basically,
when applied to a non-Gaussian process (such as the Rosenblatt process) this
statistic satisfies a non-central limit theorem even when we increase the
number of vanishing moments of the wavelet function. We apply our limit
theorems to construct estimators for the self-similarity index and we
illustrate our results by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.2691</identifier>
 <datestamp>2014-02-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.2691</id><created>2008-11-17</created><authors><author><keyname>Siven</keyname><forenames>Johannes Vitalis</forenames></author><author><keyname>Lins</keyname><forenames>Jeffrey Todd</forenames></author><author><keyname>Szymkowiak-Have</keyname><forenames>Anna</forenames></author></authors><title>Value-at-Risk Computation by Fourier Inversion with Explicit Error
  Bounds</title><categories>stat.AP</categories><journal-ref>Finance Research Letters, Elsevier, vol. 6(2), pages 95-105, June
  2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The value-at-risk of a delta-gamma approximated derivatives portfolio can be
computed by numerical integration of the characteristic function. However,
while the choice of parameters in any numerical integration scheme is
paramount, in practice it often relies on ad hoc procedures of trial and error.
For normal and multivariate $t$-distributed risk factors, we show how to
calculate the necessary parameters for one particular integration scheme as a
function of the data (the distribution of risk factors, and delta and gamma)
\emph{in order to satisfy a given error tolerance}. This allows for
implementation in a fully automated risk management system. We also demonstrate
in simulations that the method is significantly faster than the Monte Carlo
method, for a given error tolerance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.2741</identifier>
 <datestamp>2010-02-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.2741</id><created>2008-11-17</created><updated>2010-02-16</updated><authors><author><keyname>Davydov</keyname><forenames>Youri</forenames><affiliation>LPP</affiliation></author><author><keyname>Nagaev</keyname><forenames>Alexender</forenames><affiliation>LMJL</affiliation></author><author><keyname>Philippe</keyname><forenames>Anne</forenames><affiliation>LMJL</affiliation></author></authors><title>On peeling procedure applied to a Poisson point process</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00339232</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the focus of our attention is the asymptotic properties of the sequence of
convex hulls which arise as a result of a peeling procedure applied to the
convex hull generated by a Poisson point process. Processes of the considered
type are tightly connected with empirical point processes and stable random
vectors. Results are given about the limit shape of the convex hulls in the
case of a discrete spectral measure. We give some numerical experiments to
illustrate the peeling procedure for a more large class of Poisson point
processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.2769</identifier>
 <datestamp>2010-05-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.2769</id><created>2008-11-17</created><updated>2009-04-20</updated><authors><author><keyname>Meckes</keyname><forenames>Elizabeth</forenames></author></authors><title>Quantitative asymptotics of graphical projection pursuit</title><categories>math.PR math.ST stat.TH</categories><comments>Proof of Theorem 2 reorganized, some additional comments on
  motivation, including a new corollary on waiting times to find non-Gaussian
  directions. To appear in Elec. Comm. Probab</comments><journal-ref>Electron. Comm. Probab. 14 (2009)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a result of Diaconis and Freedman which says that, in a limiting
sense, for large collections of high-dimensional data most one-dimensional
projections of the data are approximately Gaussian. This paper gives
quantitative versions of that result. For a set of deterministic vectors
$\{x_i\}_{i=1}^n$ in $\R^d$ with $n$ and $d$ fixed, let $\theta\in\s^{d-1}$ be
a random point of the sphere and let $\mu_n^\theta$ denote the random measure
which puts mass $\frac{1}{n}$ at each of the points
$\inprod{x_1}{\theta},...,\inprod{x_n}{\theta}$. For a fixed bounded Lipschitz
test function $f$, $Z$ a standard Gaussian random variable and $\sigma^2$ a
suitable constant, an explicit bound is derived for the quantity $\ds\P[|\int f
d\mu_n^\theta-\E f(\sigma Z)|&gt;\epsilon]$. A bound is also given for
$\ds\P[d_{BL}(\mu_n^\theta, N(0,\sigma^2))&gt;\epsilon]$, where $d_{BL}$ denotes
the bounded-Lipschitz distance, which yields a lower bound on the waiting time
to finding a non-Gaussian projection of the $\{x_i\}$ if directions are tried
independently and uniformly on $\s^{d-1}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.2935</identifier>
 <datestamp>2010-06-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.2935</id><created>2008-11-18</created><updated>2008-12-15</updated><authors><author><keyname>Geller</keyname><forenames>Daryl</forenames><affiliation>Stony Brook University</affiliation></author><author><keyname>Marinucci</keyname><forenames>Domenico</forenames><affiliation>University of Rome Tor Vergata</affiliation></author></authors><title>Spin Wavelets on the Sphere</title><categories>math.CA astro-ph math.DG math.ST stat.TH</categories><comments>37 pages</comments><msc-class>42C40, 60G60, 33C55, 14C21, 83F05, 58J05</msc-class><journal-ref>Journal of Fourier Analysis and Applications, online first (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, a rapidly growing literature has focussed on the
construction of wavelet systems to analyze functions defined on the sphere. Our
purpose in this paper is to generalize these constructions to situations where
sections of line bundles, rather than ordinary scalar-valued functions, are
considered. In particular, we propose {\em needlet-type spin wavelets} as an
extension of the needlet approach recently introduced by Narcowich, Petrushev
and Ward, and then considered for more general manifolds by Geller and Mayeli.
We discuss localization properties in the real and harmonic domains, and
investigate stochastic properties for the analysis of spin random fields. Our
results are strongly motivated by cosmological applications, in particular in
connection to the analysis of Cosmic Microwave Background polarization data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.3330</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.3330</id><created>2008-11-20</created><updated>2011-06-19</updated><authors><author><keyname>Bouzebda</keyname><forenames>Salim</forenames><affiliation>LSTA</affiliation></author><author><keyname>Zari</keyname><forenames>Tarek</forenames><affiliation>LSTA</affiliation></author></authors><title>Strong Approximation of Empirical Copula Processes by Gaussian Processes</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide the strong approximation of empirical copula processes by a
Gaussian process. In addition we establish a strong approximation of the
smoothed empirical copula processes and a law of iterated logarithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.3552</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.3552</id><created>2008-11-21</created><updated>2009-08-18</updated><authors><author><keyname>Hashorva</keyname><forenames>Enkelejd</forenames></author></authors><title>On the residual dependence index of elliptical distributions</title><categories>math.PR math.ST stat.TH</categories><comments>11 pages, case \theta=1 now included</comments><msc-class>60F05, 60G70</msc-class><doi>10.1016/j.spl.2010.03.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The residual dependence index of bivariate Gaussian distributions is
determined by the correlation coefficient. This tail index is of certain
statistical importance when extremes and related rare events of bivariate
samples with asymptotic independent components are being modeled. In this paper
we calculate the partial residual dependence indices of a multivariate
elliptical random vector assuming that the associated random radius is in the
Gumbel max-domain of attraction. Furthermore, we discuss the estimation of
these indices when the associated random radius possesses a Weibull-tail
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.4095</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.4095</id><created>2008-11-25</created><updated>2009-09-02</updated><authors><author><keyname>Vihola</keyname><forenames>Matti</forenames></author></authors><title>Grapham: Graphical Models with Adaptive Random Walk Metropolis
  Algorithms</title><categories>stat.CO</categories><comments>9 pages, 3 figures; added references, revised language, other minor
  changes</comments><journal-ref>Computational Statistics &amp; Data Analysis 54 (2010) 49-54</journal-ref><doi>10.1016/j.csda.2009.09.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently developed adaptive Markov chain Monte Carlo (MCMC) methods have been
applied successfully to many problems in Bayesian statistics. Grapham is a new
open source implementation covering several such methods, with emphasis on
graphical models for directed acyclic graphs. The implemented algorithms
include the seminal Adaptive Metropolis algorithm adjusting the proposal
covariance according to the history of the chain and a Metropolis algorithm
adjusting the proposal scale based on the observed acceptance probability.
Different variants of the algorithms allow one, for example, to use these two
algorithms together, employ delayed rejection and adjust several parameters of
the algorithms. The implemented Metropolis-within-Gibbs update allows arbitrary
sampling blocks. The software is written in C and uses a simple extension
language Lua in configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0811.4167</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0811.4167</id><created>2008-11-25</created><updated>2008-12-17</updated><authors><author><keyname>Zhang</keyname><forenames>Dabao</forenames></author><author><keyname>Lin</keyname><forenames>Yanzhu</forenames></author><author><keyname>Zhang</keyname><forenames>Min</forenames></author></authors><title>Penalized Orthogonal-Components Regression for Large p Small n Data</title><categories>stat.ME stat.ML</categories><comments>12 pages</comments><doi>10.1214/09-EJS354</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a penalized orthogonal-components regression (POCRE) for large p
small n data. Orthogonal components are sequentially constructed to maximize,
upon standardization, their correlation to the response residuals. A new
penalization framework, implemented via empirical Bayes thresholding, is
presented to effectively identify sparse predictors of each component. POCRE is
computationally efficient owing to its sequential construction of leading
sparse principal components. In addition, such construction offers other
properties such as grouping highly correlated predictors and allowing for
collinear or nearly collinear predictors. With multivariate responses, POCRE
can construct common components and thus build up latent-variable models for
large p small n data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.0159</identifier>
 <datestamp>2010-10-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.0159</id><created>2008-11-30</created><updated>2009-07-02</updated><authors><author><keyname>Novikov</keyname><forenames>Andrey</forenames></author></authors><title>Optimal sequential procedures with Bayes decision rules</title><categories>math.ST math.PR stat.ME stat.TH</categories><comments>Shortened version for print publication, 17 pages</comments><journal-ref>Kybernetika 46 (2010), no. 4, pp.754-770</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, a general problem of sequential statistical inference for
general discrete-time stochastic processes is considered. The problem is to
minimize an average sample number given that Bayesian risk due to incorrect
decision does not exceed some given bound. We characterize the form of optimal
sequential stopping rules in this problem. In particular, we have a
characterization of the form of optimal sequential decision procedures when the
Bayesian risk includes both the loss due to incorrect decision and the cost of
observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.0881</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.0881</id><created>2008-12-04</created><updated>2009-05-13</updated><authors><author><keyname>Hashorva</keyname><forenames>Enkelejd</forenames></author><author><keyname>Pakes</keyname><forenames>Anthony</forenames></author></authors><title>Distribution and asymptotics under beta random scaling</title><categories>math.PR math.ST stat.TH</categories><comments>12 pages</comments><msc-class>60F05, 60G70</msc-class><doi>10.1016/j.jmaa.2010.07.045</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let X,Y,B be three independent random variables such that $X$ has the same
distribution function as Y B. Assume that B is a Beta random variable with
positive parameters a,b and Y has distribution function H. Pakes and Navarro
(2007) show under some mild conditions that the distribution function H_{a,b}
of X determines H. Based on that result we derive in this paper a recursive
formula for calculation of H, if H_{a,b} is known. Furthermore, we investigate
the relation between the tail asymptotic behaviour of X and Y. We present three
applications of our asymptotic results concerning the extremes of two random
samples with underlying distribution functions H and H_{a,b}, respectively, and
the conditional limiting distribution of bivariate elliptical distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.1395</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.1395</id><created>2008-12-07</created><authors><author><keyname>Novikov</keyname><forenames>Andrey</forenames></author></authors><title>Optimal sequential testing of two simple hypotheses in presence of
  control variables</title><categories>math.ST math.PR stat.ME stat.TH</categories><comments>To be published in International Mathematical Forum, v. 3 (2008),
  no.41, 2025 - 2048</comments><journal-ref>International Mathematical Forum, v. 3 (2008), no.41, 2025 - 2048</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that at any stage of a statistical experiment a control variable $X$
that affects the distribution of the observed data $Y$ can be used. The
distribution of $Y$ depends on some unknown parameter $\theta$, and we consider
the classical problem of testing a simple hypothesis $H_0: \theta=\theta_0$
against a simple alternative $H_1: \theta=\theta_1$ allowing the data to be
controlled by $X$, in the following sequential context. The experiment starts
with assigning a value $X_1$ to the control variable and observing $Y_1$ as a
response. After some analysis, we choose another value $X_2$ for the control
variable, and observe $Y_2$ as a response, etc. It is supposed that the
experiment eventually stops, and at that moment a final decision in favour of
$H_0$ or $H_1$ is to be taken.
  In this article, our aim is to characterize the structure of optimal
sequential procedures, based on this type of data, for testing a simple
hypothesis against a simple alternative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.1804</identifier>
 <datestamp>2010-07-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.1804</id><created>2008-12-09</created><updated>2010-07-20</updated><authors><author><keyname>Finesso</keyname><forenames>Lorenzo</forenames></author><author><keyname>Spreij</keyname><forenames>Peter</forenames></author></authors><title>Approximate factor analysis model building via alternating I-divergence
  minimization</title><categories>math.PR math.ST stat.TH</categories><msc-class>62H25, 62B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a positive definite covariance matrix $\widehat \Sigma$, we strive to
construct an optimal \emph{approximate} factor analysis model $HH^\top +D$,
with $H$ having a prescribed number of columns and $D&gt;0$ diagonal. The
optimality criterion we minimize is the I-divergence between the corresponding
normal laws. Lifting the problem into a properly chosen larger space enables us
to derive an alternating minimization algorithm \`a la Csisz\'ar-Tusn\'ady for
the construction of the best approximation. The convergence properties of the
algorithm are studied, with special attention given to the case where $D$ is
singular.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.1938</identifier>
 <datestamp>2010-10-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.1938</id><created>2008-12-10</created><updated>2010-10-04</updated><authors><author><keyname>Sullivant</keyname><forenames>Seth</forenames></author><author><keyname>Talaska</keyname><forenames>Kelli</forenames></author><author><keyname>Draisma</keyname><forenames>Jan</forenames></author></authors><title>Trek separation for Gaussian graphical models</title><categories>stat.ML math.CO math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS760 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS760</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 3, 1665-1685</journal-ref><doi>10.1214/09-AOS760</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian graphical models are semi-algebraic subsets of the cone of positive
definite covariance matrices. Submatrices with low rank correspond to
generalizations of conditional independence constraints on collections of
random variables. We give a precise graph-theoretic characterization of when
submatrices of the covariance matrix have small rank for a general class of
mixed graphs that includes directed acyclic and undirected graphs as special
cases. Our new trek separation criterion generalizes the familiar
$d$-separation criterion. Proofs are based on the trek rule, the resulting
matrix factorizations and classical theorems of algebraic combinatorics on the
expansions of determinants of path polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.2548</identifier>
 <datestamp>2010-10-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.2548</id><created>2008-12-13</created><updated>2010-10-08</updated><authors><author><keyname>Lagers</keyname><forenames>Andreas N.</forenames></author></authors><title>Copulas for Markovian dependence</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ214 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ214</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 2, 331-342</journal-ref><doi>10.3150/09-BEJ214</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Copulas have been popular to model dependence for multivariate distributions,
but have not been used much in modelling temporal dependence of univariate time
series. This paper demonstrates some difficulties with using copulas even for
Markov processes: some tractable copulas such as mixtures between copulas of
complete co- and countermonotonicity and independence (Fr\'{e}chet copulas) are
shown to imply quite a restricted type of Markov process and Archimedean
copulas are shown to be incompatible with Markov chains. We also investigate
Markov chains that are spreadable or, equivalently, conditionally i.i.d.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.2749</identifier>
 <datestamp>2011-05-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.2749</id><created>2008-12-15</created><updated>2011-05-24</updated><authors><author><keyname>Degras</keyname><forenames>David</forenames></author></authors><title>Nonparametric estimation of a trend based upon sampled continuous
  processes</title><categories>math.ST stat.ME stat.TH</categories><comments>Published at Comptes Rendus Mathematiques de l'Academie des Sciences</comments><journal-ref>Comptes Rendus Mathematiques de l'Academie des Sciences, Serie I
  347 (2009) 191--194</journal-ref><doi>10.1016/j.crma.2008.12.016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let X be a second order random process indexed by a compact interval [0,T].
Assume that n independent realizations of X are observed on a fixed grid of p
time points. Under mild regularity assumptions on the sample paths of X, we
show the asymptotic normality of suitable nonparametric estimators of the trend
function mu = EX in the space C([0,T]) as n, p go to infinity and, using
Gaussian process theory, we derive approximate simultaneous confidence bands
for mu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.2818</identifier>
 <datestamp>2010-11-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.2818</id><created>2008-12-15</created><updated>2010-11-10</updated><authors><author><keyname>Rosenbaum</keyname><forenames>Mathieu</forenames></author><author><keyname>Tsybakov</keyname><forenames>Alexandre B.</forenames></author></authors><title>Sparse recovery under matrix uncertainty</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS793 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS793</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 5, 2620-2651</journal-ref><doi>10.1214/10-AOS793</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the model {eqnarray*}y=X\theta^*+\xi, Z=X+\Xi,{eqnarray*} where
the random vector $y\in\mathbb{R}^n$ and the random $n\times p$ matrix $Z$ are
observed, the $n\times p$ matrix $X$ is unknown, $\Xi$ is an $n\times p$ random
noise matrix, $\xi\in\mathbb{R}^n$ is a noise independent of $\Xi$, and
$\theta^*$ is a vector of unknown parameters to be estimated. The matrix
uncertainty is in the fact that $X$ is observed with additive error. For
dimensions $p$ that can be much larger than the sample size $n$, we consider
the estimation of sparse vectors $\theta^*$. Under matrix uncertainty, the
Lasso and Dantzig selector turn out to be extremely unstable in recovering the
sparsity pattern (i.e., of the set of nonzero components of $\theta^*$), even
if the noise level is very small. We suggest new estimators called matrix
uncertainty selectors (or, shortly, the MU-selectors) which are close to
$\theta^*$ in different norms and in the prediction risk if the restricted
eigenvalue assumption on $X$ is satisfied. We also show that under somewhat
stronger assumptions, these estimators recover correctly the sparsity pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.2936</identifier>
 <datestamp>2013-12-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.2936</id><created>2008-12-15</created><updated>2013-12-20</updated><authors><author><keyname>Porcu</keyname><forenames>Emilio</forenames></author><author><keyname>Schilling</keyname><forenames>Ren L.</forenames></author></authors><title>From Schoenberg to Pick-Nevanlinna: Toward a complete picture of the
  variogram class</title><categories>math.ST math.PR stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ277 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm). With Addendum</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ277</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 1, 441-455</journal-ref><doi>10.3150/10-BEJ277</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a large subclass of variograms is closed under products and that
some desirable stability properties, such as the product of special
compositions, can be obtained within the proposed setting. We introduce new
classes of kernels of Schoenberg-L\'{e}vy type and demonstrate some important
properties of rotationally invariant variograms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.2987</identifier>
 <datestamp>2013-09-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.2987</id><created>2008-12-16</created><authors><author><keyname>Aguirre</keyname><forenames>Alberto Carabarin</forenames></author><author><keyname>Ivanoff</keyname><forenames>B. Gail</forenames></author></authors><title>Hazard Estimation under Generalized Censoring</title><categories>math.ST stat.TH</categories><comments>Submitted to the Electronic Journal of Statistics
  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics
  (http://www.imstat.org)</comments><proxy>vtex</proxy><msc-class>62G05, 60G42, 60G55 (Primary)</msc-class><journal-ref>Electronic Journal of Statistics, Vol. 3 (2009) 349-375</journal-ref><doi>10.1214/09-EJS340</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the problem of the estimation of the cumulative hazard
function of a distribution on a general complete separable metric space when
the data points are subject to censoring by an arbitrary adapted random set. A
problem involving observability of the estimator proposed in [8] and [9] is
resolved and a functional central limit theorem is proven for the revised
estimator. Several examples and applications are discussed, and the validity of
bootstrap methods is established in each case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.3102</identifier>
 <datestamp>2011-12-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.3102</id><created>2008-12-16</created><updated>2011-12-15</updated><authors><author><keyname>Papavasiliou</keyname><forenames>Anastasia</forenames></author><author><keyname>Ladroue</keyname><forenames>Christophe</forenames></author></authors><title>Parameter estimation for rough differential equations</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/11-AOS893 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS893</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 4, 2047-2073</journal-ref><doi>10.1214/11-AOS893</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct the "expected signature matching" estimator for differential
equations driven by rough paths and we prove its consistency and asymptotic
normality. We use it to estimate parameters of a diffusion and a fractional
diffusions, that is, a differential equation driven by fractional Brownian
motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.3141</identifier>
 <datestamp>2010-07-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.3141</id><created>2008-12-16</created><updated>2010-06-04</updated><authors><author><keyname>Arlot</keyname><forenames>Sylvain</forenames><affiliation>LIENS, INRIA Rocquencourt</affiliation></author></authors><title>Choosing a penalty for model selection in heteroscedastic regression</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of choosing between several models in least-squares
regression with heteroscedastic data. We prove that any penalization procedure
is suboptimal when the penalty is a function of the dimension of the model, at
least for some typical heteroscedastic model selection problems. In particular,
Mallows' Cp is suboptimal in this framework. On the contrary, optimal model
selection is possible with data-driven penalties such as resampling or $V$-fold
penalties. Therefore, it is worth estimating the shape of the penalty from
data, even at the price of a higher computational cost. Simulation experiments
illustrate the existence of a trade-off between statistical accuracy and
computational complexity. As a conclusion, we sketch some rules for choosing a
penalty in least-squares regression, depending on what is known about possible
variations of the noise-level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.3502</identifier>
 <datestamp>2010-10-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.3502</id><created>2008-12-18</created><updated>2010-10-20</updated><authors><author><keyname>Bigot</keyname><forenames>Jrmie</forenames></author><author><keyname>Gadat</keyname><forenames>Sbastien</forenames></author></authors><title>A deconvolution approach to estimation of a common shape in a shifted
  curves model</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS800 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS800</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 4, 2422-2464</journal-ref><doi>10.1214/10-AOS800</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of adaptive estimation of a mean pattern in
a randomly shifted curve model. We show that this problem can be transformed
into a linear inverse problem, where the density of the random shifts plays the
role of a convolution operator. An adaptive estimator of the mean pattern,
based on wavelet thresholding is proposed. We study its consistency for the
quadratic risk as the number of observed curves tends to infinity, and this
estimator is shown to achieve a near-minimax rate of convergence over a large
class of Besov balls. This rate depends both on the smoothness of the common
shape of the curves and on the decay of the Fourier coefficients of the density
of the random shifts. Hence, this paper makes a connection between mean pattern
estimation and the statistical analysis of linear inverse problems, which is a
new point of view on curve registration and image warping problems. We also
provide a new method to estimate the unknown random shifts between curves. Some
numerical experiments are given to illustrate the performances of our approach
and to compare them with another algorithm existing in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.3632</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.3632</id><created>2008-12-18</created><updated>2009-12-13</updated><authors><author><keyname>Sarnowski</keyname><forenames>Wojciech</forenames></author><author><keyname>Szajowski</keyname><forenames>Krzysztof</forenames></author></authors><title>Optimal detection of homogeneous segment of observations in stochastic
  sequence</title><categories>math.PR cs.IT math.IT math.ST stat.TH</categories><comments>13 pages</comments><report-no>Institute of Mathematics, Polish Academy of Science 696</report-no><msc-class>60G40 60K99, 90D60</msc-class><journal-ref>Stochastics An International Journal of Probability and Stochastic
  Processes, Vol. 83, Issue 4-6, 2011, pp. 569-581</journal-ref><doi>10.1080/17442508.2010.540015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Markov process is registered. At random moment $\theta$ the distribution of
observed sequence changes. Using probability maximizing approach the optimal
stopping rule for detecting the change is identified. Some explicit solution is
obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.3672</identifier>
 <datestamp>2010-10-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.3672</id><created>2008-12-18</created><updated>2010-10-11</updated><authors><author><keyname>Breton</keyname><forenames>Jean-Christophe</forenames></author><author><keyname>Houdr</keyname><forenames>Christian</forenames></author></authors><title>Asymptotics for random Young diagrams when the word length and alphabet
  size simultaneously grow to infinity</title><categories>math.PR math-ph math.MP math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ218 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ218</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 2, 471-492</journal-ref><doi>10.3150/09-BEJ218</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a random word of size $n$ whose letters are drawn independently from an
ordered alphabet of size $m$, the fluctuations of the shape of the random RSK
Young tableaux are investigated, when $n$ and $m$ converge together to
infinity. If $m$ does not grow too fast and if the draws are uniform, then the
limiting shape is the same as the limiting spectrum of the GUE. In the
non-uniform case, a control of both highest probabilities will ensure the
convergence of the first row of the tableau toward the Tracy--Widom
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.3699</identifier>
 <datestamp>2010-06-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.3699</id><created>2008-12-18</created><updated>2010-06-17</updated><authors><author><keyname>Zhang</keyname><forenames>Li-Xin</forenames></author><author><keyname>Hu</keyname><forenames>Feifang</forenames></author><author><keyname>Cheung</keyname><forenames>Siu Hung</forenames></author><author><keyname>Chan</keyname><forenames>Wei Sum</forenames></author></authors><title>Multi-color Randomly Reinforced Urn for Adaptive Designs</title><categories>stat.ME math.PR math.ST stat.TH</categories><comments>60F15</comments><msc-class>62L05, 60F15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is withdrawn
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.4105</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.4105</id><created>2008-12-22</created><updated>2009-10-13</updated><authors><author><keyname>Hashorva</keyname><forenames>Enkelejd</forenames></author></authors><title>Asymptotics of the Norm of Elliptical Random Vectors</title><categories>math.PR math.ST stat.TH</categories><comments>11 pages</comments><msc-class>60G70</msc-class><journal-ref>Journal of Multivariate Analysis. 101, 4, 926-935, 2010</journal-ref><doi>10.1016/j.jmva.2009.10.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider elliptical random vectors X in R^d,d&gt;1 with
stochastic representation A R U where R is a positive random radius independent
of the random vector U which is uniformly distributed on the unit sphere of R^d
and A is a given matrix. The main result of this paper is an asymptotic
expansion of the tail probability of the norm of X derived under the assumption
that R has distribution function is in the Gumbel or the Weibull max-domain of
attraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.4166</identifier>
 <datestamp>2010-01-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.4166</id><created>2008-12-22</created><updated>2010-01-08</updated><authors><author><keyname>Lavancier</keyname><forenames>Frdric</forenames><affiliation>LMJL</affiliation></author><author><keyname>Philippe</keyname><forenames>Anne</forenames><affiliation>LMJL</affiliation></author></authors><title>Some convergence results on quadratic forms for random fields and
  application to empirical covariances</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00348833</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Limit theorems are proved for quadratic forms of Gaussian random fields in
presence of long memory. We obtain a non central limit theorem under a minimal
integrability condition, which allows isotropic and anisotropic models. We
apply our limit theorems and those of Ginovian (99) to obtain the asymptotic
behavior of the empirical covariances of Gaussian fields, which is a particular
example of quadratic forms. We show that it is possible to obtain a Gaussian
limit when the spectral density is not in $L^2$. Therefore the dichotomy
observed in dimension $d=1$ between central and non central limit theorems
cannot be stated so easily due to possible anisotropic strong dependence in
$d&gt;1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.4701</identifier>
 <datestamp>2010-02-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.4701</id><created>2008-12-26</created><updated>2010-02-28</updated><authors><author><keyname>Little</keyname><forenames>Mark P.</forenames></author><author><keyname>Heidenreich</keyname><forenames>Wolfgang F.</forenames></author><author><keyname>Li</keyname><forenames>Guangquan</forenames></author></authors><title>Parameter identifiability and redundancy: theoretical considerations</title><categories>math.ST stat.ME stat.TH</categories><comments>This is now exactly as per the version of the article published in
  Little et al. (PLoS ONE 2010 5 (1)e8915)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we outline general considerations on parameter identifiability,
and introduce the notion of weak local identifiability and gradient weak local
identifiability. These are based on local properties of the likelihood, in
particular the rank of the Hessian matrix. We relate these to the notions of
parameter identifiability and redundancy previously introduced by Rothenberg
(Econometrica 39 (1971) 577-591) and Catchpole and Morgan (Biometrika 84 (1997)
187-196). Within the exponential family parameter irredundancy, local
identifiability, gradient weak local identifiability and weak local
identifiability are shown to be equivalent. We consider applications to a
recently developed class of cancer models of Little and Wright (Math
Biosciences 183 (2003) 111-134) and Little et al. (J Theoret Biol 254 (2008)
229-238) that generalize a large number of other recently used quasi-biological
cancer models, in particular those of Armitage and Doll (Br J Cancer 8 (1954)
1-12) and the two-mutation model (Moolgavkar and Venzon Math Biosciences 47
(1979) 55-77).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0812.5087</identifier>
 <datestamp>2010-10-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0812.5087</id><created>2008-12-30</created><updated>2010-10-20</updated><authors><author><keyname>Kolar</keyname><forenames>Mladen</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author><author><keyname>Ahmed</keyname><forenames>Amr</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Estimating time-varying networks</title><categories>stat.ML q-bio.MN q-bio.QM stat.AP stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOAS308 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS308</report-no><journal-ref>Annals of Applied Statistics 2010, Vol. 4, No. 1, 94-123</journal-ref><doi>10.1214/09-AOAS308</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic networks are a plausible representation of the relational
information among entities in dynamic systems such as living cells or social
communities. While there is a rich literature in estimating a static or
temporally invariant network from observation data, little has been done toward
estimating time-varying networks from time series of entity attributes. In this
paper we present two new machine learning methods for estimating time-varying
networks, which both build on a temporally smoothed $l_1$-regularized logistic
regression formalism that can be cast as a standard convex-optimization problem
and solved efficiently using generic solvers scalable to large networks. We
report promising results on recovering simulated time-varying networks. For
real data sets, we reverse engineer the latent sequence of temporally rewiring
political networks between Senators from the US Senate voting records and the
latent evolving regulatory networks underlying 588 genes across the life cycle
of Drosophila melanogaster from the microarray time course.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.0017</identifier>
 <datestamp>2011-06-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.0017</id><created>2008-12-30</created><updated>2011-06-01</updated><authors><author><keyname>Chrtien</keyname><forenames>Stphane</forenames></author><author><keyname>Hero</keyname><forenames>Alfred</forenames></author><author><keyname>Perdry</keyname><forenames>Herv</forenames></author></authors><title>Space Alternating Penalized Kullback Proximal Point Algorithms for
  Maximizing Likelihood with Nondifferentiable Penalty</title><categories>stat.CO stat.ME</categories><comments>3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The EM algorithm is a widely used methodology for penalized likelihood
estimation. Provable monotonicity and convergence are the hallmarks of the EM
algorithm and these properties are well established for smooth likelihood and
smooth penalty functions. However, many relaxed versions of variable selection
penalties are not smooth. The goal of this paper is to introduce a new class of
Space Alternating Penalized Kullback Proximal extensions of the EM algorithm
for nonsmooth likelihood inference. We show that the cluster points of the new
method are stationary points even when on the boundary of the parameter set.
Special attention has been paid to the construction of component-wise version
of the method in order to ease the implementation for complicated models.
Illustration for the problems of model selection for finite mixtures of
regression and to sparse image reconstruction is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.0135</identifier>
 <datestamp>2010-11-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.0135</id><created>2008-12-31</created><updated>2010-11-08</updated><authors><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author><author><keyname>Fu</keyname><forenames>Wenjie</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author></authors><title>A state-space mixed membership blockmodel for dynamic network tomography</title><categories>stat.ML q-bio.MN q-bio.QM stat.AP stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOAS311 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS311</report-no><journal-ref>Annals of Applied Statistics 2010, Vol. 4, No. 2, 535-566</journal-ref><doi>10.1214/09-AOAS311</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a dynamic social or biological environment, the interactions between the
actors can undergo large and systematic changes. In this paper we propose a
model-based approach to analyze what we will refer to as the dynamic tomography
of such time-evolving networks. Our approach offers an intuitive but powerful
tool to infer the semantic underpinnings of each actor, such as its social
roles or biological functions, underlying the observed network topologies. Our
model builds on earlier work on a mixed membership stochastic blockmodel for
static networks, and the state-space model for tracking object trajectory. It
overcomes a major limitation of many current network inference techniques,
which assume that each actor plays a unique and invariant role that accounts
for all its interactions with other actors; instead, our method models the role
of each actor as a time-evolving mixed membership vector that allows actors to
behave differently over time and carry out different roles/functions when
interacting with different peers, which is closer to reality. We present an
efficient algorithm for approximate inference and learning using our model; and
we applied our model to analyze a social network between monks (i.e., the
Sampson's network), a dynamic email communication network between the Enron
employees, and a rewiring gene interaction network of fruit fly collected
during its full life cycle. In all cases, our model reveals interesting
patterns of the dynamic roles of the actors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.0264</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.0264</id><created>2009-01-02</created><updated>2011-03-02</updated><authors><author><keyname>Mas</keyname><forenames>Andr</forenames><affiliation>I3M</affiliation></author></authors><title>Representation of small ball probabilities in Hilbert space and lower
  bound in regression for functional data</title><categories>math.PR math.ST stat.TH</categories><proxy>ccsd</proxy><journal-ref>Electronic Journal of Statistics (2012) 6, 1745-1778</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $S=\sum_{i=1}^{+\infty}\lambda_{i}Z_{i}$ where the $Z_{i}$'s are i.d.d.
positive with $\mathbb{E}\| Z\| ^{3}&lt;+\infty$ and
$(\lambda_{i})_{i\in\mathbb{N}}$ a positive nonincreasing sequence such that
$\sum\lambda_{i}&lt;+\infty$. We study the small ball probability
$\mathbb{P}(S&lt;\epsilon) $ when $\epsilon\downarrow0$. We start from a result by
Lifshits (1997) who computed this probability by means of the Laplace transform
of $S$. We prove that $\mathbb{P}(S&lt;\cdot) $ belongs to a class of functions
introduced by de Haan, well-known in extreme value theory, the class of
Gamma-varying functions, for which an exponential-integral representation is
available. This approach allows to derive bounds for the rate in nonparametric
regression for functional data at a fixed point $x_{0}$ :
$\mathbb{E}(y|X=x_{0}%) $ where $(y_{i},X_{i})_{1\leq i\leq n}$ is a sample in
$(\mathbb{R},\mathcal{F}) $ and $\mathcal{F}$ is some space of functions. It
turns out that, in a general framework, the minimax lower bound for the risk is
of order $(\log n)^{-\tau}$ for some $\tau&gt;0$ depending on the regularity of
the data and polynomial rates cannot be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.0638</identifier>
 <datestamp>2011-12-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.0638</id><created>2009-01-06</created><updated>2011-12-07</updated><authors><author><keyname>Shaw</keyname><forenames>William T.</forenames></author><author><keyname>Luu</keyname><forenames>Thomas</forenames></author><author><keyname>Brickman</keyname><forenames>Nick</forenames></author></authors><title>Quantile Mechanics II: Changes of Variables in Monte Carlo methods and
  GPU-Optimized Normal Quantiles</title><categories>q-fin.CP q-fin.RM q-fin.ST stat.AP stat.CO</categories><comments>This revision adds substantial discussion of precision and
  optimization issues, new code for float and double precision operation.
  Timings for GTX 285, 480, Quadro 4000, Tesla C2050, and comparisons with most
  major competing approaches</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents differential equations and solution methods for the
functions of the form $Q(x) = F^{-1}(G(x))$, where $F$ and $G$ are cumulative
distribution functions. Such functions allow the direct recycling of Monte
Carlo samples from one distribution into samples from another. The method may
be developed analytically for certain special cases, and illuminate the idea
that it is a more precise form of the traditional Cornish-Fisher expansion. In
this manner the model risk of distributional risk may be assessed free of the
Monte Carlo noise associated with resampling. Examples are given of equations
for converting normal samples to Student t, and converting exponential to
hyperbolic, variance gamma and normal. In the case of the normal distribution,
the change of variables employed allows the sampling to take place to good
accuracy based on a single rational approximation over a very wide range of the
sample space. The avoidance of any branching statement is of use in optimal GPU
computations as it avoids the effect of {\it warp divergence}, and we give
examples of branch-free normal quantiles that offer performance improvements in
a GPU environment, while retaining the best precision characteristics of
well-known methods. We also offer models based on a low-probability of warp
divergence. Comparisons of new and old forms are made on the Nvidia Quadro
4000, GTX 285 and 480, and Tesla C2050 GPUs. We argue that in single-precision
mode, the change-of-variables approach offers performance competitive with the
fastest existing scheme while substantially improving precision, and that in
double-precision mode, this approach offers the most GPU-optimal Gaussian
quantile yet, and without compromise on precision for Monte Carlo applications,
working twice as fast as the CUDA 4 library function with increased precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.0684</identifier>
 <datestamp>2010-01-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.0684</id><created>2009-01-06</created><updated>2010-01-30</updated><authors><author><keyname>Tiruneh</keyname><forenames>Gizachew</forenames></author></authors><title>Age and Winning Professional Golf Tournaments</title><categories>stat.AP</categories><comments>Introduction and regression analyses added; 16 pages; 14 tables and
  figures; typos in the conclusions part edited; conclusions sections edited;
  typo edited in conclusions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most professional golfers and analysts think that winning on the PGA Tour
peaks when golfers are in their thirties. Rather than relying on educated
guesses, we can actually use available statistical data to determine the actual
ages at which golfers peak their golf game. We can also test the hypothesis
that age affects winning professional golf tournaments. Using data available
from the Golf Channel, the PGA Tour, and LPGA Tour, I calculated and provided
the mean, the median, and the mode ages at which professional golfers on the
PGA, European PGA, Champions, and LPGA Tours had won over a five-year period.
More specifically, the ages at which golfers on the PGA, European PGA,
Champions Tour, and LPGA Tours peak their wins are 35, 30, 52, and 25,
respectively. The regression analyses I conducted seem to support my hypothesis
that age affects winning professional golf tournaments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.0876</identifier>
 <datestamp>2011-03-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.0876</id><created>2009-01-07</created><authors><author><keyname>Pitsoulis</keyname><forenames>L.</forenames></author><author><keyname>Zioutas</keyname><forenames>G.</forenames></author></authors><title>A Fast Algorithm for Robust Regression with Penalised Trimmed Squares</title><categories>stat.CO</categories><comments>27 pages</comments><journal-ref>Computational Statistics, Volume 25, Number 4, 663-689, 2010</journal-ref><doi>10.1007/s00180-010-0196-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The presence of groups containing high leverage outliers makes linear
regression a difficult problem due to the masking effect. The available high
breakdown estimators based on Least Trimmed Squares often do not succeed in
detecting masked high leverage outliers in finite samples.
  An alternative to the LTS estimator, called Penalised Trimmed Squares (PTS)
estimator, was introduced by the authors in \cite{ZiouAv:05,ZiAvPi:07} and it
appears to be less sensitive to the masking problem. This estimator is defined
by a Quadratic Mixed Integer Programming (QMIP) problem, where in the objective
function a penalty cost for each observation is included which serves as an
upper bound on the residual error for any feasible regression line. Since the
PTS does not require presetting the number of outliers to delete from the data
set, it has better efficiency with respect to other estimators. However, due to
the high computational complexity of the resulting QMIP problem, exact
solutions for moderately large regression problems is infeasible.
  In this paper we further establish the theoretical properties of the PTS
estimator, such as high breakdown and efficiency, and propose an approximate
algorithm called Fast-PTS to compute the PTS estimator for large data sets
efficiently. Extensive computational experiments on sets of benchmark instances
with varying degrees of outlier contamination, indicate that the proposed
algorithm performs well in identifying groups of high leverage outliers in
reasonable computational time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.1342</identifier>
 <datestamp>2010-04-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.1342</id><created>2009-01-11</created><updated>2009-11-13</updated><authors><author><keyname>Shalizi</keyname><forenames>Cosma Rohilla</forenames></author></authors><title>Dynamics of Bayesian Updating with Dependent Data and Misspecified
  Models</title><categories>math.ST q-bio.PE stat.TH</categories><comments>36 pages, 1 figure. v2: typo fixes, minor formatting changes. v3:
  Improved notation, added references, new theorem on convergence rates. v4:
  minor changes to text, added references. v5: Minor typo corrections; matches
  journal version except for format details</comments><msc-class>62C10; 62G20; 62M09; 60F10; 62M05; 92D15; 94A17</msc-class><journal-ref>_Electronic Journal of Statistics_, vol. 3 (2009): 1039--1074</journal-ref><doi>10.1214/09-EJS485</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much is now known about the consistency of Bayesian updating on
infinite-dimensional parameter spaces with independent or Markovian data.
Necessary conditions for consistency include the prior putting enough weight on
the correct neighborhoods of the data-generating distribution; various
sufficient conditions further restrict the prior in ways analogous to capacity
control in frequentist nonparametrics. The asymptotics of Bayesian updating
with mis-specified models or priors, or non-Markovian data, are far less well
explored. Here I establish sufficient conditions for posterior convergence when
all hypotheses are wrong, and the data have complex dependencies. The main
dynamical assumption is the asymptotic equipartition (Shannon-McMillan-Breiman)
property of information theory. This, along with Egorov's Theorem on uniform
convergence, lets me build a sieve-like structure for the prior. The main
statistical assumption, also a form of capacity control, concerns the
compatibility of the prior and the data-generating process, controlling the
fluctuations in the log-likelihood when averaged over the sieve-like sets. In
addition to posterior convergence, I derive a kind of large deviations
principle for the posterior measure, extending in some cases to rates of
convergence, and discuss the advantages of predicting using a combination of
models known to be wrong. An appendix sketches connections between these
results and the replicator dynamics of evolutionary theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.1378</identifier>
 <datestamp>2010-10-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.1378</id><created>2009-01-10</created><updated>2010-10-15</updated><authors><author><keyname>Atchad</keyname><forenames>Yves F.</forenames></author></authors><title>A cautionary tale on the efficiency of some adaptive Monte Carlo schemes</title><categories>stat.CO math.PR math.ST stat.ME stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AAP636 the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP636</report-no><journal-ref>Annals of Applied Probability 2010, Vol. 20, No. 3, 841-868</journal-ref><doi>10.1214/09-AAP636</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a growing interest in the literature for adaptive Markov chain Monte
Carlo methods based on sequences of random transition kernels $\{P_n\}$ where
the kernel $P_n$ is allowed to have an invariant distribution $\pi_n$ not
necessarily equal to the distribution of interest $\pi$ (target distribution).
These algorithms are designed such that as $n\to\infty$, $P_n$ converges to
$P$, a kernel that has the correct invariant distribution $\pi$. Typically, $P$
is a kernel with good convergence properties, but one that cannot be directly
implemented. It is then expected that the algorithm will inherit the good
convergence properties of $P$. The equi-energy sampler of [Ann. Statist. 34
(2006) 1581--1619] is an example of this type of adaptive MCMC. We show in this
paper that the asymptotic variance of this type of adaptive MCMC is always at
least as large as the asymptotic variance of the Markov chain with transition
kernel $P$. We also show by simulation that the difference can be substantial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.1911</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.1911</id><created>2009-01-13</created><authors><author><keyname>Kabaila</keyname><forenames>Paul</forenames></author><author><keyname>Syuhada</keyname><forenames>Khreshna</forenames></author></authors><title>The Asymptotic Efficiency of Improved Prediction Intervals</title><categories>math.ST stat.TH</categories><journal-ref>Statistics and Probability Letters, 80, 1348-1353 (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Barndorff-Nielsen and Cox (1994, p.319) modify an estimative prediction limit
to obtain an improved prediction limit with better coverage properties. Kabaila
and Syuhada (2008) present a simulation-based approximation to this improved
prediction limit, which avoids the extensive algebraic manipulations required
for this modification. We present a modification of an estimative prediction
interval, analogous to the Barndorff-Nielsen and Cox modification, to obtain an
improved prediction interval with better coverage properties. We also present
an analogue, for the prediction interval context, of this simulation-based
approximation. The parameter estimator on which the estimative and improved
prediction limits and intervals are based is assumed to have the same
asymptotic distribution as the (conditional) maximum likelihood estimator. The
improved prediction limit and interval depend on the asymptotic conditional
bias of this estimator. This bias can be very sensitive to very small changes
in the estimator. It may require considerable effort to find this bias. We
show, however, that the improved prediction limit and interval have asymptotic
efficiencies that are functionally independent of this bias. Thus, improved
prediction limits and intervals obtained using the Barndorff-Nielsen and Cox
type of methodology can conveniently be based on the (conditional) maximum
likelihood estimator, whose asymptotic conditional bias is given by the formula
of Vidoni (2004, p.144). Also, improved prediction limits and intervals
obtained using Kabaila and Syuhada type approximations have asymptotic
efficiencies that are independent of the estimator on which these intervals are
based.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.2044</identifier>
 <datestamp>2010-10-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.2044</id><created>2009-01-14</created><updated>2010-10-21</updated><authors><author><keyname>Bunea</keyname><forenames>Florentina</forenames></author><author><keyname>Tsybakov</keyname><forenames>Alexandre B.</forenames></author><author><keyname>Wegkamp</keyname><forenames>Marten H.</forenames></author><author><keyname>Barbu</keyname><forenames>Adrian</forenames></author></authors><title>SPADES and mixture models</title><categories>math.ST stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS790 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS790</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 4, 2525-2558</journal-ref><doi>10.1214/09-AOS790</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies sparse density estimation via $\ell_1$ penalization
(SPADES). We focus on estimation in high-dimensional mixture models and
nonparametric adaptive density estimation. We show, respectively, that SPADES
can recover, with high probability, the unknown components of a mixture of
probability densities and that it yields minimax adaptive density estimates.
These results are based on a general sparsity oracle inequality that the SPADES
estimates satisfy. We offer a data driven method for the choice of the tuning
parameter used in the construction of SPADES. The method uses the generalized
bisection method first introduced in \citebb09. The suggested procedure
bypasses the need for a grid search and offers substantial computational
savings. We complement our theoretical results with a simulation study that
employs this method for approximations of one and two-dimensional densities
with mixtures. The numerical results strongly support our theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.2234</identifier>
 <datestamp>2010-08-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.2234</id><created>2009-01-15</created><authors><author><keyname>Haufe</keyname><forenames>Stefan</forenames></author><author><keyname>Nolte</keyname><forenames>Guido</forenames></author><author><keyname>Mueller</keyname><forenames>Klaus-Robert</forenames></author><author><keyname>Kraemer</keyname><forenames>Nicole</forenames></author></authors><title>Sparse Causal Discovery in Multivariate Time Series</title><categories>stat.ME stat.AP stat.ML</categories><comments>to appear in Journal of Machine Learning Research, Proceedings of the
  NIPS'08 workshop on Causality</comments><journal-ref>JMLR Workshop and Conference Proceedings 6: Causality: Objectives
  and Assessment (NIPS 2008), 97 - 106</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our goal is to estimate causal interactions in multivariate time series.
Using vector autoregressive (VAR) models, these can be defined based on
non-vanishing coefficients belonging to respective time-lagged instances. As in
most cases a parsimonious causality structure is assumed, a promising approach
to causal discovery consists in fitting VAR models with an additional
sparsity-promoting regularization. Along this line we here propose that
sparsity should be enforced for the subgroups of coefficients that belong to
each pair of time series, as the absence of a causal relation requires the
coefficients for all time-lags to become jointly zero. Such behavior can be
achieved by means of l1-l2-norm regularized regression, for which an efficient
active set solver has been proposed recently. Our method is shown to outperform
standard methods in recovering simulated causality graphs. The results are on
par with a second novel approach which uses multiple statistical testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.2445</identifier>
 <datestamp>2011-05-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.2445</id><created>2009-01-16</created><updated>2011-05-09</updated><authors><author><keyname>Chen</keyname><forenames>Louis H. Y.</forenames></author><author><keyname>Xia</keyname><forenames>Aihua</forenames></author></authors><title>Poisson process approximation for dependent superposition of point
  processes</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ290 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ290</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 2, 530-544</journal-ref><doi>10.3150/10-BEJ290</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the study of weak convergence of superpositions of point processes
to the Poisson process dates back to the work of Grigelionis in 1963, it was
only recently that Schuhmacher [Stochastic Process. Appl. 115 (2005)
1819--1837] obtained error bounds for the weak convergence. Schuhmacher
considered dependent superposition, truncated the individual point processes to
0--1 point processes and then applied Stein's method to the latter. In this
paper, we adopt a different approach to the problem by using Palm theory and
Stein's method, thereby expressing the error bounds in terms of the mean
measures of the individual point processes, which is not possible with
Schuhmacher's approach. We consider locally dependent superposition as a
generalization of the locally dependent point process introduced in Chen and
Xia [Ann. Probab. 32 (2004) 2545--2569] and apply the main theorem to the
superposition of thinned point processes and of renewal processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.2593</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.2593</id><created>2009-01-19</created><updated>2011-07-14</updated><authors><author><keyname>Rysland</keyname><forenames>Kjetil</forenames></author></authors><title>A martingale approach to continuous-time marginal structural models</title><categories>math.ST math.PR stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ303 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ303</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 3, 895-915</journal-ref><doi>10.3150/10-BEJ303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Marginal structural models were introduced in order to provide estimates of
causal effects from interventions based on observational studies in
epidemiological research. The key point is that this can be understood in terms
of Girsanov's change of measure. This offers a mathematical interpretation of
marginal structural models that has not been available before. We consider both
a model of an observational study and a model of a hypothetical randomized
trial. These models correspond to different martingale measures -- the
observational measure and the randomized trial measure -- on some underlying
space. We describe situations where the randomized trial measure is absolutely
continuous with respect to the observational measure. The resulting
continuous-time likelihood ratio process with respect to these two probability
measures corresponds to the weights in discrete-time marginal structural
models. In order to do inference for the hypothetical randomized trial, we can
simulate samples using observational data weighted by this likelihood ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.2808</identifier>
 <datestamp>2011-10-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.2808</id><created>2009-01-19</created><updated>2009-04-16</updated><authors><author><keyname>Ayache</keyname><forenames>Antoine</forenames><affiliation>LPP</affiliation></author><author><keyname>Bertrand</keyname><forenames>Pierre R.</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author></authors><title>A process very similar to multifractional Brownian motion</title><categories>stat.ME math.PR</categories><comments>18 pages</comments><proxy>ccsd hal-00354081</proxy><journal-ref>Recent Developments in Fractals and Related Fields (2010) 311--326</journal-ref><doi>10.1007/978-0-8176-4888-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Ayache and Taqqu (2005), the multifractional Brownian (mBm) motion is
obtained by replacing the constant parameter $H$ of the fractional Brownian
motion (fBm) by a smooth enough functional parameter $H(.)$ depending on the
time $t$. Here, we consider the process $Z$ obtained by replacing in the
wavelet expansion of the fBm the index $H$ by a function $H(.)$ depending on
the dyadic point $k/2^j$. This process was introduced in Benassi et al (2000)
to model fBm with piece-wise constant Hurst index and continuous paths. In this
work, we investigate the case where the functional parameter satisfies an
uniform H\"older condition of order $\beta&gt;\sup_{t\in \rit} H(t)$ and ones
shows that, in this case, the process $Z$ is very similar to the mBm in the
following senses: i) the difference between $Z$ and a mBm satisfies an uniform
H\"older condition of order $d&gt;\sup_{t\in \R} H(t)$; ii) as a by product, one
deduces that at each point $t\in \R$ the pointwise H\"older exponent of $Z$ is
$H(t)$ and that $Z$ is tangent to a fBm with Hurst parameter $H(t)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.2951</identifier>
 <datestamp>2012-01-31</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.2951</id><created>2009-01-19</created><updated>2011-05-03</updated><authors><author><keyname>Mandel</keyname><forenames>Jan</forenames></author><author><keyname>Cobb</keyname><forenames>Loren</forenames></author><author><keyname>Beezley</keyname><forenames>Jonathan D.</forenames></author></authors><title>On the Convergence of the Ensemble Kalman Filter</title><categories>math.ST math.PR physics.ao-ph stat.TH</categories><comments>Revised, 8 pages</comments><report-no>UCD CCM Report 278</report-no><msc-class>62F12, 62C12</msc-class><journal-ref>Applications of Mathematics 56, 533-541, 2011</journal-ref><doi>10.1007/s10492-011-0031-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convergence of the ensemble Kalman filter in the limit for large ensembles to
the Kalman filter is proved. In each step of the filter, convergence of the
ensemble sample covariance follows from a weak law of large numbers for
exchangeable random variables, the continuous mapping theorem gives convergence
in probability of the ensemble members, and $L^p$ bounds on the ensemble then
give $L^p$ convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.3471</identifier>
 <datestamp>2011-03-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.3471</id><created>2009-01-22</created><updated>2011-03-09</updated><authors><author><keyname>Anevski</keyname><forenames>Dragi</forenames></author><author><keyname>Soulier</keyname><forenames>Philippe</forenames></author></authors><title>Monotone spectral density estimation</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS804 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS804</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 1, 418-438</journal-ref><doi>10.1214/10-AOS804</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two estimators of a monotone spectral density, that are based on
the periodogram. These are the isotonic regression of the periodogram and the
isotonic regression of the log-periodogram. We derive pointwise limit
distribution results for the proposed estimators for short memory linear
processes and long memory Gaussian processes and also that the estimators are
rate optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.3531</identifier>
 <datestamp>2010-08-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.3531</id><created>2009-01-22</created><authors><author><keyname>Kohl</keyname><forenames>Matthias</forenames></author><author><keyname>Ruckdeschel</keyname><forenames>Peter</forenames></author><author><keyname>Rieder</keyname><forenames>Helmut</forenames></author></authors><title>Infinitesimally Robust Estimation in General Smoothly Parametrized
  Models</title><categories>stat.ME stat.AP</categories><journal-ref>Statistical Methods and Application 2010</journal-ref><doi>10.1007/s10260-010-0133-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the shrinking neighborhood approach of Robust Statistics, which
applies to general smoothly parametrized models, especially, exponential
families. Equal generality is achieved by object oriented implementation of the
optimally robust estimators. We evaluate the estimates on real datasets from
literature by means of our R packages ROptEst and RobLox.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.3795</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.3795</id><created>2009-01-23</created><updated>2010-01-26</updated><authors><author><keyname>Szajowski</keyname><forenames>Krzysztof</forenames></author></authors><title>On a random number of disorders</title><categories>math.PR cs.IT math.IT math.ST stat.TH</categories><comments>in Institute of Mathematics, Polish Academy of Science, Preprint no.
  702, 25 references, 34 pages</comments><msc-class>60G40, 60K99, 90D60</msc-class><journal-ref>Probability and Mathematical Statistics, vol. 31, Fasc. 1 (2011),
  pp. 17-45</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We register a random sequence which has the following properties: it has
three segments being the homogeneous Markov processes. Each segment has his own
one step transition probability law and the length of the segment is unknown
and random. It means that at two random successive moments (they can be equal
also and equal zero too) the source of observations is changed and the first
observation in new segment is chosen according to new transition probability
starting from the last state of the previous segment. In effect the number of
homogeneous segments is random. The transition probabilities of each process
are known and a priori distribution of the disorder moments is given. The
former research on such problem has been devoted to various questions
concerning the distribution changes. The random number of distributional
segments creates new problems in solutions with relation to analysis of the
model with deterministic number of segments. Two cases are presented in
details. In the first one the objectives is to stop on or between the disorder
moments while in the second one our objective is to find the strategy which
immediately detects the distribution changes. Both problems are reformulated to
optimal stopping of the observed sequences. The detailed analysis of the
problem is presented to show the form of optimal decision function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.4025</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.4025</id><created>2009-01-26</created><authors><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>Gilbert</keyname><forenames>Peter</forenames></author><author><keyname>Longini,</keyname><forenames>Ira M.</forenames><suffix>Jr.</suffix></author><author><keyname>Halloran</keyname><forenames>M. Elizabeth</forenames></author></authors><title>A Bayesian framework for estimating vaccine efficacy per infectious
  contact</title><categories>stat.AP</categories><comments>Published in at http://dx.doi.org/10.1214/08-AOAS193 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS193</report-no><journal-ref>Annals of Applied Statistics 2008, Vol. 2, No. 4, 1409-1431</journal-ref><doi>10.1214/08-AOAS193</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In vaccine studies for infectious diseases such as human immunodeficiency
virus (HIV), the frequency and type of contacts between study participants and
infectious sources are among the most informative risk factors, but are often
not adequately adjusted for in standard analyses. Such adjustment can improve
the assessment of vaccine efficacy as well as the assessment of risk factors.
It can be attained by modeling transmission per contact with infectious
sources. However, information about contacts that rely on self-reporting by
study participants are subject to nontrivial measurement error in many studies.
We develop a Bayesian hierarchical model fitted using Markov chain Monte Carlo
(MCMC) sampling to estimate the vaccine efficacy controlled for exposure to
infection, while adjusting for measurement error in contact-related factors.
Our method is used to re-analyze two recent HIV vaccine studies, and the
results are compared with the published primary analyses that used standard
methods. The proposed method could also be used for other vaccines where
contact information is collected, such as human papilloma virus vaccines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.4192</identifier>
 <datestamp>2010-03-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.4192</id><created>2009-01-27</created><updated>2009-07-03</updated><authors><author><keyname>Johnson</keyname><forenames>Jason K.</forenames></author><author><keyname>Bickson</keyname><forenames>Danny</forenames></author><author><keyname>Dolev</keyname><forenames>Danny</forenames></author></authors><title>Fixing Convergence of Gaussian Belief Propagation</title><categories>cs.IT cs.LG math.IT stat.CO</categories><comments>In the IEEE International Symposium on Information Theory (ISIT)
  2009, Seoul, South Korea, July 2009</comments><doi>10.1109/ISIT.2009.5205777</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian belief propagation (GaBP) is an iterative message-passing algorithm
for inference in Gaussian graphical models. It is known that when GaBP
converges it converges to the correct MAP estimate of the Gaussian random
vector and simple sufficient conditions for its convergence have been
established. In this paper we develop a double-loop algorithm for forcing
convergence of GaBP. Our method computes the correct MAP estimate even in cases
where standard GaBP would not have converged. We further extend this
construction to compute least-squares solutions of over-constrained linear
systems. We believe that our construction has numerous applications, since the
GaBP algorithm is linked to solution of linear systems of equations, which is a
fundamental problem in computer science and engineering. As a case study, we
discuss the linear detection problem. We show that using our new construction,
we are able to force convergence of Montanari's linear detection algorithm, in
cases where it would originally fail. As a consequence, we are able to increase
significantly the number of users that can transmit concurrently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.4203</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.4203</id><created>2009-01-27</created><authors><author><keyname>Gormley</keyname><forenames>Isobel Claire</forenames></author><author><keyname>Murphy</keyname><forenames>Thomas Brendan</forenames></author></authors><title>A mixture of experts model for rank data with applications in election
  studies</title><categories>stat.AP stat.CO</categories><comments>Published in at http://dx.doi.org/10.1214/08-AOAS178 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS178</report-no><journal-ref>Annals of Applied Statistics 2008, Vol. 2, No. 4, 1452-1477</journal-ref><doi>10.1214/08-AOAS178</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A voting bloc is defined to be a group of voters who have similar voting
preferences. The cleavage of the Irish electorate into voting blocs is of
interest. Irish elections employ a ``single transferable vote'' electoral
system; under this system voters rank some or all of the electoral candidates
in order of preference. These rank votes provide a rich source of preference
information from which inferences about the composition of the electorate may
be drawn. Additionally, the influence of social factors or covariates on the
electorate composition is of interest. A mixture of experts model is a mixture
model in which the model parameters are functions of covariates. A mixture of
experts model for rank data is developed to provide a model-based method to
cluster Irish voters into voting blocs, to examine the influence of social
factors on this clustering and to examine the characteristic preferences of the
voting blocs. The Benter model for rank data is employed as the family of
component densities within the mixture of experts model; generalized linear
model theory is employed to model the influence of covariates on the mixing
proportions. Model fitting is achieved via a hybrid of the EM and MM
algorithms. An example of the methodology is illustrated by examining an Irish
presidential election. The existence of voting blocs in the electorate is
established and it is determined that age and government satisfaction levels
are important factors in influencing voting in this election.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0901.4752</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0901.4752</id><created>2009-01-29</created><updated>2014-10-08</updated><authors><author><keyname>Chretien</keyname><forenames>Stephane</forenames></author></authors><title>Estimation of Gaussian mixtures in small sample studies using $l_1$
  penalization</title><categories>stat.CO stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many experiments in medicine and ecology can be conveniently modeled by
finite Gaussian mixtures but face the problem of dealing with small data sets.
We propose a robust version of the estimator based on self-regression and
sparsity promoting penalization in order to estimate the components of Gaussian
mixtures in such contexts. A space alternating version of the penalized EM
algorithm is obtained and we prove that its cluster points satisfy the
Karush-Kuhn-Tucker conditions. Monte Carlo experiments are presented in order
to compare the results obtained by our method and by standard maximum
likelihood estimation. In particular, our estimator is seen to perform better
than the maximum likelihood estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.0240</identifier>
 <datestamp>2011-12-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.0240</id><created>2009-02-02</created><updated>2010-01-18</updated><authors><author><keyname>Rufibach</keyname><forenames>Kaspar</forenames></author></authors><title>An Active Set Algorithm to Estimate Parameters in Generalized Linear
  Models with Ordered Predictors</title><categories>stat.ME stat.CO</categories><comments>24 pages, 1 Figure, 3 Tables</comments><journal-ref>Comput. Statist. Data Anal. (2010), 54, 1442-1456</journal-ref><doi>10.1016/j.csda.2010.01.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In biomedical studies, researchers are often interested in assessing the
association between one or more ordinal explanatory variables and an outcome
variable, at the same time adjusting for covariates of any type. The outcome
variable may be continuous, binary, or represent censored survival times. In
the absence of precise knowledge of the response function, using monotonicity
constraints on the ordinal variables improves efficiency in estimating
parameters, especially when sample sizes are small. An active set algorithm
that can efficiently compute such estimators is proposed, and a
characterization of the solution is provided. Having an efficient algorithm at
hand is especially relevant when applying likelihood ratio tests in restricted
generalized linear models, where one needs the value of the likelihood at the
restricted maximizer. The algorithm is illustrated on a real life data set from
oncology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.0347</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.0347</id><created>2009-02-02</created><updated>2012-11-22</updated><authors><author><keyname>Ionides</keyname><forenames>Edward L.</forenames></author><author><keyname>Bhadra</keyname><forenames>Anindya</forenames></author><author><keyname>Atchad</keyname><forenames>Yves</forenames></author><author><keyname>King</keyname><forenames>Aaron</forenames></author></authors><title>Iterated filtering</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/11-AOS886 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS886</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 3, 1776-1802</journal-ref><doi>10.1214/11-AOS886</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference for partially observed Markov process models has been a
longstanding methodological challenge with many scientific and engineering
applications. Iterated filtering algorithms maximize the likelihood function
for partially observed Markov process models by solving a recursive sequence of
filtering problems. We present new theoretical results pertaining to the
convergence of iterated filtering algorithms implemented via sequential Monte
Carlo filters. This theory complements the growing body of empirical evidence
that iterated filtering algorithms provide an effective inference strategy for
scientific models of nonlinear dynamic systems. The first step in our theory
involves studying a new recursive approach for maximizing the likelihood
function of a latent variable model, when this likelihood is evaluated via
importance sampling. This leads to the consideration of an iterated importance
sampling algorithm which serves as a simple special case of iterated filtering,
and may have applicability in its own right.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.0392</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.0392</id><created>2009-02-02</created><updated>2011-09-21</updated><authors><author><keyname>Dimitrakakis</keyname><forenames>Christos</forenames></author></authors><title>Tree Exploration for Bayesian RL Exploration</title><categories>stat.ML cs.LG</categories><comments>13 pages, 1 figure. Slightly extended and corrected version (notation
  errors and lower bound calculation) of homonymous paper presented at the
  conference of Computational Intelligence for Modelling, Control and
  Automation 2008 (CIMCA'08)</comments><report-no>IAS-08-04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research in reinforcement learning has produced algorithms for optimal
decision making under uncertainty that fall within two main types. The first
employs a Bayesian framework, where optimality improves with increased
computational time. This is because the resulting planning task takes the form
of a dynamic programming problem on a belief tree with an infinite number of
states. The second type employs relatively simple algorithm which are shown to
suffer small regret within a distribution-free framework. This paper presents a
lower bound and a high probability upper bound on the optimal value function
for the nodes in the Bayesian belief tree, which are analogous to similar
bounds in POMDPs. The bounds are then used to create more efficient strategies
for exploring the tree. The resulting algorithms are compared with the
distribution-free algorithm UCB1, as well as a simpler baseline algorithm on
multi-armed bandit problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.0552</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.0552</id><created>2009-02-03</created><authors><author><keyname>Bai</keyname><forenames>Zhidong</forenames></author><author><keyname>Jiang</keyname><forenames>Dandan</forenames></author><author><keyname>Yao</keyname><forenames>Jian-feng</forenames></author><author><keyname>Zheng</keyname><forenames>Shurong</forenames></author></authors><title>Corrections to LRT on Large Dimensional Covariance Matrix by RMT</title><categories>math.ST stat.TH</categories><comments>25 pages, 2 figures and 3 tables</comments><journal-ref>The Annals of Statistics 2009, Vol. 37, No. 6B, 3822-3840</journal-ref><doi>10.1214/09-AOS694</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we give an explanation to the failure of two likelihood ratio
procedures for testing about covariance matrices from Gaussian populations when
the dimension is large compared to the sample size. Next, using recent central
limit theorems for linear spectral statistics of sample covariance matrices and
of random F-matrices, we propose necessary corrections for these LR tests to
cope with high-dimensional effects. The asymptotic distributions of these
corrected tests under the null are given. Simulations demonstrate that the
corrected LR tests yield a realized size close to nominal level for both
moderate p (around 20) and high dimension, while the traditional LR tests with
chi-square approximation fails. Another contribution from the paper is that for
testing the equality between two covariance matrices, the proposed correction
applies equally for non-Gaussian populations yielding a valid pseudo-likelihood
ratio test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.0600</identifier>
 <datestamp>2011-06-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.0600</id><created>2009-02-03</created><updated>2011-06-06</updated><authors><author><keyname>Brodu</keyname><forenames>Nicolas</forenames></author></authors><title>Reconstruction of Epsilon-Machines in Predictive Frameworks and
  Decisional States</title><categories>stat.ML</categories><comments>Minor revision, final version. Free/libre code at
  http://nicolas.brodu.numerimoire.net</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces both a new algorithm for reconstructing
epsilon-machines from data, as well as the decisional states. These are defined
as the internal states of a system that lead to the same decision, based on a
user-provided utility or pay-off function. The utility function encodes some a
priori knowledge external to the system, it quantifies how bad it is to make
mistakes. The intrinsic underlying structure of the system is modeled by an
epsilon-machine and its causal states. The decisional states form a partition
of the lower-level causal states that is defined according to the higher-level
user's knowledge. In a complex systems perspective, the decisional states are
thus the "emerging" patterns corresponding to the utility function. The
transitions between these decisional states correspond to events that lead to a
change of decision. The new REMAPF algorithm estimates both the epsilon-machine
and the decisional states from data. Application examples are given for hidden
model reconstruction, cellular automata filtering, and edge detection in
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.0645</identifier>
 <datestamp>2011-12-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.0645</id><created>2009-02-03</created><updated>2011-12-16</updated><authors><author><keyname>Johannes</keyname><forenames>J.</forenames></author><author><keyname>Schenk</keyname><forenames>R.</forenames></author></authors><title>On rate optimal local estimation in functional linear model</title><categories>math.ST stat.TH</categories><comments>Major revision</comments><msc-class>62J05 (Primary) 62G20, 62G08 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the estimation of the value of a linear functional of the slope
parameter in functional linear regression, where scalar responses are modeled
in dependence of random functions. The theory in this paper covers in
particular point-wise estimation as well as the estimation of weighted averages
of the slope parameter. We propose a plug-in estimator which is based on a
dimension reduction technique and additional thresholding. It is shown that
this estimator is consistent under mild assumptions. We derive a lower bound
for the maximal mean squared error of any estimator over a certain ellipsoid of
slope parameters and a certain class of covariance operators associated with
the regressor. It is shown that the proposed estimator attains this lower bound
up to a constant and hence it is minimax optimal. Our results are appropriate
to discuss a wide range of possible regressors, slope parameters and
functionals. They are illustrated by considering the point-wise estimation of
the slope parameter or its derivatives and its average value over a given
interval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.1733</identifier>
 <datestamp>2010-07-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.1733</id><created>2009-02-10</created><updated>2010-07-04</updated><authors><author><keyname>Audibert</keyname><forenames>Jean-Yves</forenames><affiliation>Imagine, INRIA Rocquencourt</affiliation></author><author><keyname>Catoni</keyname><forenames>Olivier</forenames><affiliation>DMA</affiliation></author></authors><title>Risk bounds in linear regression through PAC-Bayesian truncation</title><categories>stat.ML math.ST stat.TH</categories><comments>78 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of predicting as well as the best linear combination
of d given functions in least squares regression, and variants of this problem
including constraints on the parameters of the linear combination. When the
input distribution is known, there already exists an algorithm having an
expected excess risk of order d/n, where n is the size of the training data.
Without this strong assumption, standard results often contain a multiplicative
log n factor, and require some additional assumptions like uniform boundedness
of the d-dimensional input representation and exponential moments of the
output. This work provides new risk bounds for the ridge estimator and the
ordinary least squares estimator, and their variants. It also provides
shrinkage procedures with convergence rate d/n (i.e., without the logarithmic
factor) in expectation and in deviations, under various assumptions. The key
common surprising factor of these results is the absence of exponential moment
condition on the output distribution while achieving exponential deviations.
All risk bounds are obtained through a PAC-Bayesian analysis on truncated
differences of losses. Finally, we show that some of these results are not
particular to the least squares loss, but can be generalized to similar
strongly convex loss functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.2544</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.2544</id><created>2009-02-16</created><updated>2010-06-07</updated><authors><author><keyname>Bissiri</keyname><forenames>Pier Giovanni</forenames></author><author><keyname>Walker</keyname><forenames>Stephen G.</forenames></author></authors><title>On Bayesian learning from Bernoulli observations</title><categories>math.ST stat.TH</categories><comments>This is a personal 19-pages manuscript version of the article that
  has been accepted for publication by Journal of Statistical Planning and
  Inference</comments><doi>10.1016/j.jspi.2010.05.023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a reason for Bayesian updating, in the Bernoulli case, even when
it is assumed that observations are independent and identically distributed
with a fixed but unknown parameter $\theta_0$. The motivation relies on the use
of loss functions and asymptotics. Such a justification is important due to the
recent interest and focus on Bayesian consistency which indeed assumes that the
observations are independent and identically distributed rather than being
conditionally independent with joint distribution depending on the choice of
prior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.2808</identifier>
 <datestamp>2011-01-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.2808</id><created>2009-02-16</created><authors><author><keyname>Murtagh</keyname><forenames>Fionn</forenames></author><author><keyname>Spagat</keyname><forenames>Michael</forenames></author><author><keyname>Restrepo</keyname><forenames>Jorge A.</forenames></author></authors><title>Ultrametric Wavelet Regression of Multivariate Time Series: Application
  to Colombian Conflict Analysis</title><categories>stat.ML stat.AP</categories><comments>36 pages, 13 figures</comments><journal-ref>IEEE Transactions on Systems, Man, and Cybernetics - Part A,
  Systems and Humans, 2011</journal-ref><doi>10.1109/TSMCA.2010.2064301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first pursue the study of how hierarchy provides a well-adapted tool for
the analysis of change. Then, using a time sequence-constrained hierarchical
clustering, we develop the practical aspects of a new approach to wavelet
regression. This provides a new way to link hierarchical relationships in a
multivariate time series data set with external signals. Violence data from the
Colombian conflict in the years 1990 to 2004 is used throughout. We conclude
with some proposals for further study on the relationship between social
violence and market forces, viz. between the Colombian conflict and the US
narcotics market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.2924</identifier>
 <datestamp>2012-07-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.2924</id><created>2009-02-17</created><updated>2012-07-03</updated><authors><author><keyname>Alquier</keyname><forenames>Pierre</forenames><affiliation>LPMA, CREST</affiliation></author><author><keyname>Wintenberger</keyname><forenames>Olivier</forenames><affiliation>CEREMADE</affiliation></author></authors><title>Model selection for weakly dependent time series forecasting</title><categories>stat.ME math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Observing a stationary time series, we propose a two-step procedure for the
prediction of the next value of the time series. The first step follows machine
learning theory paradigm and consists in determining a set of possible
predictors as randomized estimators in (possibly numerous) different predictive
models. The second step follows the model selection paradigm and consists in
choosing one predictor with good properties among all the predictors of the
first steps. We study our procedure for two different types of bservations:
causal Bernoulli shifts and bounded weakly dependent processes. In both cases,
we give oracle inequalities: the risk of the chosen predictor is close to the
best prediction risk in all predictive models that we consider. We apply our
procedure for predictive models such as linear predictors, neural networks
predictors and non-parametric autoregressive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.2996</identifier>
 <datestamp>2011-08-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.2996</id><created>2009-02-17</created><authors><author><keyname>Das</keyname><forenames>Bikramjit</forenames></author><author><keyname>Resnick</keyname><forenames>Sidney I.</forenames></author></authors><title>Detecting a conditional extrme value model</title><categories>math.ST stat.TH</categories><comments>21 pages, 4 figures</comments><msc-class>60G70, 62G32</msc-class><doi>10.1007/s10687-009-0097-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In classical extreme value theory probabilities of extreme events are
estimated assuming all the components of a random vector to be in a domain of
attraction of an extreme value distribution. In contrast, the conditional
extreme value model assumes a domain of attraction condition on a
sub-collection of the components of a multivariate random vector. This model
has been studied in
\cite{heffernan:tawn:2004,heffernan:resnick:2007,das:resnick:2008a}.
  In this paper we propose three statistics which act as tools to detect this
model in a bivariate set-up. In addition, the proposed statistics also help to
distinguish between two forms of the limit measure that is obtained in the
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.3130</identifier>
 <datestamp>2012-06-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.3130</id><created>2009-02-18</created><updated>2012-03-01</updated><authors><author><keyname>Gey</keyname><forenames>Servane</forenames><affiliation>MAP5</affiliation></author></authors><title>Risk Bounds for CART Classifiers under a Margin Condition</title><categories>stat.ML math.ST stat.TH</categories><proxy>ccsd</proxy><report-no>MAP5 2009-04</report-no><journal-ref>Pattern Recognition 45 (2012) 3523-3534</journal-ref><doi>10.1016/j.patcog.2012.02.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Risk bounds for Classification and Regression Trees (CART, Breiman et. al.
1984) classifiers are obtained under a margin condition in the binary
supervised classification framework. These risk bounds are obtained
conditionally on the construction of the maximal deep binary tree and permit to
prove that the linear penalty used in the CART pruning algorithm is valid under
a margin condition. It is also shown that, conditionally on the construction of
the maximal tree, the final selection by test sample does not alter
dramatically the estimation accuracy of the Bayes classifier. In the two-class
classification framework, the risk bounds that are proved, obtained by using
penalized model selection, validate the CART algorithm which is used in many
data mining applications such as Biology, Medicine or Image Coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.3137</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.3137</id><created>2009-02-18</created><updated>2011-06-10</updated><authors><author><keyname>Cherfi</keyname><forenames>Mohamed</forenames><affiliation>LSTA</affiliation></author></authors><title>Large Deviations Theorems in Nonparametric Regression on Functional Data</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove large deviations principles for the Nadaraya-Watson
estimator of the regression of a real-valued variable with a functional
covariate. Under suitable conditions, we show pointwise and uniform large
deviations theorems with good rate functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.3347</identifier>
 <datestamp>2010-08-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.3347</id><created>2009-02-19</created><authors><author><keyname>Kraemer</keyname><forenames>Nicole</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author><author><keyname>Braun</keyname><forenames>Mikio</forenames></author></authors><title>Lanczos Approximations for the Speedup of Kernel Partial Least Squares
  Regression</title><categories>stat.ML</categories><comments>to appear in Proceedings of the 12th International Conference on
  Artificial Intelligence and Statistics (AISTATS 09)</comments><journal-ref>JMLR Workshop and Conference Proceedings 5 (AISTATS 2009), p
  288-295, 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The runtime for Kernel Partial Least Squares (KPLS) to compute the fit is
quadratic in the number of examples. However, the necessity of obtaining
sensitivity measures as degrees of freedom for model selection or confidence
intervals for more detailed analysis requires cubic runtime, and thus
constitutes a computational bottleneck in real-world data analysis. We propose
a novel algorithm for KPLS which not only computes (a) the fit, but also (b)
its approximate degrees of freedom and (c) error bars in quadratic runtime. The
algorithm exploits a close connection between Kernel PLS and the Lanczos
algorithm for approximating the eigenvalues of symmetric matrices, and uses
this approximation to compute the trace of powers of the kernel matrix in
quadratic runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.3619</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.3619</id><created>2009-02-20</created><updated>2012-03-19</updated><authors><author><keyname>Galves</keyname><forenames>Antonio</forenames></author><author><keyname>Galves</keyname><forenames>Charlotte</forenames></author><author><keyname>Garca</keyname><forenames>Jess E.</forenames></author><author><keyname>Garcia</keyname><forenames>Nancy L.</forenames></author><author><keyname>Leonardi</keyname><forenames>Florencia</forenames></author></authors><title>Context tree selection and linguistic rhythm retrieval from written
  texts</title><categories>stat.ML stat.AP</categories><comments>Published in at http://dx.doi.org/10.1214/11-AOAS511 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS511</report-no><journal-ref>Annals of Applied Statistics 2012, Vol. 6, No. 1, 186-209</journal-ref><doi>10.1214/11-AOAS511</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The starting point of this article is the question "How to retrieve
fingerprints of rhythm in written texts?" We address this problem in the case
of Brazilian and European Portuguese. These two dialects of Modern Portuguese
share the same lexicon and most of the sentences they produce are superficially
identical. Yet they are conjectured, on linguistic grounds, to implement
different rhythms. We show that this linguistic question can be formulated as a
problem of model selection in the class of variable length Markov chains. To
carry on this approach, we compare texts from European and Brazilian
Portuguese. These texts are previously encoded according to some basic rhythmic
features of the sentences which can be automatically retrieved. This is an
entirely new approach from the linguistic point of view. Our statistical
contribution is the introduction of the smallest maximizer criterion which is a
constant free procedure for model selection. As a by-product, this provides a
solution for the problem of optimal choice of the penalty constant when using
the BIC to select a variable length Markov chain. Besides proving the
consistency of the smallest maximizer criterion when the sample size diverges,
we also make a simulation study comparing our approach with both the standard
BIC selection and the Peres-Shields order estimation. Applied to the linguistic
sample constituted for our case study, the smallest maximizer criterion assigns
different context-tree models to the two dialects of Portuguese. The features
of the selected models are compatible with current conjectures discussed in the
linguistic literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.3714</identifier>
 <datestamp>2010-01-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.3714</id><created>2009-02-20</created><updated>2010-01-27</updated><authors><author><keyname>Yang</keyname><forenames>Shu</forenames></author><author><keyname>Kolaczyk</keyname><forenames>Eric D.</forenames></author></authors><title>Target Detection via Network Filtering</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A method of `network filtering' has been proposed recently to detect the
effects of certain external perturbations on the interacting members in a
network. However, with large networks, the goal of detection seems a priori
difficult to achieve, especially since the number of observations available
often is much smaller than the number of variables describing the effects of
the underlying network. Under the assumption that the network possesses a
certain sparsity property, we provide a formal characterization of the accuracy
with which the external effects can be detected, using a network filtering
system that combines Lasso regression in a sparse simultaneous equation model
with simple residual analysis. We explore the implications of the technical
conditions underlying our characterization, in the context of various network
topologies, and we illustrate our method using simulated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.3837</identifier>
 <datestamp>2010-10-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.3837</id><created>2009-02-22</created><updated>2010-10-04</updated><authors><author><keyname>Hall</keyname><forenames>Peter</forenames></author><author><keyname>Jin</keyname><forenames>Jiashun</forenames></author></authors><title>Innovated higher criticism for detecting sparse signals in correlated
  noise</title><categories>math.ST math.SP stat.ME stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS764 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS764</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 3, 1686-1732</journal-ref><doi>10.1214/09-AOS764</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher criticism is a method for detecting signals that are both sparse and
weak. Although first proposed in cases where the noise variables are
independent, higher criticism also has reasonable performance in settings where
those variables are correlated. In this paper we show that, by exploiting the
nature of the correlation, performance can be improved by using a modified
approach which exploits the potential advantages that correlation has to offer.
Indeed, it turns out that the case of independent noise is the most difficult
of all, from a statistical viewpoint, and that more accurate signal detection
(for a given level of signal sparsity and strength) can be obtained when
correlation is present. We characterize the advantages of correlation by
showing how to incorporate them into the definition of an optimal detection
boundary. The boundary has particularly attractive properties when correlation
decays at a polynomial rate or the correlation matrix is Toeplitz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.3977</identifier>
 <datestamp>2011-02-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.3977</id><created>2009-02-23</created><updated>2009-04-08</updated><authors><author><keyname>Arlot</keyname><forenames>Sylvain</forenames><affiliation>LIENS</affiliation></author><author><keyname>Celisse</keyname><forenames>Alain</forenames></author></authors><title>Segmentation of the mean of heteroscedastic data via cross-validation</title><categories>stat.ME math.ST stat.TH</categories><proxy>ccsd hal-00363627</proxy><journal-ref>Statistics and Computing (2009) electronic</journal-ref><doi>10.1007/s11222-010-9196-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper tackles the problem of detecting abrupt changes in the mean of a
heteroscedastic signal by model selection, without knowledge on the variations
of the noise. A new family of change-point detection procedures is proposed,
showing that cross-validation methods can be successful in the heteroscedastic
framework, whereas most existing procedures are not robust to
heteroscedasticity. The robustness to heteroscedasticity of the proposed
procedures is supported by an extensive simulation study, together with recent
theoretical results. An application to Comparative Genomic Hybridization (CGH)
data is provided, showing that robustness to heteroscedasticity can indeed be
required for their analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.4111</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.4111</id><created>2009-02-24</created><updated>2011-10-15</updated><authors><author><keyname>Lilly</keyname><forenames>Jonathan M.</forenames></author><author><keyname>Olhede</keyname><forenames>Sofia C.</forenames></author></authors><title>Bivariate Instantaneous Frequency and Bandwidth</title><categories>stat.ME physics.ao-ph stat.AP</categories><journal-ref>Lilly, J. M., and S. C. Olhede (2010). Bivariate instantaneous
  frequency and bandwidth. IEEE Transactions on Signal Processing, 58 (2),
  591--603</journal-ref><doi>10.1109/TSP.2009.2031729</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalizations of instantaneous frequency and instantaneous bandwidth to
a bivariate signal are derived. These are uniquely defined whether the signal
is represented as a pair of real-valued signals, or as one analytic and one
anti-analytic signal. A nonstationary but oscillatory bivariate signal has a
natural representation as an ellipse whose properties evolve in time, and this
representation provides a simple geometric interpretation for the bivariate
instantaneous moments. The bivariate bandwidth is shown to consist of three
terms measuring the degree of instability of the time-varying ellipse:
amplitude modulation with fixed eccentricity, eccentricity modulation, and
orientation modulation or precession. An application to the analysis of data
from a free-drifting oceanographic float is presented and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.4214</identifier>
 <datestamp>2010-01-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.4214</id><created>2009-02-24</created><updated>2010-01-04</updated><authors><author><keyname>Pinelis</keyname><forenames>Iosif</forenames></author></authors><title>Positive-part moments via the Fourier-Laplace transform</title><categories>math.PR math.ST stat.TH</categories><comments>Accepted for publication in Journal of Theoretical Probability, with
  Proposition 2.6 downgraded to Example 1. A couple of possible applications
  are added. Other changes are minor</comments><msc-class>60E10, 42A38 (Primary) 60E07, 60E15, 42A55, 42A61 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integral expressions for positive-part moments E X_+^p (p&gt;0) of random
variables X are presented, in terms of the Fourier-Laplace or Fourier
transforms of the distribution of X. A necessary and sufficient condition for
the validity of such an expression is given. This study was motivated by
extremal problems in probability and statistics, where one needs to evaluate
such positive-part moments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.4380</identifier>
 <datestamp>2010-08-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.4380</id><created>2009-02-25</created><updated>2010-01-14</updated><authors><author><keyname>Blanchard</keyname><forenames>Gilles</forenames></author><author><keyname>Kraemer</keyname><forenames>Nicole</forenames></author></authors><title>Kernel Partial Least Squares is Universally Consistent</title><categories>stat.ME math.ST stat.ML stat.TH</categories><comments>18 pages, no figures</comments><journal-ref>JMLR Workshop and Conference Proceedings 9 (AISTATS 2010) 57-64,
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the statistical consistency of kernel Partial Least Squares
Regression applied to a bounded regression learning problem on a reproducing
kernel Hilbert space. Partial Least Squares stands out of well-known classical
approaches as e.g. Ridge Regression or Principal Components Regression, as it
is not defined as the solution of a global cost minimization procedure over a
fixed model nor is it a linear estimator. Instead, approximate solutions are
constructed by projections onto a nested set of data-dependent subspaces. To
prove consistency, we exploit the known fact that Partial Least Squares is
equivalent to the conjugate gradient algorithm in combination with early
stopping. The choice of the stopping rule (number of iterations) is a crucial
point. We study two empirical stopping rules. The first one monitors the
estimation error in each iteration step of Partial Least Squares, and the
second one estimates the empirical complexity in terms of a condition number.
Both stopping rules lead to universally consistent estimators provided the
kernel is universal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0902.4583</identifier>
 <datestamp>2010-11-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0902.4583</id><created>2009-02-26</created><updated>2010-11-29</updated><authors><author><keyname>Dalang</keyname><forenames>Robert C.</forenames></author><author><keyname>Sanz-Sol</keyname><forenames>Marta</forenames></author></authors><title>Criteria for hitting probabilities with applications to systems of
  stochastic wave equations</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ247 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ247</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 4, 1343-1368</journal-ref><doi>10.3150/09-BEJ247</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop several results on hitting probabilities of random fields which
highlight the role of the dimension of the parameter space. This yields upper
and lower bounds in terms of Hausdorff measure and Bessel--Riesz capacity,
respectively. We apply these results to a system of stochastic wave equations
in spatial dimension $k\ge1$ driven by a $d$-dimensional spatially homogeneous
additive Gaussian noise that is white in time and colored in space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.0664</identifier>
 <datestamp>2013-10-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.0664</id><created>2009-03-03</created><updated>2013-10-02</updated><authors><author><keyname>Johnson</keyname><forenames>Alicia A.</forenames></author><author><keyname>Jones</keyname><forenames>Galin L.</forenames></author><author><keyname>Neath</keyname><forenames>Ronald C.</forenames></author></authors><title>Component-Wise Markov Chain Monte Carlo: Uniform and Geometric
  Ergodicity under Mixing and Composition</title><categories>stat.CO stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/13-STS423 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS423</report-no><journal-ref>Statistical Science 2013, Vol. 28, No. 3, 360-375</journal-ref><doi>10.1214/13-STS423</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is common practice in Markov chain Monte Carlo to update the simulation
one variable (or sub-block of variables) at a time, rather than conduct a
single full-dimensional update. When it is possible to draw from each
full-conditional distribution associated with the target this is just a Gibbs
sampler. Often at least one of the Gibbs updates is replaced with a
Metropolis-Hastings step, yielding a Metropolis-Hastings-within-Gibbs
algorithm. Strategies for combining component-wise updates include composition,
random sequence and random scans. While these strategies can ease MCMC
implementation and produce superior empirical performance compared to
full-dimensional updates, the theoretical convergence properties of the
associated Markov chains have received limited attention. We present conditions
under which some component-wise Markov chains converge to the stationary
distribution at a geometric rate. We pay particular attention to the
connections between the convergence rates of the various component-wise
strategies. This is important since it ensures the existence of tools that an
MCMC practitioner can use to be as confident in the simulation results as if
they were based on independent and identically distributed samples. We
illustrate our results in two examples including a hierarchical linear mixed
model and one involving maximum likelihood estimation for mixed models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.1223</identifier>
 <datestamp>2012-06-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.1223</id><created>2009-03-06</created><updated>2010-02-16</updated><authors><author><keyname>Dalalyan</keyname><forenames>Arnak</forenames><affiliation>IGM-LabInfo</affiliation></author><author><keyname>Tsybakov</keyname><forenames>Alexandre B.</forenames><affiliation>PMA</affiliation></author></authors><title>Sparse Regression Learning by Aggregation and Langevin Monte-Carlo</title><categories>stat.AP math.ST stat.TH</categories><comments>Short version published in COLT 2009</comments><proxy>ccsd hal-00362471</proxy><journal-ref>Journal of Computer and System Sciences 78 (2012) 1423-1443</journal-ref><doi>10.1016/j.jcss.2011.12.023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of regression learning for deterministic design and
independent random errors. We start by proving a sharp PAC-Bayesian type bound
for the exponentially weighted aggregate (EWA) under the expected squared
empirical loss. For a broad class of noise distributions the presented bound is
valid whenever the temperature parameter $\beta$ of the EWA is larger than or
equal to $4\sigma^2$, where $\sigma^2$ is the noise variance. A remarkable
feature of this result is that it is valid even for unbounded regression
functions and the choice of the temperature parameter depends exclusively on
the noise level. Next, we apply this general bound to the problem of
aggregating the elements of a finite-dimensional linear space spanned by a
dictionary of functions $\phi_1,...,\phi_M$. We allow $M$ to be much larger
than the sample size $n$ but we assume that the true regression function can be
well approximated by a sparse linear combination of functions $\phi_j$. Under
this sparsity scenario, we propose an EWA with a heavy tailed prior and we show
that it satisfies a sparsity oracle inequality with leading constant one.
Finally, we propose several Langevin Monte-Carlo algorithms to approximately
compute such an EWA when the number $M$ of aggregated functions can be large.
We discuss in some detail the convergence of these algorithms and present
numerical experiments that confirm our theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.1468</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.1468</id><created>2009-03-08</created><authors><author><keyname>Lounici</keyname><forenames>Karim</forenames></author><author><keyname>Pontil</keyname><forenames>Massimiliano</forenames></author><author><keyname>Tsybakov</keyname><forenames>Alexandre B.</forenames></author><author><keyname>van de Geer</keyname><forenames>Sara</forenames></author></authors><title>Taking Advantage of Sparsity in Multi-Task Learning</title><categories>stat.ML math.ST stat.TH</categories><journal-ref>10 pages, 1 figure, Proc. Computational Learning Theory Conference
  (COLT 2009)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of estimating multiple linear regression equations for
the purpose of both prediction and variable selection. Following recent work on
multi-task learning Argyriou et al. [2008], we assume that the regression
vectors share the same sparsity pattern. This means that the set of relevant
predictor variables is the same across the different equations. This assumption
leads us to consider the Group Lasso as a candidate estimation method. We show
that this estimator enjoys nice sparsity oracle inequalities and variable
selection properties. The results hold under a certain restricted eigenvalue
condition and a coherence condition on the design matrix, which naturally
extend recent work in Bickel et al. [2007], Lounici [2008]. In particular, in
the multi-task learning scenario, in which the number of tasks can grow, we are
able to remove completely the effect of the number of predictor variables in
the bounds. Finally, we show how our results can be extended to more general
noise distributions, of which we only require the variance to be finite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.1869</identifier>
 <datestamp>2011-06-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.1869</id><created>2009-03-10</created><updated>2011-06-08</updated><authors><author><keyname>Jankowski</keyname><forenames>Hanna K.</forenames></author><author><keyname>Stanberry</keyname><forenames>Larissa I.</forenames></author></authors><title>Confidence Regions for Means of Random Sets using Oriented Distance
  Functions</title><categories>stat.ME math.ST stat.AP stat.TH</categories><comments>26 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image analysis frequently deals with shape estimation and image
reconstruction. The ob jects of interest in these problems may be thought of as
random sets, and one is interested in finding a representative, or expected,
set. We consider a definition of set expectation using oriented distance
functions and study the properties of the associated empirical set. Conditions
are given such that the empirical average is consistent, and a method to
calculate a confidence region for the expected set is introduced. The proposed
method is applied to both real and simulated data examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.1880</identifier>
 <datestamp>2010-08-17</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.1880</id><created>2009-03-11</created><authors><author><keyname>Pendse</keyname><forenames>Gautam</forenames></author><author><keyname>Schwarz</keyname><forenames>Adam</forenames></author><author><keyname>Baumgartner</keyname><forenames>Richard</forenames></author><author><keyname>Coimbra</keyname><forenames>Alexandre</forenames></author><author><keyname>Borsook</keyname><forenames>David</forenames></author><author><keyname>Becerra</keyname><forenames>Lino</forenames></author></authors><title>SMART: A statistical framework for optimal design matrix generation with
  application to fMRI</title><categories>stat.ME stat.AP stat.CO</categories><comments>68 pages, 34 figures</comments><doi>10.1109/TMI.2010.2044512</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The general linear model (GLM) is a well established tool for analyzing
functional magnetic resonance imaging (fMRI) data. Most fMRI analyses via GLM
proceed in a massively univariate fashion where the same design matrix is used
for analyzing data from each voxel. A major limitation of this approach is the
locally varying nature of signals of interest as well as associated confounds.
This local variability results in a potentially large bias and uncontrolled
increase in variance for the contrast of interest. The main contributions of
this paper are two fold (1) We develop a statistical framework called SMART
that enables estimation of an optimal design matrix while explicitly
controlling the bias variance decomposition over a set of potential design
matrices and (2) We develop and validate a numerical algorithm for computing
optimal design matrices for general fMRI data sets. The implications of this
framework include the ability to match optimally the magnitude of underlying
signals to their true magnitudes while also matching the "null" signals to zero
size thereby optimizing both the sensitivity and specificity of signal
detection. By enabling the capture of multiple profiles of interest using a
single contrast (as opposed to an F-test) in a way that optimizes for both bias
and variance enables the passing of first level parameter estimates and their
variances to the higher level for group analysis which is not possible using
F-tests. We demonstrate the application of this approach to in vivo
pharmacological fMRI data capturing the acute response to a drug infusion, to
task-evoked, block design fMRI and to the estimation of a haemodynamic response
function (HRF) response in event-related fMRI. Our framework is quite general
and has potentially wide applicability to a variety of disciplines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.1951</identifier>
 <datestamp>2011-03-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.1951</id><created>2009-03-11</created><updated>2011-03-18</updated><authors><author><keyname>Dedecker</keyname><forenames>Jrme</forenames></author><author><keyname>Merlevde</keyname><forenames>Florence</forenames></author><author><keyname>Peligrad</keyname><forenames>Magda</forenames></author></authors><title>Invariance principles for linear processes with application to isotonic
  regression</title><categories>math.ST math.PR stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ273 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ273</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 1, 88-113</journal-ref><doi>10.3150/10-BEJ273</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove maximal inequalities and study the functional central
limit theorem for the partial sums of linear processes generated by dependent
innovations. Due to the general weights, these processes can exhibit long-range
dependence and the limiting distribution is a fractional Brownian motion. The
proofs are based on new approximations by a linear process with martingale
difference innovations. The results are then applied to study an estimator of
the isotonic regression when the error process is a (possibly long-range
dependent) time series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.2003</identifier>
 <datestamp>2010-10-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.2003</id><created>2009-03-11</created><updated>2010-10-08</updated><authors><author><keyname>Ahdesmki</keyname><forenames>Miika</forenames></author><author><keyname>Strimmer</keyname><forenames>Korbinian</forenames></author></authors><title>Feature selection in omics prediction problems using cat scores and
  false nondiscovery rate control</title><categories>stat.AP stat.ML</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOAS277 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS277</report-no><journal-ref>Annals of Applied Statistics 2010, Vol. 4, No. 1, 503-519</journal-ref><doi>10.1214/09-AOAS277</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the problem of feature selection in linear discriminant analysis
(LDA), that is, when features are correlated. First, we introduce a pooled
centroids formulation of the multiclass LDA predictor function, in which the
relative weights of Mahalanobis-transformed predictors are given by
correlation-adjusted $t$-scores (cat scores). Second, for feature selection we
propose thresholding cat scores by controlling false nondiscovery rates (FNDR).
Third, training of the classifier is based on James--Stein shrinkage estimates
of correlations and variances, where regularization parameters are chosen
analytically without resampling. Overall, this results in an effective and
computationally inexpensive framework for high-dimensional prediction with
natural feature selection. The proposed shrinkage discriminant procedures are
implemented in the R package ``sda'' available from the R repository CRAN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.2421</identifier>
 <datestamp>2010-02-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.2421</id><created>2009-03-13</created><updated>2010-02-28</updated><authors><author><keyname>Barczy</keyname><forenames>Matyas</forenames></author><author><keyname>Ispany</keyname><forenames>Marton</forenames></author><author><keyname>Pap</keyname><forenames>Gyula</forenames></author><author><keyname>Scotto</keyname><forenames>Manuel</forenames></author><author><keyname>Silva</keyname><forenames>Maria Eduarda</forenames></author></authors><title>Outliers in INAR(1) models</title><categories>math.PR math.ST stat.TH</categories><comments>106 pages; the proofs of the existence of CLS estimators are
  corrected</comments><msc-class>60J80; 62F12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the integer-valued autoregressive model of order one,
contaminated with additive or innovational outliers is studied in some detail.
Moreover, parameter estimation is also addressed. Supposing that the time
points of the outliers are known but their sizes are unknown, we prove that the
Conditional Least Squares (CLS) estimators of the offspring and innovation
means are strongly consistent. In contrast, however, the CLS estimators of the
outliers' sizes are not strongly consistent, although they converge to a random
limit with probability 1. This random limit depends on the values of the
process at the outliers' time points and on the values at the preceding time
points and in case of additive outliers also on the values at the following
time points. We also prove that the joint CLS estimator of the offspring and
innovation means is asymptotically normal. Conditionally on the above described
values of the process, the joint CLS estimator of the sizes of the outliers is
also asymptotically normal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.2890</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.2890</id><created>2009-03-16</created><updated>2010-05-28</updated><authors><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Sinopoli</keyname><forenames>Bruno</forenames></author><author><keyname>Moura</keyname><forenames>Jose M. F.</forenames></author></authors><title>Kalman Filtering with Intermittent Observations: Weak Convergence to a
  Stationary Distribution</title><categories>cs.IT cs.LG math.IT math.ST stat.TH</categories><comments>Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies the asymptotic behavior of Random Algebraic Riccati
Equations (RARE) arising in Kalman filtering when the arrival of the
observations is described by a Bernoulli i.i.d. process. We model the RARE as
an order-preserving, strongly sublinear random dynamical system (RDS). Under a
sufficient condition, stochastic boundedness, and using a limit-set dichotomy
result for order-preserving, strongly sublinear RDS, we establish the
asymptotic properties of the RARE: the sequence of random prediction error
covariance matrices converges weakly to a unique invariant distribution, whose
support exhibits fractal behavior. In particular, this weak convergence holds
under broad conditions and even when the observations arrival rate is below the
critical probability for mean stability. We apply the weak-Feller property of
the Markov process governing the RARE to characterize the support of the
limiting invariant distribution as the topological closure of a countable set
of points, which, in general, is not dense in the set of positive semi-definite
matrices. We use the explicit characterization of the support of the invariant
distribution and the almost sure ergodicity of the sample paths to easily
compute the moments of the invariant distribution. A one dimensional example
illustrates that the support is a fractured subset of the non-negative reals
with self-similarity properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.2919</identifier>
 <datestamp>2010-11-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.2919</id><created>2009-03-17</created><updated>2010-11-10</updated><authors><author><keyname>Reynaud-Bouret</keyname><forenames>Patricia</forenames></author><author><keyname>Schbath</keyname><forenames>Sophie</forenames></author></authors><title>Adaptive estimation for Hawkes processes; application to genome analysis</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS806 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS806</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 5, 2781-2822</journal-ref><doi>10.1214/10-AOS806</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to provide a new method for the detection of either
favored or avoided distances between genomic events along DNA sequences. These
events are modeled by a Hawkes process. The biological problem is actually
complex enough to need a nonasymptotic penalized model selection approach. We
provide a theoretical penalty that satisfies an oracle inequality even for
quite complex families of models. The consecutive theoretical estimator is
shown to be adaptive minimax for H\"{o}lderian functions with regularity in
$(1/2,1]$: those aspects have not yet been studied for the Hawkes' process.
Moreover, we introduce an efficient strategy, named Islands, which is not
classically used in model selection, but that happens to be particularly
relevant to the biological question we want to answer. Since a multiplicative
constant in the theoretical penalty is not computable in practice, we provide
extensive simulations to find a data-driven calibration of this constant. The
results obtained on real genomic data are coherent with biological knowledge
and eventually refine them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.3330</identifier>
 <datestamp>2010-03-17</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.3330</id><created>2009-03-19</created><updated>2010-03-16</updated><authors><author><keyname>Genest</keyname><forenames>Christian</forenames></author><author><keyname>Segers</keyname><forenames>Johan</forenames></author></authors><title>On the covariance of the asymptotic empirical copula process</title><categories>math.ST stat.TH</categories><comments>14 pages, 2 figures</comments><report-no>Institut de statistique, biostatistique et sciences actuarielles
  (ISBA), Universit\'e catholique de Louvain, DP0906</report-no><msc-class>62G30 (Primary); 60G15 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditions are given under which the empirical copula process associated with
a random sample from a bivariate continuous distribution has a smaller
asymptotic covariance function than the standard empirical process based on
observations from the copula. Illustrations are provided and consequences for
inference are outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.3400</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.3400</id><created>2009-03-19</created><updated>2010-01-13</updated><authors><author><keyname>Qi</keyname><forenames>Xin</forenames></author><author><keyname>Zhao</keyname><forenames>Hongyu</forenames></author></authors><title>Asymptotic efficiency and finite-sample properties of the generalized
  profiling estimation of parameters in ordinary differential equations</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS724 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><report-no>IMS-AOS-AOS724</report-no><msc-class>62F12 (Primary) 65L05, 65D07 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 1, 435-481</journal-ref><doi>10.1214/09-AOS724</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ordinary differential equations (ODEs) are commonly used to model dynamic
behavior of a system. Because many parameters are unknown and have to be
estimated from the observed data, there is growing interest in statistics to
develop efficient estimation procedures for these parameters. Among the
proposed methods in the literature, the generalized profiling estimation method
developed by Ramsay and colleagues is particularly promising for its
computational efficiency and good performance. In this approach, the ODE
solution is approximated with a linear combination of basis functions. The
coefficients of the basis functions are estimated by a penalized smoothing
procedure with an ODE-defined penalty. However, the statistical properties of
this procedure are not known. In this paper, we first give an upper bound on
the uniform norm of the difference between the true solutions and their
approximations. Then we use this bound to prove the consistency and asymptotic
normality of this estimation procedure. We show that the asymptotic covariance
matrix is the same as that of the maximum likelihood estimation. Therefore,
this procedure is asymptotically efficient. For a fixed sample and fixed basis
functions, we study the limiting behavior of the approximation when the
smoothing parameter tends to infinity. We propose an algorithm to choose the
smoothing parameters and a method to compute the deviation of the spline
approximation from solution without solving the ODEs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.3623</identifier>
 <datestamp>2012-07-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.3623</id><created>2009-03-20</created><authors><author><keyname>Slater</keyname><forenames>Paul B.</forenames></author></authors><title>Matrix plots of reordered bistochastized transaction flow tables: A
  United States intercounty migration example</title><categories>physics.soc-ph cs.SI physics.data-an stat.AP</categories><comments>12 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a number of variously rearranged matrix plots of the $3, 107
\times 3, 107$ 1995-2000 (asymmetric) intercounty migration table for the
United States, principally in its bistochasticized form (all 3,107 row and
column sums iteratively proportionally fitted to equal 1). In one set of plots,
the counties are seriated on the bases of the subdominant (left and right)
eigenvectors of the bistochastic matrix. In another set, we use the ordering of
counties in the dendrogram generated by the associated strong component
hierarchical clustering. Interesting, diverse features of U. S. intercounty
migration emerge--such as a contrast in centralized, hub-like
(cosmopolitan/provincial) properties between cosmopolitan "Sunbelt" and
provincial "Black Belt" counties. The methodologies employed should also be
insightful for the many other diverse forms of interesting transaction
flow-type data--interjournal citations being an obvious, much-studied example,
where one might expect that the journals Science, Nature and PNAS would display
"cosmopolitan" characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.3796</identifier>
 <datestamp>2012-06-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.3796</id><created>2009-03-23</created><authors><author><keyname>Bianconcini</keyname><forenames>Silvia</forenames></author><author><keyname>Cagnone</keyname><forenames>Silvia</forenames></author></authors><title>A general multivariate latent growth model with applications in student
  careers Data warehouses</title><categories>stat.AP</categories><comments>20 pages</comments><report-no>rfc 1867</report-no><doi>10.3102/1076998610396886</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evaluation of the formative process in the University system has been
assuming an ever increasing importance in the European countries. Within this
context the analysis of student performance and capabilities plays a
fundamental role. In this work we propose a multivariate latent growth model
for studying the performances of a cohort of students of the University of
Bologna. The model proposed is innovative since it is composed by: (1)
multivariate growth models that allow to capture the different dynamics of
student performance indicators over time and (2) a factor model that allows to
measure the general latent student capability. The flexibility of the model
proposed allows its applications in several fields such as socio-economic
settings in which personal behaviours are studied by using panel data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.4061</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.4061</id><created>2009-03-24</created><updated>2011-04-05</updated><authors><author><keyname>Vihola</keyname><forenames>Matti</forenames></author></authors><title>On the stability and ergodicity of adaptive scaling Metropolis
  algorithms</title><categories>math.PR math.ST stat.TH</categories><comments>24 pages, 1 figure; major revision</comments><msc-class>65C40, 60J27, 93E15, 93E35</msc-class><journal-ref>Stochastic Processes and their Applications 121(12):2839-2860,
  2011</journal-ref><doi>10.1016/j.spa.2011.08.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stability and ergodicity properties of two adaptive random walk
Metropolis algorithms are considered. The both algorithms adjust the scaling of
the proposal distribution continuously based on the observed acceptance
probability. Unlike the previously proposed forms of the algorithms, the
adapted scaling parameter is not constrained within a predefined compact
interval. The first algorithm is based on scale adaptation only, while the
second one incorporates also covariance adaptation. A strong law of large
numbers is shown to hold assuming that the target density is smooth enough and
has either compact support or super-exponentially decaying tails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.4817</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.4817</id><created>2009-03-27</created><updated>2012-10-25</updated><authors><author><keyname>Grtner</keyname><forenames>Bernd</forenames></author><author><keyname>Jaggi</keyname><forenames>Martin</forenames></author><author><keyname>Maria</keyname><forenames>Clment</forenames></author></authors><title>An Exponential Lower Bound on the Complexity of Regularization Paths</title><categories>cs.LG cs.CG cs.CV math.OC stat.ML</categories><comments>Journal version, 28 Pages, 5 Figures</comments><msc-class>90C20</msc-class><acm-class>F.2.2; I.5.1</acm-class><journal-ref>Journal of Computational Geometry (JoCG) 3(1), 168-195, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a variety of regularized optimization problems in machine learning,
algorithms computing the entire solution path have been developed recently.
Most of these methods are quadratic programs that are parameterized by a single
parameter, as for example the Support Vector Machine (SVM). Solution path
algorithms do not only compute the solution for one particular value of the
regularization parameter but the entire path of solutions, making the selection
of an optimal parameter much easier.
  It has been assumed that these piecewise linear solution paths have only
linear complexity, i.e. linearly many bends. We prove that for the support
vector machine this complexity can be exponential in the number of training
points in the worst case. More strongly, we construct a single instance of n
input points in d dimensions for an SVM such that at least \Theta(2^{n/2}) =
\Theta(2^d) many distinct subsets of support vectors occur as the
regularization parameter changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.5061</identifier>
 <datestamp>2010-03-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.5061</id><created>2009-03-29</created><updated>2010-03-17</updated><authors><author><keyname>Hoepfner</keyname><forenames>Reinhard</forenames></author><author><keyname>Kutoyants</keyname><forenames>Yury</forenames></author></authors><title>Estimating discontinuous periodic signals in a non-time homogeneous
  diffusion process</title><categories>math.ST stat.TH</categories><comments>42 pages</comments><msc-class>62 F 12 , 60 J 60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a diffusion $(\xi_t)_{t\ge 0}$ with some $T$-periodic time
dependent input term contained in the drift: under an unknown parameter
$\vth\in\Theta$, some discontinuity - an additional periodic signal - occurs at
times $kT{+}\vth$, $k\in\bbn$. Assuming positive Harris recurrence of
$(\xi_{kT})_{k\in\bbn_0}$ and exploiting the periodicity structure, we prove
limit theorems for certain martingales and functionals of the process
$(\xi_t)_{t\ge 0}$. They allow to consider the statistical model parametrized
by $\vth\in\Theta$ locally in small neighbourhoods of some fixed $\vth$, with
radius $1/n$ as $\nto$. We prove convergence of local models to a limit
experiment studied by Ibragimov and Khasminskii [IH 81] and discuss the
behaviour of estimators under contiguous alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.5255</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.5255</id><created>2009-03-30</created><updated>2012-11-13</updated><authors><author><keyname>Fan</keyname><forenames>Jianqing</forenames></author><author><keyname>Song</keyname><forenames>Rui</forenames></author></authors><title>Sure independence screening in generalized linear models with
  NP-dimensionality</title><categories>stat.ME math.ST stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS798 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS798</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 6, 3567-3604</journal-ref><doi>10.1214/10-AOS798</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ultrahigh-dimensional variable selection plays an increasingly important role
in contemporary scientific discoveries and statistical research. Among others,
Fan and Lv [J. R. Stat. Soc. Ser. B Stat. Methodol. 70 (2008) 849-911] propose
an independent screening framework by ranking the marginal correlations. They
showed that the correlation ranking procedure possesses a sure independence
screening property within the context of the linear model with Gaussian
covariates and responses. In this paper, we propose a more general version of
the independent learning with ranking the maximum marginal likelihood estimates
or the maximum marginal likelihood itself in generalized linear models. We show
that the proposed methods, with Fan and Lv [J. R. Stat. Soc. Ser. B Stat.
Methodol. 70 (2008) 849-911] as a very special case, also possess the sure
screening property with vanishing false selection rate. The conditions under
which the independence learning possesses a sure screening is surprisingly
simple. This justifies the applicability of such a simple method in a wide
spectrum. We quantify explicitly the extent to which the dimensionality can be
reduced by independence screening, which depends on the interactions of the
covariance matrix of covariates and true parameters. Simulation studies are
used to illustrate the utility of the proposed approaches. In addition, we
establish an exponential inequality for the quasi-maximum likelihood estimator
which is useful for high-dimensional statistical learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.5292</identifier>
 <datestamp>2010-12-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.5292</id><created>2009-03-30</created><updated>2009-03-30</updated><authors><author><keyname>Craiu</keyname><forenames>Radu V.</forenames></author><author><keyname>Di Narzo</keyname><forenames>Antonio Fabio</forenames></author></authors><title>A Mixture-Based Approach to Regional Adaptation for MCMC</title><categories>stat.CO</categories><journal-ref>Journal of Computational and Graphical Statistics, 2011</journal-ref><doi>10.1198/jcgs.2010.09035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in adaptive Markov chain Monte Carlo (AMCMC) include the need
for regional adaptation in situations when the optimal transition kernel is
different across different regions of the sample space. Motivated by these
findings, we propose a mixture-based approach to determine the partition needed
for regional AMCMC. The mixture model is fitted using an online EM algorithm
(see Andrieu and Moulines, 2006) which allows us to bypass simultaneously the
heavy computational load and to implement the regional adaptive algorithm with
online recursion (RAPTOR). The method is tried on simulated as well as real
data examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0903.5463</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0903.5463</id><created>2009-03-31</created><updated>2012-02-27</updated><authors><author><keyname>Stdler</keyname><forenames>Nicolas</forenames></author><author><keyname>Bhlmann</keyname><forenames>Peter</forenames></author></authors><title>Missing values: sparse inverse covariance estimation and an extension to
  sparse regression</title><categories>stat.ME</categories><comments>The final publication is available at http://www.springerlink.com</comments><journal-ref>Statistics and Computing, 2012, Volume 22, 219-235</journal-ref><doi>10.1007/s11222-010-9219-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an l1-regularized likelihood method for estimating the inverse
covariance matrix in the high-dimensional multivariate normal model in presence
of missing data. Our method is based on the assumption that the data are
missing at random (MAR) which entails also the completely missing at random
case. The implementation of the method is non-trivial as the observed negative
log-likelihood generally is a complicated and non-convex function. We propose
an efficient EM algorithm for optimization with provable numerical convergence
properties. Furthermore, we extend the methodology to handle missing values in
a sparse regression context. We demonstrate both methods on simulated and real
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.0068</identifier>
 <datestamp>2012-07-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.0068</id><created>2009-04-01</created><updated>2010-05-31</updated><authors><author><keyname>Iouditski</keyname><forenames>Anatoli</forenames><affiliation>LJK</affiliation></author><author><keyname>Karzan</keyname><forenames>Fatma Kilinc</forenames><affiliation>ISyE</affiliation></author><author><keyname>Nemirovski</keyname><forenames>Arkadii S.</forenames><affiliation>ISyE</affiliation></author></authors><title>Verifiable conditions of $\ell_1$-recovery of sparse signals with sign
  restrictions</title><categories>math.ST math.OC stat.TH</categories><proxy>ccsd</proxy><journal-ref>Mathematical Programming, 2011, 127(1): 89-122</journal-ref><doi>10.1007/s10107-010-0418-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose necessary and sufficient conditions for a sensing matrix to be
"s-semigood" -- to allow for exact $\ell_1$-recovery of sparse signals with at
most $s$ nonzero entries under sign restrictions on part of the entries. We
express the error bounds for imperfect $\ell_1$-recovery in terms of the
characteristics underlying these conditions. Furthermore, we demonstrate that
these characteristics, although difficult to evaluate, lead to verifiable
sufficient conditions for exact sparse $\ell_1$-recovery and to efficiently
computable upper bounds on those $s$ for which a given sensing matrix is
$s$-semigood. We concentrate on the properties of proposed verifiable
sufficient conditions of $s$-semigoodness and describe their limits of
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.0144</identifier>
 <datestamp>2010-04-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.0144</id><created>2009-04-01</created><updated>2010-04-19</updated><authors><author><keyname>Hashorva</keyname><forenames>Enkelejd</forenames></author></authors><title>Exact Tail Asymptotics of Dirichlet Distributions</title><categories>math.PR math.ST stat.TH</categories><comments>11 pages</comments><msc-class>60F05, 60G70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let X be a generalised symmetrised Dirichlet random vector in R^k, and let
u_n be thresholds such that P{X&gt; u_n} tends to 0 as n goes infinity. In this
paper we derive an exact asymptotic expansion of P{X&gt; u_n} assuming that the
associated random radius of X has distribution function in the Gumbel
max-domain of attraction
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.0365</identifier>
 <datestamp>2010-11-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.0365</id><created>2009-04-02</created><updated>2010-11-22</updated><authors><author><keyname>Birmele</keyname><forenames>Etienne</forenames></author></authors><title>Detection of network motifs by local concentration</title><categories>stat.AP q-bio.MN</categories><comments>This paper has been withdrawn as a new version with a different
  mathematical approach has been submitted</comments><report-no>SSB-23</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studying the topology of so-called {\em real networks}, that is networks
obtained from sociological or biological data for instance, has become a major
field of interest in the last decade. One way to deal with it is to consider
that networks are built from small functional units called {\em motifs}, which
can be found by looking for small subgraphs whose numbers of occurrences in the
whole network of interest are surprisingly high. In this paper, we propose to
define motifs through a local over-representation in the network and develop a
statistic which allows us to detect them limiting the number of false positives
and without time-consuming simulations. We apply it to the Yeast gene
interaction data and show that the known biologically relevant motifs are found
again and that our method gives some more information than the existing ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.0580</identifier>
 <datestamp>2010-02-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.0580</id><created>2009-04-03</created><updated>2009-09-22</updated><authors><author><keyname>Fougres</keyname><forenames>Anne-Laure</forenames><affiliation>ICJ</affiliation></author><author><keyname>Soulier</keyname><forenames>Philippe</forenames><affiliation>MODAL'X</affiliation></author></authors><title>Limit conditional distributions for bivariate vectors with polar
  representation</title><categories>math.PR math.ST stat.TH</categories><proxy>ccsd hal-00373049</proxy><journal-ref>Stocahstic models 26, 1 (2010) 54-77</journal-ref><doi>10.1080/15326340903291362</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate conditions for the existence of the limiting conditional
distribution of a bivariate random vector when one component becomes large. We
revisit the existing literature on the topic, and present some new sufficient
conditions. We concentrate on the case where the conditioning variable belongs
to the maximum domain of attraction of the Gumbel law, and we study geometric
conditions on the joint distribution of the vector. We show that these
conditions are of a local nature and imply asymptotic independence when both
variables belong to the domain of attraction of an extreme value distribution.
The new model we introduce can also be useful for simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.0584</identifier>
 <datestamp>2010-10-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.0584</id><created>2009-04-03</created><authors><author><keyname>Tomioka</keyname><forenames>Ryota</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Dual Augmented Lagrangian Method for Efficient Sparse Reconstruction</title><categories>stat.ML stat.ME</categories><comments>10 pages, 3 figures</comments><journal-ref>IEEE Signal Processing Letters, volume 16, issue 12, pages 1067 -
  1070, 2009</journal-ref><doi>10.1109/LSP.2009.2030111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an efficient algorithm for sparse signal reconstruction problems.
The proposed algorithm is an augmented Lagrangian method based on the dual
sparse reconstruction problem. It is efficient when the number of unknown
variables is much larger than the number of observations because of the dual
formulation. Moreover, the primal variable is explicitly updated and the
sparsity in the solution is exploited. Numerical comparison with the
state-of-the-art algorithms shows that the proposed algorithm is favorable when
the design matrix is poorly conditioned or dense and very large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.0635</identifier>
 <datestamp>2010-07-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.0635</id><created>2009-04-03</created><updated>2010-05-31</updated><authors><author><keyname>Blum</keyname><forenames>Michael</forenames><affiliation>TIMC</affiliation></author></authors><title>Approximate Bayesian Computation: a nonparametric perspective</title><categories>stat.CO math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate Bayesian Computation is a family of likelihood-free inference
techniques that are well-suited to models defined in terms of a stochastic
generating mechanism. In a nutshell, Approximate Bayesian Computation proceeds
by computing summary statistics s_obs from the data and simulating summary
statistics for different values of the parameter theta. The posterior
distribution is then approximated by an estimator of the conditional density
g(theta|s_obs). In this paper, we derive the asymptotic bias and variance of
the standard estimators of the posterior distribution which are based on
rejection sampling and linear adjustment. Additionally, we introduce an
original estimator of the posterior distribution based on quadratic adjustment
and we show that its bias contains a fewer number of terms than the estimator
with linear adjustment. Although we find that the estimators with adjustment
are not universally superior to the estimator based on rejection sampling, we
find that they can achieve better performance when there is a nearly
homoscedastic relationship between the summary statistics and the parameter of
interest. To make this relationship as homoscedastic as possible, we propose to
use transformations of the summary statistics. In different examples borrowed
from the population genetics and epidemiological literature, we show the
potential of the methods with adjustment and of the transformations of the
summary statistics. Supplemental materials containing the details of the proofs
are available online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.0645</identifier>
 <datestamp>2011-02-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.0645</id><created>2009-04-04</created><authors><author><keyname>Blocker</keyname><forenames>Alexander W.</forenames></author><author><keyname>Protopapas</keyname><forenames>Pavlos</forenames></author><author><keyname>Alcock</keyname><forenames>Charles R.</forenames></author></authors><title>A Bayesian approach to the analysis of time symmetry in light curves:
  Reconsidering Scorpius X-1 occultations</title><categories>astro-ph.IM astro-ph.EP stat.AP stat.ME</categories><comments>24 pages, 18 figures. Preprint typeset using LaTeX style emulateapj
  v. 04/20/08</comments><journal-ref>Astrophys.J.701:1742-1752,2009</journal-ref><doi>10.1088/0004-637X/701/2/1742</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach to the analysis of time symmetry in light curves,
such as those in the x-ray at the center of the Scorpius X-1 occultation
debate. Our method uses a new parameterization for such events (the bilogistic
event profile) and provides a clear, physically relevant characterization of
each event's key features. We also demonstrate a Markov Chain Monte Carlo
algorithm to carry out this analysis, including a novel independence chain
configuration for the estimation of each event's location in the light curve.
These tools are applied to the Scorpius X-1 light curves presented in Chang et
al. (2007), providing additional evidence based on the time series that the
events detected thus far are most likely not occultations by TNOs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.0838</identifier>
 <datestamp>2011-04-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.0838</id><created>2009-04-05</created><updated>2011-04-07</updated><authors><author><keyname>Shimizu</keyname><forenames>Shohei</forenames></author><author><keyname>Washio</keyname><forenames>Takashi</forenames></author><author><keyname>Hyvarinen</keyname><forenames>Aapo</forenames></author><author><keyname>Imoto</keyname><forenames>Seiya</forenames></author></authors><title>Finding Exogenous Variables in Data with Many More Variables than
  Observations</title><categories>stat.ML</categories><comments>A revised version of this was published in Proc. ICANN2010</comments><journal-ref>ARTIFICIAL NEURAL NETWORKS - ICANN 2010. Lecture Notes in Computer
  Science, 2010, Volume 6352/2010, 67-76</journal-ref><doi>10.1007/978-3-642-15819-3_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many statistical methods have been proposed to estimate causal models in
classical situations with fewer variables than observations (p&lt;n, p: the number
of variables and n: the number of observations). However, modern datasets
including gene expression data need high-dimensional causal modeling in
challenging situations with orders of magnitude more variables than
observations (p&gt;&gt;n). In this paper, we propose a method to find exogenous
variables in a linear non-Gaussian causal model, which requires much smaller
sample sizes than conventional methods and works even when p&gt;&gt;n. The key idea
is to identify which variables are exogenous based on non-Gaussianity instead
of estimating the entire structure of the model. Exogenous variables work as
triggers that activate a causal chain in the model, and their identification
leads to more efficient experimental designs and better understanding of the
causal mechanism. We present experiments with artificial data and real-world
gene expression data to evaluate the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.0951</identifier>
 <datestamp>2013-09-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.0951</id><created>2009-04-06</created><updated>2013-09-18</updated><authors><author><keyname>Chernozhukov</keyname><forenames>Victor</forenames></author><author><keyname>Fernandez-Val</keyname><forenames>Ivan</forenames></author><author><keyname>Melly</keyname><forenames>Blaise</forenames></author></authors><title>Inference on Counterfactual Distributions</title><categories>stat.ME stat.AP</categories><comments>55 pages, 1 table, 3 figures, supplementary appendix with additional
  results available from the authors' web sites</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counterfactual distributions are important ingredients for policy analysis
and decomposition analysis in empirical economics. In this article we develop
modeling and inference tools for counterfactual distributions based on
regression methods. The counterfactual scenarios that we consider consist of
ceteris paribus changes in either the distribution of covariates related to the
outcome of interest or the conditional distribution of the outcome given
covariates. For either of these scenarios we derive joint functional central
limit theorems and bootstrap validity results for regression-based estimators
of the status quo and counterfactual outcome distributions. These results allow
us to construct simultaneous confidence sets for function-valued effects of the
counterfactual changes, including the effects on the entire distribution and
quantile functions of the outcome as well as on related functionals. These
confidence sets can be used to test functional hypotheses such as no-effect,
positive effect, or stochastic dominance. Our theory applies to general
counterfactual changes and covers the main regression methods including
classical, quantile, duration, and distribution regressions. We illustrate the
results with an empirical application to wage decompositions using data for the
United States.
  As a part of developing the main results, we introduce distribution
regression as a comprehensive and flexible tool for modeling and estimating the
\textit{entire} conditional distribution. We show that distribution regression
encompasses the Cox duration regression and represents a useful alternative to
quantile regression. We establish functional central limit theorems and
bootstrap validity results for the empirical distribution regression process
and various related functionals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.0966</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.0966</id><created>2009-04-06</created><updated>2010-04-19</updated><authors><author><keyname>Hashorva</keyname><forenames>Enkelejd</forenames></author></authors><title>Exact Asymptotics of Bivariate Scale Mixture Distributions</title><categories>math.PR math.ST stat.TH</categories><comments>14 pages</comments><msc-class>60F05, 60G70</msc-class><doi>10.1007/s10687-011-0129-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let (RU_1, R U_2) be a given bivariate scale mixture random vector, with R&gt;0
being independent of the bivariate random vector (U_1,U_2). In this paper we
derive exact asymptotic expansions of the tail probability P{RU_1&gt; x, RU_2&gt;
ax}, a \in (0,1] as x tends infintiy assuming that R has distribution function
in the Gumbel max-domain of attraction and (U_1,U_2) has a specific tail
behaviour around some absorbing point. As a special case of our results we
retrieve the exact asymptotic behaviour of bivariate polar random vectors. We
apply our results to investigate the asymptotic independence and the asymptotic
behaviour of conditional excess for bivariate scale mixture distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.1300</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.1300</id><created>2009-04-08</created><authors><author><keyname>Martino</keyname><forenames>Luca</forenames></author><author><keyname>Miguez</keyname><forenames>Joaquin</forenames></author></authors><title>Generalized Rejection Sampling Schemes and Applications in Signal
  Processing</title><categories>stat.CO</categories><journal-ref>Signal Processing, Volume 90, Issue 11, Pages 2981-2995, 2010</journal-ref><doi>10.1016/j.sigpro.2010.04.025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian methods and their implementations by means of sophisticated Monte
Carlo techniques, such as Markov chain Monte Carlo (MCMC) and particle filters,
have become very popular in signal processing over the last years. However, in
many problems of practical interest these techniques demand procedures for
sampling from probability distributions with non-standard forms, hence we are
often brought back to the consideration of fundamental simulation algorithms,
such as rejection sampling (RS). Unfortunately, the use of RS techniques
demands the calculation of tight upper bounds for the ratio of the target
probability density function (pdf) over the proposal density from which
candidate samples are drawn. Except for the class of log-concave target pdf's,
for which an efficient algorithm exists, there are no general methods to
analytically determine this bound, which has to be derived from scratch for
each specific case. In this paper, we introduce new schemes for (a) obtaining
upper bounds for likelihood functions and (b) adaptively computing proposal
densities that approximate the target pdf closely. The former class of methods
provides the tools to easily sample from a posteriori probability distributions
(that appear very often in signal processing problems) by drawing candidates
from the prior distribution. However, they are even more useful when they are
exploited to derive the generalized adaptive RS (GARS) algorithm introduced in
the second part of the paper. The proposed GARS method yields a sequence of
proposal densities that converge towards the target pdf and enable a very
efficient sampling of a broad class of probability distributions, possibly with
multiple modes and non-standard forms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.1401</identifier>
 <datestamp>2010-11-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.1401</id><created>2009-04-08</created><updated>2010-11-29</updated><authors><author><keyname>Darses</keyname><forenames>Sbastien</forenames></author><author><keyname>Nourdin</keyname><forenames>Ivan</forenames></author><author><keyname>Nualart</keyname><forenames>David</forenames></author></authors><title>Limit theorems for nonlinear functionals of Volterra processes via white
  noise analysis</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ258 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ258</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 4, 1262-1293</journal-ref><doi>10.3150/10-BEJ258</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By means of white noise analysis, we prove some limit theorems for nonlinear
functionals of a given Volterra process. In particular, our results apply to
fractional Brownian motion (fBm) and should be compared with the classical
convergence results of the 1980s due to Breuer, Dobrushin, Giraitis, Major,
Surgailis and Taqqu, as well as the recent advances concerning the construction
of a L\'{e}vy area for fBm due to Coutin, Qian and Unterberger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.1551</identifier>
 <datestamp>2011-03-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.1551</id><created>2009-04-09</created><updated>2011-03-09</updated><authors><author><keyname>Chi</keyname><forenames>Zhiyi</forenames></author></authors><title>Effects of statistical dependence on multiple testing under a hidden
  Markov model</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS822 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS822</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 1, 439-473</journal-ref><doi>10.1214/10-AOS822</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of multiple hypothesis testing is known to be affected by the
statistical dependence among random variables involved. The mechanisms
responsible for this, however, are not well understood. We study the effects of
the dependence structure of a finite state hidden Markov model (HMM) on the
likelihood ratios critical for optimal multiple testing on the hidden states.
Various convergence results are obtained for the likelihood ratios as the
observations of the HMM form an increasing long chain. Analytic expansions of
the first and second order derivatives are obtained for the case of binary
states, explicitly showing the effects of the parameters of the HMM on the
likelihood ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.1950</identifier>
 <datestamp>2012-02-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.1950</id><created>2009-04-13</created><updated>2012-02-08</updated><authors><author><keyname>Goldenshluger</keyname><forenames>Alexander</forenames></author><author><keyname>Lepski</keyname><forenames>Oleg</forenames></author></authors><title>Uniform bounds for norms of sums of independent random functions</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOP595 the Annals of
  Probability (http://www.imstat.org/aop/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOP-AOP595</report-no><journal-ref>Annals of Probability 2011, Vol. 39, No. 6, 2318-2384</journal-ref><doi>10.1214/10-AOP595</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a general machinery for finding explicit uniform
probability and moment bounds on sub-additive positive functionals of random
processes. Using the developed general technique, we derive uniform bounds on
the ${\mathbb{L}}_s$-norms of empirical and regression-type processes.
Usefulness of the obtained results is illustrated by application to the
processes appearing in kernel density estimation and in nonparametric
estimation of regression functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.1980</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.1980</id><created>2009-04-13</created><updated>2011-10-19</updated><authors><author><keyname>Zwiernik</keyname><forenames>Piotr</forenames></author><author><keyname>Smith</keyname><forenames>Jim Q.</forenames></author></authors><title>Implicit inequality constraints in a binary tree model</title><categories>math.ST stat.TH</categories><msc-class>62H05, 62E15 (Primary), 60K99, 62F99 (Secondary)</msc-class><journal-ref>Electron. J. Statist. Volume 5 (2011), 1276-1312</journal-ref><doi>10.1214/11-EJS640</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the geometry of a discrete Bayesian network
whose graph is a tree all of whose variables are binary and the only observed
variables are those labeling its leaves. We provide the full geometric
description of these models which is given by a set of polynomial equations
together with a set of complementary implied inequalities induced by the
positivity of probabilities on hidden variables. The phylogenetic invariants
given by the equations can be useful in the construction of simple diagnostic
tests. However, in this paper we point out the importance of also incorporating
the associated inequalities into any statistical analysis. The full
characterization of these inequality constraints derived in this paper helps us
determine how and why routine statistical methods can break down for this model
class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.1990</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.1990</id><created>2009-04-13</created><updated>2013-03-26</updated><authors><author><keyname>Chernozhukov</keyname><forenames>Victor</forenames></author><author><keyname>Fernandez-Val</keyname><forenames>Ivan</forenames></author><author><keyname>Hahn</keyname><forenames>Jinyong</forenames></author><author><keyname>Newey</keyname><forenames>Whitney</forenames></author></authors><title>Average and Quantile Effects in Nonseparable Panel Models</title><categories>stat.ME math.ST stat.AP stat.TH</categories><comments>83 pages, 3 tables, 6 figures; includes supplementary appendix with
  proofs and additional results</comments><journal-ref>Econometrica (2013) 81 (2): 535-580</journal-ref><doi>10.3982/ECTA8405</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonseparable panel models are important in a variety of economic settings,
including discrete choice. This paper gives identification and estimation
results for nonseparable models under time homogeneity conditions that are like
"time is randomly assigned" or "time is an instrument." Partial identification
results for average and quantile effects are given for discrete regressors,
under static or dynamic conditions, in fully nonparametric and in
semiparametric models, with time effects. It is shown that the usual, linear,
fixed-effects estimator is not a consistent estimator of the identified average
effect, and a consistent estimator is given. A simple estimator of identified
quantile treatment effects is given, providing a solution to the important
problem of estimating quantile treatment effects from panel data. Bounds for
overall effects in static and dynamic models are given. The dynamic bounds
provide a partial identification solution to the important problem of
estimating the effect of state dependence in the presence of unobserved
heterogeneity. The impact of $T$, the number of time periods, is shown by
deriving shrinkage rates for the identified set as $T$ grows. We also consider
semiparametric, discrete-choice models and find that semiparametric panel
bounds can be much tighter than nonparametric bounds.
Computationally-convenient methods for semiparametric models are presented. We
propose a novel inference method that applies in panel data and other settings
and show that it produces uniformly valid confidence regions in large samples.
We give empirical illustrations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.2052</identifier>
 <datestamp>2011-12-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.2052</id><created>2009-04-14</created><updated>2009-10-17</updated><authors><author><keyname>Balabdaoui</keyname><forenames>Fadoua</forenames></author><author><keyname>Rufibach</keyname><forenames>Kaspar</forenames></author><author><keyname>Santambrogio</keyname><forenames>Filippo</forenames></author></authors><title>Least Squares estimation of two ordered monotone regression curves</title><categories>stat.ME math.ST stat.TH</categories><comments>23 pages, 2 figures. Second revised version according to reviewer
  comments</comments><journal-ref>J. Nonparametr. Stat. (2011), 22(8), 1019-1037</journal-ref><doi>10.1080/10485250903548729</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of finding the Least Squares
estimators of two isotonic regression curves $g^\circ_1$ and $g^\circ_2$ under
the additional constraint that they are ordered; e.g., $g^\circ_1 \le
g^\circ_2$. Given two sets of $n$ data points $y_1, ..., y_n$ and $z_1,
&gt;...,z_n$ observed at (the same) design points, the estimates of the true
curves are obtained by minimizing the weighted Least Squares criterion $L_2(a,
b) = \sum_{j=1}^n (y_j - a_j)^2 w_{1,j}+ \sum_{j=1}^n (z_j - b_j)^2 w_{2,j}$
over the class of pairs of vectors $(a, b) \in \mathbb{R}^n \times \mathbb{R}^n
$ such that $a_1 \le a_2 \le ...\le a_n $, $b_1 \le b_2 \le ...\le b_n $, and
$a_i \le b_i, i=1, ...,n$. The characterization of the estimators is
established. To compute these estimators, we use an iterative projected
subgradient algorithm, where the projection is performed with a "generalized"
pool-adjacent-violaters algorithm (PAVA), a byproduct of this work. Then, we
apply the estimation method to real data from mechanical engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.2144</identifier>
 <datestamp>2011-03-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.2144</id><created>2009-04-14</created><updated>2011-03-08</updated><authors><author><keyname>Douc</keyname><forenames>Randal</forenames></author><author><keyname>Robert</keyname><forenames>Christian P.</forenames></author></authors><title>A vanilla Rao--Blackwellization of Metropolis--Hastings algorithms</title><categories>stat.CO math.ST stat.ME stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS838 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS838</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 1, 261-277</journal-ref><doi>10.1214/10-AOS838</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Casella and Robert [Biometrika 83 (1996) 81--94] presented a general
Rao--Blackwellization principle for accept-reject and Metropolis--Hastings
schemes that leads to significant decreases in the variance of the resulting
estimators, but at a high cost in computation and storage. Adopting a
completely different perspective, we introduce instead a universal scheme that
guarantees variance reductions in all Metropolis--Hastings-based estimators
while keeping the computation cost under control. We establish a central limit
theorem for the improved estimators and illustrate their performances on toy
examples and on a probit model estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.2426</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.2426</id><created>2009-04-16</created><updated>2010-02-07</updated><authors><author><keyname>Abramov</keyname><forenames>Vyacheslav M.</forenames></author></authors><title>Statistical analysis of single-server loss queueing systems</title><categories>math.ST math.PR stat.TH</categories><comments>This is the version of the paper that addresses the reviewer's
  report. A software for this paper (executable file) can be found in my
  homepage</comments><msc-class>62G30, 60K25</msc-class><journal-ref>Methodology and Computing in Applied Probability 13 (2011),
  763-781</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article statistical bounds for certain output characteristics of the
$M/GI/1/n$ and $GI/M/1/n$ loss queueing systems are derived on the basis of
large samples of an input characteristic of these systems, such as service time
in the $M/GI/1/n$ queueing system or interarrival time in the $GI/M/1/n$
queueing system. The analysis of this article is based on application of
Kolmogorov's statistics for empirical probability distribution functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.2931</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.2931</id><created>2009-04-19</created><updated>2011-11-01</updated><authors><author><keyname>Belloni</keyname><forenames>Alexandre</forenames></author><author><keyname>Chernozhukov</keyname><forenames>Victor</forenames></author></authors><title>L1-Penalized Quantile Regression in High-Dimensional Sparse Models</title><categories>math.ST math.PR stat.ME stat.TH</categories><journal-ref>Annals of Statistics, Volume 39, Number 1, 2011, 82-130</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider median regression and, more generally, a possibly infinite
collection of quantile regressions in high-dimensional sparse models. In these
models the overall number of regressors $p$ is very large, possibly larger than
the sample size $n$, but only $s$ of these regressors have non-zero impact on
the conditional quantile of the response variable, where $s$ grows slower than
$n$. We consider quantile regression penalized by the $\ell_1$-norm of
coefficients ($\ell_1$-QR). First, we show that $\ell_1$-QR is consistent at
the rate $\sqrt{s/n} \sqrt{\log p}$. The overall number of regressors $p$
affects the rate only through the $\log p$ factor, thus allowing nearly
exponential growth in the number of zero-impact regressors. The rate result
holds under relatively weak conditions, requiring that $s/n$ converges to zero
at a super-logarithmic speed and that regularization parameter satisfies
certain theoretical constraints. Second, we propose a pivotal, data-driven
choice of the regularization parameter and show that it satisfies these
theoretical constraints. Third, we show that $\ell_1$-QR correctly selects the
true minimal model as a valid submodel, when the non-zero coefficients of the
true model are well separated from zero. We also show that the number of
non-zero coefficients in $\ell_1$-QR is of same stochastic order as $s$.
Fourth, we analyze the rate of convergence of a two-step estimator that applies
ordinary quantile regression to the selected model. Fifth, we evaluate the
performance of $\ell_1$-QR in a Monte-Carlo experiment, and illustrate its use
on an international economic growth application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.3132</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.3132</id><created>2009-04-20</created><updated>2014-04-22</updated><authors><author><keyname>Belloni</keyname><forenames>Alexandre</forenames></author><author><keyname>Chernozhukov</keyname><forenames>Victor</forenames></author></authors><title>Posterior Inference in Curved Exponential Families under Increasing
  Dimensions</title><categories>math.ST math.PR stat.ME stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the large sample properties of the posterior-based
inference in the curved exponential family under increasing dimension. The
curved structure arises from the imposition of various restrictions on the
model, such as moment restrictions, and plays a fundamental role in
econometrics and others branches of data analysis. We establish conditions
under which the posterior distribution is approximately normal, which in turn
implies various good properties of estimation and inference procedures based on
the posterior. In the process we also revisit and improve upon previous results
for the exponential family under increasing dimension by making use of
concentration of measure. We also discuss a variety of applications to
high-dimensional versions of the classical econometric models including the
multinomial model with moment restrictions, seemingly unrelated regression
equations, and single structural equation models. In our analysis, both the
parameter dimension and the number of moments are increasing with the sample
size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.3523</identifier>
 <datestamp>2011-11-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.3523</id><created>2009-04-22</created><updated>2010-05-31</updated><authors><author><keyname>Jenatton</keyname><forenames>Rodolphe</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Audibert</keyname><forenames>Jean-Yves</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>Structured Variable Selection with Sparsity-Inducing Norms</title><categories>stat.ML</categories><proxy>ccsd</proxy><journal-ref>Journal of Machine Learning Research 12 (2011) 2777-2824</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the empirical risk minimization problem for linear supervised
learning, with regularization by structured sparsity-inducing norms. These are
defined as sums of Euclidean norms on certain subsets of variables, extending
the usual $\ell_1$-norm and the group $\ell_1$-norm by allowing the subsets to
overlap. This leads to a specific set of allowed nonzero patterns for the
solutions of such problems. We first explore the relationship between the
groups defining the norm and the resulting nonzero patterns, providing both
forward and backward algorithms to go back and forth from groups to patterns.
This allows the design of norms adapted to specific prior knowledge expressed
in terms of nonzero patterns. We also present an efficient active set
algorithm, and analyze the consistency of variable selection for least-squares
linear regression in low and high-dimensional settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.3646</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.3646</id><created>2009-04-23</created><authors><author><keyname>Vlasov</keyname><forenames>Alexander Yu.</forenames></author></authors><title>Signed Chord Length Distribution. II</title><categories>math-ph math.MP math.PR stat.AP</categories><comments>LaTeX, 12 pages, 6 figures, part II (few bodies), part I is
  arXiv:0711.4734</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper continues description of applications of signed chord length
distribution started in part I (arXiv:0711.4734). It is shown simple relation
between equation for some transfer integrals with source and target bodies and
different geometrical distributions for union of this bodies. The union of
disjoint bodies is always nonconvex object and for such a case derivatives of
correlation function (used for definition of signed radii and chord lengths
distributions) always produce (quasi)densities with negative values. Many
equations used in this part are direct consequences of analogue formulas in
part I.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.3681</identifier>
 <datestamp>2011-05-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.3681</id><created>2009-04-23</created><updated>2009-12-08</updated><authors><author><keyname>Ohsugi</keyname><forenames>Hidefumi</forenames></author><author><keyname>Hibi</keyname><forenames>Takayuki</forenames></author></authors><title>Non-very ample configurations arising from contingency tables</title><categories>math.AC math.ST stat.TH</categories><comments>6 pages, Title changed. References, Lemma etc. added</comments><msc-class>13P10</msc-class><journal-ref>Annals of the Institute of Statistical Mathematics 62 (2010),
  639-644</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, it is proved that, if a toric ideal possesses a fundamental
binomial none of whose monomials is squarefree, then the corresponding
semigroup ring is not very ample. Moreover, very ample semigroup rings of
Lawrence type are discussed. As an application, we study very ampleness of
configurations arising from contingency tables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.4139</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.4139</id><created>2009-04-27</created><authors><author><keyname>Lemonte</keyname><forenames>Artur J.</forenames></author></authors><title>Influence diagnostics in Birnbaum-Saunders nonlinear regression models</title><categories>stat.ME</categories><comments>10 pages, submitted for publication</comments><doi>10.1080/02664761003692357</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the issue of assessing influence of observations in the class of
Birnbaum-Saunders nonlinear regression models, which is useful in lifetime data
analysis. Our results generalize those in Galea et al. [2004, Influence
diagnostics in log-Birnbaum-Saunders regression models. Journal of Applied
Statistics, 31, 1049-1064] which are confined to Birnbaum-Saunders linear
regression models. Some influence methods, such as the local influence, total
local influence of an individual and generalized leverage are discussed.
Additionally, the normal curvatures of local influence are derived under
various perturbation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.4358</identifier>
 <datestamp>2011-07-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.4358</id><created>2009-04-28</created><updated>2011-07-20</updated><authors><author><keyname>Rabi</keyname><forenames>Maben</forenames></author><author><keyname>Moustakides</keyname><forenames>George V.</forenames></author><author><keyname>Baras</keyname><forenames>John S.</forenames></author></authors><title>Adaptive sampling for linear state estimation</title><categories>math.OC cs.SY math.PR math.ST stat.TH</categories><comments>Submitted to the SIAM journal on control and optimization. 32 pages,
  7 figures</comments><report-no>IR-EE-RT 2009:019</report-no><msc-class>93E10, 93E11, 62L15, 60G40, 60G35, 62L12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a sensor has continuous measurements but sends limited messages over a
data network to a supervisor which estimates the state, the available packet
rate fixes the achievable quality of state estimation. When such rate limits
turn stringent, the sensor's messaging policy should be designed anew. What are
the good causal messaging policies ? What should message packets contain ? What
is the lowest possible distortion in a causal estimate at the supervisor ? Is
Delta sampling better than periodic sampling ? We answer these questions under
an idealized model of the network and the assumption of perfect measurements at
the sensor. For a scalar, linear diffusion process, we study the problem of
choosing the causal sampling times that will give the lowest aggregate squared
error distortion. We stick to finite-horizons and impose a hard upper bound on
the number of allowed samples. We cast the design as a problem of choosing an
optimal sequence of stopping times. We reduce this to a nested sequence of
problems each asking for a single optimal stopping time. Under an unproven but
natural assumption about the least-square estimate at the supervisor, each of
these single stopping problems are of standard form. The optimal stopping times
are random times when the estimation error exceeds designed envelopes. For the
case where the state is a Brownian motion, we give analytically: the shape of
the optimal sampling envelopes, the shape of the envelopes under optimal Delta
sampling, and their performances. Surprisingly, we find that Delta sampling
performs badly. Hence, when the rate constraint is a hard limit on the number
of samples over a finite horizon, we should should not use Delta sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.4863</identifier>
 <datestamp>2012-07-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.4863</id><created>2009-04-30</created><updated>2009-06-04</updated><authors><author><keyname>Slater</keyname><forenames>Paul B.</forenames></author></authors><title>A two-stage algorithm for extracting the multiscale backbone of complex
  weighted networks</title><categories>physics.soc-ph cs.SI physics.data-an stat.AP</categories><comments>1 page, letter accepted by PNAS in response to the article by MA
  Serrano, M Boguna, and A Vespignani, "Extracting the multiscale backbone of
  complex weighted networks", Proc Natl Acad Sci 106:6483-6488 (2009)</comments><journal-ref>PNAS June 30, 2009 vol. 106 no. 26 E66</journal-ref><doi>10.1073/pnas.0904725106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The central problem of concern to Serrano, Boguna and Vespignani ("Extracting
the multiscale backbone of complex weighted networks", Proc Natl Acad Sci
106:6483-6488 [2009]) can be effectively and elegantly addressed using a
well-established two-stage algorithm that has been applied to internal
migration flows for numerous nations and several other forms of "transaction
flow data".
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.4891</identifier>
 <datestamp>2010-09-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.4891</id><created>2009-04-30</created><updated>2010-09-26</updated><authors><author><keyname>Broderick</keyname><forenames>Tamara</forenames></author><author><keyname>Gramacy</keyname><forenames>Robert B.</forenames></author></authors><title>Classification and categorical inputs with treed Gaussian process models</title><categories>stat.ME stat.CO</categories><comments>24 pages (now single spaced), 8 figures, 2 tables, presented at IFCS
  2009 and accepted at JoC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognizing the successes of treed Gaussian process (TGP) models as an
interpretable and thrifty model for nonparametric regression, we seek to extend
the model to classification. Both treed models and Gaussian processes (GPs)
have, separately, enjoyed great success in application to classification
problems. An example of the former is Bayesian CART. In the latter, real-valued
GP output may be utilized for classification via latent variables, which
provide classification rules by means of a softmax function. We formulate a
Bayesian model averaging scheme to combine these two models and describe a
Monte Carlo method for sampling from the full posterior distribution with joint
proposals for the tree topology and the GP parameters corresponding to latent
variables at the leaves. We concentrate on efficient sampling of the latent
variables, which is important to obtain good mixing in the expanded parameter
space. The tree structure is particularly helpful for this task and also for
developing an efficient scheme for handling categorical predictors, which
commonly arise in classification problems. Our proposed classification TGP
(CTGP) methodology is illustrated on a collection of synthetic and real data
sets. We assess performance relative to existing methods and thereby show how
CTGP is highly flexible, offers tractable inference, produces rules that are
easy to interpret, and performs well out of sample.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0904.4903</identifier>
 <datestamp>2010-07-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0904.4903</id><created>2009-04-30</created><updated>2009-05-04</updated><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Tamuz</keyname><forenames>Omer</forenames></author></authors><title>Iterative Maximum Likelihood on Networks</title><categories>math.ST stat.TH</categories><comments>13 pages, two figures</comments><msc-class>62F99</msc-class><journal-ref>Advances in Applied Mathematics, 45(1):36-49, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider n agents located on the vertices of a connected graph. Each agent
v receives a signal X_v(0)~N(s, 1) where s is an unknown quantity. A natural
iterative way of estimating s is to perform the following procedure. At
iteration t + 1 let X_v(t + 1) be the average of X_v(t) and of X_w(t) among all
the neighbors w of v.
  In this paper we consider a variant of simple iterative averaging, which
models "greedy" behavior of the agents. At iteration t, each agent v declares
the value of its estimator X_v(t) to all of its neighbors. Then, it updates
X_v(t + 1) by taking the maximum likelihood (or minimum variance) estimator of
s, given X_v(t) and X_w(t) for all neighbors w of v, and the structure of the
graph.
  We give an explicit efficient procedure for calculating X_v(t), study the
convergence of the process as t goes to infinity and show that if the limit
exists then it is the same for all v and w. For graphs that are symmetric under
actions of transitive groups, we show that the process is efficient. Finally,
we show that the greedy process is in some cases more efficient than simple
averaging, while in other cases the converse is true, so that, in this model,
"greed" of the individual agents may or may not have an adverse affect on the
outcome.
  The model discussed here may be viewed as the Maximum-Likelihood version of
models studied in Bayesian Economics. The ML variant is more accessible and
allows in particular to show the significance of symmetry in the efficiency of
estimators using networks of agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.0436</identifier>
 <datestamp>2010-12-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.0436</id><created>2009-05-04</created><authors><author><keyname>Yao</keyname><forenames>Fang</forenames></author><author><keyname>Craiu</keyname><forenames>Radu V.</forenames></author><author><keyname>Reiser</keyname><forenames>Benjamin</forenames></author></authors><title>Nonparametric Covariate Adjustment for Receiver Operating Characteristic
  Curves</title><categories>stat.ME math.ST stat.TH</categories><journal-ref>Canadian Journal of Statistics, vol 38, No. 1, p 27-46, 2010</journal-ref><doi>10.1002/cjs.10044</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The accuracy of a diagnostic test is typically characterised using the
receiver operating characteristic (ROC) curve. Summarising indexes such as the
area under the ROC curve (AUC) are used to compare different tests as well as
to measure the difference between two populations. Often additional information
is available on some of the covariates which are known to influence the
accuracy of such measures. We propose nonparametric methods for covariate
adjustment of the AUC. Models with normal errors and non-normal errors are
discussed and analysed separately. Nonparametric regression is used for
estimating mean and variance functions in both scenarios. In the general noise
case we propose a covariate-adjusted Mann-Whitney estimator for AUC estimation
which effectively uses available data to construct working samples at any
covariate value of interest and is computationally efficient for
implementation. This provides a generalisation of the Mann-Whitney approach for
comparing two populations by taking covariate effects into account. We derive
asymptotic properties for the AUC estimators in both settings, including
asymptotic normality, optimal strong uniform convergence rates and MSE
consistency. The usefulness of the proposed methods is demonstrated through
simulated and real data examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.0603</identifier>
 <datestamp>2010-08-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.0603</id><created>2009-05-05</created><updated>2009-08-30</updated><authors><author><keyname>Kraemer</keyname><forenames>Nicole</forenames></author><author><keyname>Schaefer</keyname><forenames>Juliane</forenames></author><author><keyname>Boulesteix</keyname><forenames>Anne-Laure</forenames></author></authors><title>Regularized estimation of large-scale gene association networks using
  graphical Gaussian models</title><categories>stat.ME stat.AP stat.CO</categories><comments>added additional experiments</comments><journal-ref>BMC Bioinformatics, 10:384, 2010</journal-ref><doi>10.1186/1471-2105-10-384</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphical Gaussian models are popular tools for the estimation of
(undirected) gene association networks from microarray data. A key issue when
the number of variables greatly exceeds the number of samples is the estimation
of the matrix of partial correlations. Since the (Moore-Penrose) inverse of the
sample covariance matrix leads to poor estimates in this scenario, standard
methods are inappropriate and adequate regularization techniques are needed. In
this article, we investigate a general framework for combining regularized
regression methods with the estimation of Graphical Gaussian models. This
framework includes various existing methods as well as two new approaches based
on ridge regression and adaptive lasso, respectively. These methods are
extensively compared both qualitatively and quantitatively within a simulation
study and through an application to six diverse real data sets. In addition,
all proposed algorithms are implemented in the R package "parcor", available
from the R repository CRAN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.0668</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.0668</id><created>2009-05-05</created><updated>2010-07-15</updated><authors><author><keyname>Lemonte</keyname><forenames>Artur J.</forenames></author><author><keyname>Ferrari</keyname><forenames>Silvia L. P.</forenames></author></authors><title>Small-sample corrections for score tests in Birnbaum-Saunders
  regressions</title><categories>stat.ME</categories><comments>To appear in the Communications in Statistics - Theory and Methods,
  http://www.informaworld.com/smpp/title~content=t713597238</comments><doi>10.1080/03610920903402613</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we deal with the issue of performing accurate small-sample
inference in the Birnbaum-Saunders regression model, which can be useful for
modeling lifetime or reliability data. We derive a Bartlett-type correction for
the score test and numerically compare the corrected test with the usual score
test, the likelihood ratio test and its Bartlett-corrected version. Our
simulation results suggest that the corrected test we propose is more reliable
than the other tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.0940</identifier>
 <datestamp>2010-11-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.0940</id><created>2009-05-06</created><updated>2010-11-21</updated><authors><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Tong</keyname><forenames>Lang</forenames></author><author><keyname>Willsky</keyname><forenames>Alan S.</forenames></author></authors><title>A Large-Deviation Analysis of the Maximum-Likelihood Learning of Markov
  Tree Structures</title><categories>stat.ML cs.IT math.IT</categories><comments>Accepted to the IEEE Transactions on Information Theory on Nov 18,
  2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of maximum-likelihood (ML) estimation of discrete tree-structured
distributions is considered. Chow and Liu established that ML-estimation
reduces to the construction of a maximum-weight spanning tree using the
empirical mutual information quantities as the edge weights. Using the theory
of large-deviations, we analyze the exponent associated with the error
probability of the event that the ML-estimate of the Markov tree structure
differs from the true tree structure, given a set of independently drawn
samples. By exploiting the fact that the output of ML-estimation is a tree, we
establish that the error exponent is equal to the exponential rate of decay of
a single dominant crossover event. We prove that in this dominant crossover
event, a non-neighbor node pair replaces a true edge of the distribution that
is along the path of edges in the true tree graph connecting the nodes in the
non-neighbor pair. Using ideas from Euclidean information theory, we then
analyze the scenario of ML-estimation in the very noisy learning regime and
show that the error exponent can be approximated as a ratio, which is
interpreted as the signal-to-noise ratio (SNR) for learning tree distributions.
We show via numerical experiments that in this regime, our SNR approximation is
accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.1424</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.1424</id><created>2009-05-09</created><authors><author><keyname>Kuznetsov</keyname><forenames>Sergei O.</forenames></author><author><keyname>Ignatov</keyname><forenames>Dmitry I.</forenames></author></authors><title>Concept Stability for Constructing Taxonomies of Web-site Users</title><categories>cs.CY cs.AI cs.SI stat.ML</categories><comments>Sergei O. Kuznetsov, D.I. Ignatov, Concept Stability for Constructing
  Taxonomies of Web-site users, in Proc. Social Network Analysis and Conceptual
  Structures: Exploring Opportunities, S. Obiedkov, C. Roth (Eds.),
  Clermont-Ferrand (France), February 16, 2007</comments><acm-class>H.2.8; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Owners of a web-site are often interested in analysis of groups of users of
their site. Information on these groups can help optimizing the structure and
contents of the site. In this paper we use an approach based on formal concepts
for constructing taxonomies of user groups. For decreasing the huge amount of
concepts that arise in applications, we employ stability index of a concept,
which describes how a group given by a concept extent differs from other such
groups. We analyze resulting taxonomies of user groups for three target
websites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.2327</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.2327</id><created>2009-05-14</created><updated>2010-05-06</updated><authors><author><keyname>Hilgert</keyname><forenames>Nadine</forenames></author><author><keyname>Portier</keyname><forenames>Bruno</forenames></author></authors><title>Strong uniform consistency and asymptotic normality of a kernel based
  error density estimator in functional autoregressive models</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the innovation probability density is an important issue in any
regression analysis. This paper focuses on functional autoregressive models. A
residual-based kernel estimator is proposed for the innovation density.
Asymptotic properties of this estimator depend on the average prediction error
of the functional autoregressive function. Sufficient conditions are studied to
provide strong uniform consistency and asymptotic normality of the kernel
density estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.2646</identifier>
 <datestamp>2010-10-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.2646</id><created>2009-05-18</created><updated>2010-10-05</updated><authors><author><keyname>Yu</keyname><forenames>Yaming</forenames></author></authors><title>Monotonic convergence of a general algorithm for computing optimal
  designs</title><categories>stat.CO math.ST stat.ME stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS761 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS761</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 3, 1593-1606</journal-ref><doi>10.1214/09-AOS761</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monotonic convergence is established for a general class of multiplicative
algorithms introduced by Silvey, Titterington and Torsney [Comm. Statist.
Theory Methods 14 (1978) 1379--1389] for computing optimal designs. A
conjecture of Titterington [Appl. Stat. 27 (1978) 227--234] is confirmed as a
consequence. Optimal designs for logistic regression are used as an
illustration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.2776</identifier>
 <datestamp>2011-11-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.2776</id><created>2009-05-17</created><updated>2010-02-16</updated><authors><author><keyname>Honda</keyname><forenames>Junya</forenames></author><author><keyname>Takemura</keyname><forenames>Akimichi</forenames></author></authors><title>An Asymptotically Optimal Policy for Finite Support Models in the
  Multiarmed Bandit Problem</title><categories>math.ST stat.TH</categories><msc-class>62L05, 60G40</msc-class><journal-ref>Machine Learning 85 (2011) 361--391</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose minimum empirical divergence (MED) policy for the multiarmed
bandit problem. We prove asymptotic optimality of the proposed policy for the
case of finite support models. In our setting, Burnetas and Katehakis has
already proposed an asymptotically optimal policy. For choosing an arm our
policy uses a criterion which is dual to the quantity used in Burnetas and
Katehakis. Our criterion is easily computed by a convex optimization technique
and has an advantage in practical implementation. We confirm by simulations
that MED policy demonstrates good performance in finite time in comparison to
other currently popular policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.2979</identifier>
 <datestamp>2011-08-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.2979</id><created>2009-05-19</created><updated>2011-07-29</updated><authors><author><keyname>Bovy</keyname><forenames>Jo</forenames></author><author><keyname>Hogg</keyname><forenames>David W.</forenames></author><author><keyname>Roweis</keyname><forenames>Sam T.</forenames></author></authors><title>Extreme deconvolution: Inferring complete distribution functions from
  noisy, heterogeneous and incomplete observations</title><categories>stat.ME astro-ph.GA physics.data-an stat.AP stat.CO</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOAS439 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS439</report-no><journal-ref>Annals of Applied Statistics 2011, Vol. 5, No. 2B, 1657-1677</journal-ref><doi>10.1214/10-AOAS439</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the well-known mixtures of Gaussians approach to density
estimation and the accompanying Expectation--Maximization technique for finding
the maximum likelihood parameters of the mixture to the case where each data
point carries an individual $d$-dimensional uncertainty covariance and has
unique missing data properties. This algorithm reconstructs the
error-deconvolved or "underlying" distribution function common to all samples,
even when the individual data points are samples from different distributions,
obtained by convolving the underlying distribution with the heteroskedastic
uncertainty distribution of the data point and projecting out the missing data
directions. We show how this basic algorithm can be extended with conjugate
priors on all of the model parameters and a "split-and-merge" procedure
designed to avoid local maxima of the likelihood. We demonstrate the full
method by applying it to the problem of inferring the three-dimensional
velocity distribution of stars near the Sun from noisy two-dimensional,
transverse velocity measurements from the Hipparcos satellite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.3217</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.3217</id><created>2009-05-20</created><updated>2009-05-29</updated><authors><author><keyname>Hirakawa</keyname><forenames>Keigo</forenames></author><author><keyname>Wolfe</keyname><forenames>Patrick J.</forenames></author></authors><title>Skellam shrinkage: Wavelet-based intensity estimation for inhomogeneous
  Poisson data</title><categories>stat.ME stat.AP stat.CO</categories><comments>27 pages, 8 figures, slight formatting changes; submitted for
  publication</comments><journal-ref>IEEE Transactions on Information Theory, vol. 58, pp. 1080-1093,
  2012</journal-ref><doi>10.1109/TIT.2011.2165933</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ubiquity of integrating detectors in imaging and other applications
implies that a variety of real-world data are well modeled as Poisson random
variables whose means are in turn proportional to an underlying vector-valued
signal of interest. In this article, we first show how the so-called Skellam
distribution arises from the fact that Haar wavelet and filterbank transform
coefficients corresponding to measurements of this type are distributed as sums
and differences of Poisson counts. We then provide two main theorems on Skellam
shrinkage, one showing the near-optimality of shrinkage in the Bayesian setting
and the other providing for unbiased risk estimation in a frequentist context.
These results serve to yield new estimators in the Haar transform domain,
including an unbiased risk estimate for shrinkage of Haar-Fisz
variance-stabilized data, along with accompanying low-complexity algorithms for
inference. We conclude with a simulation study demonstrating the efficacy of
our Skellam shrinkage estimators both for the standard univariate wavelet test
functions as well as a variety of test images taken from the image processing
literature, confirming that they offer substantial performance improvements
over existing alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.3310</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.3310</id><created>2009-05-20</created><updated>2011-11-23</updated><authors><author><keyname>Aletti</keyname><forenames>Giacomo</forenames></author><author><keyname>May</keyname><forenames>Caterina</forenames></author><author><keyname>Secchi</keyname><forenames>Piercesare</forenames></author></authors><title>A functional equation whose unknown is P([0,1]) valued</title><categories>math.PR math.ST stat.TH</categories><comments>31 pages, pre-galleys version of accepted paper</comments><msc-class>62E10, 39B52, 62E20</msc-class><journal-ref>Journal of Theoretical Probability, December 2012, Volume 25,
  Issue 4, pp 1207-1232</journal-ref><doi>10.1007/s10959-011-0399-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a functional equation whose unknown maps a Euclidean space into the
space of probability distributions on [0,1]. We prove existence and uniqueness
of its solution under suitable regularity and boundary conditions, we show that
it depends continuously on the boundary datum, and we characterize solutions
that are diffuse on [0,1]. A canonical solution is obtained by means of a
Randomly Reinforced Urn with different reinforcement distributions having equal
means. The general solution to the functional equation defines a new parametric
collection of distributions on [0,1] generalizing the Beta family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.3321</identifier>
 <datestamp>2010-01-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.3321</id><created>2009-05-20</created><updated>2010-01-11</updated><authors><author><keyname>Mijatovi</keyname><forenames>Aleksandar</forenames></author><author><keyname>Schneider</keyname><forenames>Paul</forenames></author></authors><title>Globally optimal parameter estimates for nonlinear diffusions</title><categories>math.ST math.PR stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS710 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><report-no>IMS-AOS-AOS710</report-no><msc-class>62F12, 60J60 (Primary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 1, 215-245</journal-ref><doi>10.1214/09-AOS710</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies an approximation method for the log-likelihood function of
a nonlinear diffusion process using the bridge of the diffusion. The main
result (Theorem \refthm:approx) shows that this approximation converges
uniformly to the unknown likelihood function and can therefore be used
efficiently with any algorithm for sampling from the law of the bridge. We also
introduce an expected maximum likelihood (EML) algorithm for inferring the
parameters of discretely observed diffusion processes. The approach is
applicable to a subclass of nonlinear SDEs with constant volatility and drift
that is linear in the model parameters. In this setting, globally optimal
parameters are obtained in a single step by solving a linear system. Simulation
studies to test the EML algorithm show that it performs well when compared with
algorithms based on the exact maximum likelihood as well as closed-form
likelihood expansions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.3463</identifier>
 <datestamp>2010-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.3463</id><created>2009-05-21</created><updated>2010-11-09</updated><authors><author><keyname>Hosman</keyname><forenames>Carrie A.</forenames></author><author><keyname>Hansen</keyname><forenames>Ben B.</forenames></author><author><keyname>Holland</keyname><forenames>Paul W.</forenames></author></authors><title>The sensitivity of linear regression coefficients' confidence limits to
  the omission of a confounder</title><categories>stat.ME stat.AP</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOAS315 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS315</report-no><journal-ref>Annals of Applied Statistics 2010, Vol. 4, No. 2, 849-870</journal-ref><doi>10.1214/09-AOAS315</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Omitted variable bias can affect treatment effect estimates obtained from
observational data due to the lack of random assignment to treatment groups.
Sensitivity analyses adjust these estimates to quantify the impact of potential
omitted variables. This paper presents methods of sensitivity analysis to
adjust interval estimates of treatment effect---both the point estimate and
standard error---obtained using multiple linear regression. Central to our
approach is what we term benchmarking, the use of data to establish reference
points for speculation about omitted confounders. The method adapts to
treatment effects that may differ by subgroup, to scenarios involving omission
of multiple variables, and to combinations of covariance adjustment with
propensity score stratification. We illustrate it using data from an
influential study of health outcomes of patients admitted to critical care.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.3959</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.3959</id><created>2009-05-25</created><updated>2014-02-08</updated><authors><author><keyname>Modarresi</keyname><forenames>N.</forenames></author><author><keyname>Rezakhah</keyname><forenames>S.</forenames></author></authors><title>Characterization of Discrete Time Scale Invariant Markov Sequences</title><categories>math.PR math.ST stat.TH</categories><comments>20 pages</comments><msc-class>62L12, 60G22, 60G18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By considering special sampling of discrete scale invariant (DSI) processes
we provide a sequence which is in correspondence to multi-dimensional
self-similar process. By imposing Markov property we show that the covariance
functions of such discrete scale invariant Markov (DSIM) sequences are
characterized by variance, and covariance of adjacent samples in the first
scale interval. We also provide a theoretical method for estimating spectral
density matrix of corresponding multi-dimensional self-similar Markov process.
Some examples such as simple Brownian motion with drift and scale invariant
autoregressive model of order one are presented and these properties are
investigated. By simulating DSIM sequences we provide visualization of their
behavior and investigate these results. Finally we present a new method to
estimate Hurst parameter of DSI processes and show that it has much better
performance than maximum likelihood method for simulated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.4221</identifier>
 <datestamp>2010-03-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.4221</id><created>2009-05-26</created><updated>2009-12-15</updated><authors><author><keyname>Gagunashvili</keyname><forenames>N. D.</forenames></author></authors><title>Chi-Square Tests for Comparing Weighted Histograms</title><categories>physics.data-an math.ST physics.ins-det stat.TH</categories><comments>26 pages, 8 figures</comments><journal-ref>Nucl.Instrum.Meth.A614:287-296,2010</journal-ref><doi>10.1016/j.nima.2009.12.037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weighted histograms in Monte Carlo simulations are often used for the
estimation of probability density functions. They are obtained as a result of
random experiments with random events that have weights. In this paper, the bin
contents of a weighted histogram are considered as a sum of random variables
with a random number of terms. Generalizations of the classical chi-square test
for comparing weighted histograms was proposed. Numerical examples illustrate
an application of the tests for the histograms with different statistics of
events and different weighted functions. The proposed tests can be used for the
comparison of experimental data histograms with simulated data histograms, as
well as for the two simulated data histograms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.4602</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.4602</id><created>2009-05-28</created><updated>2012-05-02</updated><authors><author><keyname>Barone</keyname><forenames>Piero</forenames></author></authors><title>A black box method for solving the complex exponentials approximation
  problem</title><categories>stat.CO math.NA math.ST stat.TH</categories><comments>43 pages, 10 figures</comments><msc-class>30E10, 65C60</msc-class><journal-ref>Digital Signal Processing (2012)</journal-ref><doi>10.1016/j.dsp.2012.09.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common problem, arising in many different applied contexts, consists in
estimating the number of exponentially damped sinusoids whose weighted sum best
fits a finite set of noisy data and in estimating their parameters. Many
different methods exist to this purpose. The best of them are based on
approximate Maximum Likelihood estimators, assuming to know the number of
damped sinusoids, which can then be estimated by an order selection procedure.
As the problem can be severely ill posed, a stochastic perturbation method is
proposed which provides better results than Maximum Likelihood based methods
when the signal-to-noise ratio is low. The method depends on some
hyperparameters which turn out to be essentially independent of the
application. Therefore they can be fixed once and for all, giving rise to a
black box method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.4691</identifier>
 <datestamp>2010-01-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.4691</id><created>2009-05-28</created><updated>2009-07-10</updated><authors><author><keyname>Hall</keyname><forenames>Joseph Lorenzo</forenames><affiliation>University of California, Berkeley; School of Information</affiliation><affiliation>Princeton University; Center for Information Technology Policy</affiliation></author><author><keyname>Miratrix</keyname><forenames>Luke W.</forenames><affiliation>University of California, Berkeley; Department of Statistics</affiliation></author><author><keyname>Stark</keyname><forenames>Philip B.</forenames><affiliation>University of California, Berkeley; Department of Statistics</affiliation></author><author><keyname>Briones</keyname><forenames>Melvin</forenames><affiliation>Marin County, California; Registrar of Voters</affiliation></author><author><keyname>Ginnold</keyname><forenames>Elaine</forenames><affiliation>Marin County, California; Registrar of Voters</affiliation></author><author><keyname>Oakley</keyname><forenames>Freddie</forenames><affiliation>Yolo County, California; County Clerk/Recorder</affiliation></author><author><keyname>Peaden</keyname><forenames>Martin</forenames><affiliation>Santa Cruz County, California; County Clerk</affiliation></author><author><keyname>Pellerin</keyname><forenames>Gail</forenames><affiliation>Santa Cruz County, California; County Clerk</affiliation></author><author><keyname>Stanionis</keyname><forenames>Tom</forenames><affiliation>Yolo County, California; County Clerk/Recorder</affiliation></author><author><keyname>Webber</keyname><forenames>Tricia</forenames><affiliation>Santa Cruz County, California; County Clerk</affiliation></author></authors><title>Implementing Risk-Limiting Post-Election Audits in California</title><categories>stat.AP</categories><comments>Accepted to the Electronic Voting Technology Workshop/Workshop on
  Trustworthy Elections 2009 (EVT/WOTE '09),
  http://www.usenix.org/events/evtwote09/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Risk-limiting post-election audits limit the chance of certifying an
electoral outcome if the outcome is not what a full hand count would show.
Building on previous work, we report on pilot risk-limiting audits in four
elections during 2008 in three California counties: one during the February
2008 Primary Election in Marin County and three during the November 2008
General Elections in Marin, Santa Cruz and Yolo Counties. We explain what makes
an audit risk-limiting and how existing and proposed laws fall short. We
discuss the differences among our four pilot audits. We identify challenges to
practical, efficient risk-limiting audits and conclude that current approaches
are too complex to be used routinely on a large scale. One important logistical
bottleneck is the difficulty of exporting data from commercial election
management systems in a format amenable to audit calculations. Finally, we
propose a bare-bones risk-limiting audit that is less efficient than these
pilot audits, but avoids many practical problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.4841</identifier>
 <datestamp>2010-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.4841</id><created>2009-05-29</created><updated>2010-01-19</updated><authors><author><keyname>Rapallo</keyname><forenames>Fabio</forenames></author><author><keyname>Yoshida</keyname><forenames>Ruriko</forenames></author></authors><title>Markov bases and subbases for bounded contingency tables</title><categories>math.CO math.ST stat.TH</categories><comments>22 pages. It will appear in the Annals of the Institution of
  Statistical Mathematics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the computation of Markov bases for contingency tables
whose cell entries have an upper bound. In general a Markov basis for unbounded
contingency table under a certain model differs from a Markov basis for bounded
tables. Rapallo, (2007) applied Lawrence lifting to compute a Markov basis for
contingency tables whose cell entries are bounded. However, in the process, one
has to compute the universal Gr\"obner basis of the ideal associated with the
design matrix for a model which is, in general, larger than any reduced
Gr\"obner basis. Thus, this is also infeasible in small- and medium-sized
problems. In this paper we focus on bounded two-way contingency tables under
independence model and show that if these bounds on cells are positive, i.e.,
they are not structural zeros, the set of basic moves of all $2 \times 2$
minors connects all tables with given margins. We end this paper with an open
problem that if we know the given margins are positive, we want to find the
necessary and sufficient condition on the set of structural zeros so that the
set of basic moves of all $2 \times 2$ minors connects all incomplete
contingency tables with given margins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0905.4937</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0905.4937</id><created>2009-05-29</created><updated>2014-12-26</updated><authors><author><keyname>Ryabko</keyname><forenames>Daniil</forenames><affiliation>INRIA Futurs, LIFL, INRIA Lille - Nord Europe</affiliation></author></authors><title>A criterion for hypothesis testing for stationary processes</title><categories>math.ST cs.IT math.IT math.PR stat.TH</categories><comments>part or this report appeared as: Test, vol. 21(2), pp. 317-329, 2012</comments><proxy>ccsd inria-00389689</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a finite-valued sample $X_1,...,X_n$ we wish to test whether it was
generated by a stationary ergodic process belonging to a family $H_0$, or it
was generated by a stationary ergodic process outside $H_0$. We require the
Type I error of the test to be uniformly bounded, while the type II error has
to be mande not more than a finite number of times with probability 1. For this
notion of consistency we provide necessary and sufficient conditions on the
family $H_0$ for the existence of a consistent test. This criterion is
illustrated with applications to testing for a membership to parametric
families, generalizing some existing results. In addition, we analyze a
stronger notion of consistency, which requires finite-sample guarantees on
error of both types, and provide some necessary and some sufficient conditions
for the existence of a consistent test. We emphasize that no assumption on the
process distributions are made beyond stationarity and ergodicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.0113</identifier>
 <datestamp>2010-06-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.0113</id><created>2009-06-01</created><updated>2009-07-11</updated><authors><author><keyname>Patriota</keyname><forenames>Alexandre G.</forenames></author></authors><title>A note on Influence diagnostics in nonlinear mixed-effects elliptical
  models</title><categories>stat.ME</categories><comments>Paper submitted for possible publication, 6 pages</comments><doi>10.1016/j.csda.2010.06.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides general matrix formulas for computing the score function,
the (expected and observed) Fisher information and the $\Delta$ matrices
(required for the assessment of local influence) for a quite general model
which includes the one proposed by Russo et al. (2009). Additionally, we also
present an expression for the generalized leverage. The matrix formulation has
a considerable advantage, since although the complexity of the postulated
model, all general formulas are compact, clear and have nice forms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.0180</identifier>
 <datestamp>2011-01-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.0180</id><created>2009-05-31</created><authors><author><keyname>Soulier</keyname><forenames>Philippe</forenames><affiliation>MODAL'X</affiliation></author></authors><title>Best attainable rates of convergence for the estimation of the memory
  parameter</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00390120</proxy><journal-ref>Dependence in Probability and Statistics, Paul Doukhan, Gabriel
  Lang, Donatas Surgailis and Gilles Teyssi\`ere (Ed.) (2010) 45-37</journal-ref><doi>10.1007/978-3-642-14104-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this note is to prove a lower bound for the estimation of the
memory parameter of a stationary long memory process. The memory parameter is
defined here as the index of regular variation of the spectral density at 0.
The rates of convergence obtained in the literature assume second order regular
variation of the spectral density at zero. In this note, we do not make this
assumption, and show that the rates of convergence in this case can be
extremely slow. We prove that the log-periodogram regression (GPH) estimator
achieves the optimal rate of convergence for Gaussian long memory processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.0423</identifier>
 <datestamp>2010-10-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.0423</id><created>2009-06-02</created><updated>2010-10-05</updated><authors><author><keyname>Pensky</keyname><forenames>Marianna</forenames></author><author><keyname>Sapatinas</keyname><forenames>Theofanis</forenames></author></authors><title>On convergence rates equivalency and sampling strategies in functional
  deconvolution models</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS767 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS767</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 3, 1793-1844</journal-ref><doi>10.1214/09-AOS767</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the asymptotical minimax framework, we examine convergence rates
equivalency between a continuous functional deconvolution model and its
real-life discrete counterpart over a wide range of Besov balls and for the
$L^2$-risk. For this purpose, all possible models are divided into three
groups. For the models in the first group, which we call uniform, the
convergence rates in the discrete and the continuous models coincide no matter
what the sampling scheme is chosen, and hence the replacement of the discrete
model by its continuous counterpart is legitimate. For the models in the second
group, to which we refer as regular, one can point out the best sampling
strategy in the discrete model, but not every sampling scheme leads to the same
convergence rates; there are at least two sampling schemes which deliver
different convergence rates in the discrete model (i.e., at least one of the
discrete models leads to convergence rates that are different from the
convergence rates in the continuous model). The third group consists of models
for which, in general, it is impossible to devise the best sampling strategy;
we call these models irregular. We formulate the conditions when each of these
situations takes place. In the regular case, we not only point out the number
and the selection of sampling points which deliver the fastest convergence
rates in the discrete model but also investigate when, in the case of an
arbitrary sampling scheme, the convergence rates in the continuous model
coincide or do not coincide with the convergence rates in the discrete model.
We also study what happens if one chooses a uniform, or a more general
pseudo-uniform, sampling scheme which can be viewed as an intuitive replacement
of the continuous model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.0434</identifier>
 <datestamp>2011-07-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.0434</id><created>2009-06-02</created><authors><author><keyname>Chopra</keyname><forenames>Aditya</forenames></author><author><keyname>Lian</keyname><forenames>Heng</forenames></author></authors><title>Total Variation, Adaptive Total Variation and Nonconvex Smoothly Clipped
  Absolute Deviation Penalty for Denoising Blocky Images</title><categories>cs.CV cs.NA stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The total variation-based image denoising model has been generalized and
extended in numerous ways, improving its performance in different contexts. We
propose a new penalty function motivated by the recent progress in the
statistical literature on high-dimensional variable selection. Using a
particular instantiation of the majorization-minimization algorithm, the
optimization problem can be efficiently solved and the computational procedure
realized is similar to the spatially adaptive total variation model. Our
two-pixel image model shows theoretically that the new penalty function solves
the bias problem inherent in the total variation model. The superior
performance of the new penalty is demonstrated through several experiments. Our
investigation is limited to "blocky" images which have small total variation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.0874</identifier>
 <datestamp>2010-02-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.0874</id><created>2009-06-04</created><updated>2010-02-08</updated><authors><author><keyname>Sei</keyname><forenames>Tomonari</forenames></author></authors><title>A Jacobian inequality for gradient maps on the sphere and its
  application to directional statistics</title><categories>math.DG math.ST stat.TH</categories><comments>20 pages, 14 figures</comments><msc-class>49N60; 62E15; 90B06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of optimal transport theory, an optimal map is known to be a
gradient map of a potential function satisfying cost-convexity. In this paper,
the Jacobian determinant of a gradient map is shown to be log-concave with
respect to a convex combination of the potential functions when the underlying
manifold is the sphere and the cost function is the distance squared. The proof
uses the non-negative cross-curvature property of the sphere recently
established by Kim and McCann, and Figalli and Rifford. As an application to
statistics, a new family of probability densities on the sphere is defined in
terms of cost-convex functions. The log-concave property of the likelihood
function follows from the inequality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.1004</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.1004</id><created>2009-06-04</created><updated>2013-01-25</updated><authors><author><keyname>Harrison</keyname><forenames>Matthew T.</forenames></author></authors><title>A Dynamic Programming Approach for Approximate Uniform Generation of
  Binary Matrices with Specified Margins</title><categories>stat.CO</categories><comments>27 pages, minor typographic corrections from previous version,
  superseded by arXiv:1301.3928</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the collection of all binary matrices having a specific sequence of
row and column sums and consider sampling binary matrices uniformly from this
collection. Practical algorithms for exact uniform sampling are not known, but
there are practical algorithms for approximate uniform sampling. Here it is
shown how dynamic programming and recent asymptotic enumeration results can be
used to simplify and improve a certain class of approximate uniform samplers.
The dynamic programming perspective suggests interesting generalizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.1310</identifier>
 <datestamp>2011-02-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.1310</id><created>2009-06-06</created><updated>2011-02-03</updated><authors><author><keyname>Cheng</keyname><forenames>Guang</forenames></author><author><keyname>Huang</keyname><forenames>Jianhua Z.</forenames></author></authors><title>Bootstrap consistency for general semiparametric $M$-estimation</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS809 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS809</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 5, 2884-2915</journal-ref><doi>10.1214/10-AOS809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider $M$-estimation in a semiparametric model that is characterized by a
Euclidean parameter of interest and an infinite-dimensional nuisance parameter.
As a general purpose approach to statistical inferences, the bootstrap has
found wide applications in semiparametric $M$-estimation and, because of its
simplicity, provides an attractive alternative to the inference approach based
on the asymptotic distribution theory. The purpose of this paper is to provide
theoretical justifications for the use of bootstrap as a semiparametric
inferential tool. We show that, under general conditions, the bootstrap is
asymptotically consistent in estimating the distribution of the $M$-estimate of
Euclidean parameter; that is, the bootstrap distribution asymptotically
imitates the distribution of the $M$-estimate. We also show that the bootstrap
confidence set has the asymptotically correct coverage probability. These
general conclusions hold, in particular, when the nuisance parameter is not
estimable at root-$n$ rate, and apply to a broad class of bootstrap methods
with exchangeable bootstrap weights. This paper provides a first general
theoretical study of the bootstrap in semiparametric models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.2027</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.2027</id><created>2009-06-10</created><updated>2012-04-09</updated><authors><author><keyname>Keshavan</keyname><forenames>Raghunandan H.</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author></authors><title>Matrix Completion from Noisy Entries</title><categories>cs.LG stat.ML</categories><comments>22 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a matrix M of low-rank, we consider the problem of reconstructing it
from noisy observations of a small, random subset of its entries. The problem
arises in a variety of applications, from collaborative filtering (the `Netflix
problem') to structure-from-motion and positioning. We study a low complexity
algorithm introduced by Keshavan et al.(2009), based on a combination of
spectral techniques and manifold optimization, that we call here OptSpace. We
prove performance guarantees that are order-optimal in a number of
circumstances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.2098</identifier>
 <datestamp>2011-07-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.2098</id><created>2009-06-11</created><updated>2011-07-13</updated><authors><author><keyname>Marchetti</keyname><forenames>Giovanni M.</forenames></author><author><keyname>Lupparelli</keyname><forenames>Monia</forenames></author></authors><title>Chain graph models of multivariate regression type for categorical data</title><categories>stat.ME math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ300 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ300</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 3, 827-844</journal-ref><doi>10.3150/10-BEJ300</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss a class of chain graph models for categorical variables defined by
what we call a multivariate regression chain graph Markov property. First, the
set of local independencies of these models is shown to be Markov equivalent to
those of a chain graph model recently defined in the literature. Next we
provide a parametrization based on a sequence of generalized linear models with
a multivariate logistic link function that captures all independence
constraints in any chain graph model of this kind.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.2220</identifier>
 <datestamp>2011-08-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.2220</id><created>2009-06-11</created><authors><author><keyname>Chandrasekaran</keyname><forenames>Venkat</forenames></author><author><keyname>Sanghavi</keyname><forenames>Sujay</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo A.</forenames></author><author><keyname>Willsky</keyname><forenames>Alan S.</forenames></author></authors><title>Rank-Sparsity Incoherence for Matrix Decomposition</title><categories>math.OC math.ST stat.TH</categories><journal-ref>SIAM Journal on Optimization, Vol. 21, Issue 2, pp. 572-596, 2011</journal-ref><doi>10.1137/090761793</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose we are given a matrix that is formed by adding an unknown sparse
matrix to an unknown low-rank matrix. Our goal is to decompose the given matrix
into its sparse and low-rank components. Such a problem arises in a number of
applications in model and system identification, and is NP-hard in general. In
this paper we consider a convex optimization formulation to splitting the
specified matrix into its components, by minimizing a linear combination of the
$\ell_1$ norm and the nuclear norm of the components. We develop a notion of
\emph{rank-sparsity incoherence}, expressed as an uncertainty principle between
the sparsity pattern of a matrix and its row and column spaces, and use it to
characterize both fundamental identifiability as well as (deterministic)
sufficient conditions for exact recovery. Our analysis is geometric in nature,
with the tangent spaces to the algebraic varieties of sparse and low-rank
matrices playing a prominent role. When the sparse and low-rank matrices are
drawn from certain natural random ensembles, we show that the sufficient
conditions for exact recovery are satisfied with high probability. We conclude
with simulation results on synthetic matrix decomposition problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.2234</identifier>
 <datestamp>2011-01-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.2234</id><created>2009-06-11</created><updated>2011-01-10</updated><authors><author><keyname>Zhang</keyname><forenames>Zhongyang</forenames></author><author><keyname>Lange</keyname><forenames>Kenneth</forenames></author><author><keyname>Ophoff</keyname><forenames>Roel</forenames></author><author><keyname>Sabatti</keyname><forenames>Chiara</forenames></author></authors><title>Reconstructing DNA copy number by penalized estimation and imputation</title><categories>stat.ME q-bio.GN stat.AP</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOAS357 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS357</report-no><journal-ref>Annals of Applied Statistics 2010, Vol. 4, No. 4, 1749-1773</journal-ref><doi>10.1214/10-AOAS357</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in genomics have underscored the surprising ubiquity of DNA
copy number variation (CNV). Fortunately, modern genotyping platforms also
detect CNVs with fairly high reliability. Hidden Markov models and algorithms
have played a dominant role in the interpretation of CNV data. Here we explore
CNV reconstruction via estimation with a fused-lasso penalty as suggested by
Tibshirani and Wang [Biostatistics 9 (2008) 18--29]. We mount a fresh attack on
this difficult optimization problem by the following: (a) changing the penalty
terms slightly by substituting a smooth approximation to the absolute value
function, (b) designing and implementing a new MM (majorization--minimization)
algorithm, and (c) applying a fast version of Newton's method to jointly update
all model parameters. Together these changes enable us to minimize the
fused-lasso criterion in a highly effective way. We also reframe the
reconstruction problem in terms of imputation via discrete optimization. This
approach is easier and more accurate than parameter estimation because it
relies on the fact that only a handful of possible copy number states exist at
each SNP. The dynamic programming framework has the added bonus of exploiting
information that the current fused-lasso approach ignores. The accuracy of our
imputations is comparable to that of hidden Markov models at a substantially
lower computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.2717</identifier>
 <datestamp>2011-10-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.2717</id><created>2009-06-15</created><updated>2010-05-31</updated><authors><author><keyname>Bartkiewicz</keyname><forenames>Katarzyna</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Jakubowski</keyname><forenames>Adam</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Mikosch</keyname><forenames>Thomas</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Wintenberger</keyname><forenames>Olivier</forenames><affiliation>CEREMADE</affiliation></author></authors><title>Stable limits for sums of dependent infinite variance random variables</title><categories>math.PR math.ST stat.TH</categories><comments>35 pages</comments><proxy>ccsd</proxy><journal-ref>Probability Theory and Related Fields 150, 3 (2011) 337-372</journal-ref><doi>10.1007/s00440- 010-0276-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to provide conditions which ensure that the affinely
transformed partial sums of a strictly stationary process converge in
distribution to an infinite variance stable distribution. Conditions for this
convergence to hold are known in the literature. However, most of these results
are qualitative in the sense that the parameters of the limit distribution are
expressed in terms of some limiting point process. In this paper we will be
able to determine the parameters of the limiting stable distribution in terms
of some tail characteristics of the underlying stationary sequence. We will
apply our results to some standard time series models, including the GARCH(1,
1) process and its squares, the stochastic volatility models and solutions to
stochastic recurrence equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.2855</identifier>
 <datestamp>2010-04-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.2855</id><created>2009-06-16</created><authors><author><keyname>ekanaviius</keyname><forenames>Vydas</forenames></author><author><keyname>Pekz</keyname><forenames>Erol A.</forenames></author><author><keyname>Rllin</keyname><forenames>Adrian</forenames></author><author><keyname>Shwartz</keyname><forenames>Michael</forenames></author></authors><title>A Three-Parameter Binomial Approximation</title><categories>math.PR math.ST stat.TH</categories><msc-class>60F05</msc-class><journal-ref>J. Appl. Probab. Volume 46, Number 4 (2009), 1073-1085</journal-ref><doi>10.1239/jap/1261670689</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We approximate the distribution of the sum of independent but not necessarily
identically distributed Bernoulli random variables using a shifted binomial
distribution where the three parameters (the number of trials, the probability
of success, and the shift amount) are chosen to match up the first three
moments of the two distributions. We give a bound on the approximation error in
terms of the total variation metric using Stein's method. A numerical study is
discussed that shows shifted binomial approximations typically are more
accurate than Poisson or standard binomial approximations. The application of
the approximation to solving a problem arising in Bayesian hierarchical
modeling is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.3027</identifier>
 <datestamp>2012-02-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.3027</id><created>2009-06-16</created><updated>2012-02-06</updated><authors><author><keyname>Sokal</keyname><forenames>Alan D.</forenames></author></authors><title>When is a Riesz distribution a complex measure?</title><categories>math.CA math.RT math.ST stat.TH</categories><comments>LaTeX2e, 15 pages. Version 2 contains some small changes suggested by
  a referee</comments><msc-class>43A85 (Primary) 17A15, 17C99, 28C10, 44A10, 46F10, 47G10, 60E05,
  62H05 (Secondary)</msc-class><journal-ref>Bull. Soc. Math. France 139, 519-534 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let R_\alpha be the Riesz distribution on a simple Euclidean Jordan algebra,
parametrized by the complex number \alpha. I give an elementary proof of the
necessary and sufficient condition for R_\alpha to be a locally finite complex
measure (= complex Radon measure).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.3090</identifier>
 <datestamp>2011-08-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.3090</id><created>2009-06-17</created><updated>2010-01-21</updated><authors><author><keyname>Perry</keyname><forenames>Patrick O.</forenames></author><author><keyname>Wolfe</keyname><forenames>Patrick J.</forenames></author></authors><title>Minimax rank estimation for subspace tracking</title><categories>stat.ME stat.AP</categories><comments>10 pages, 4 figures; final version</comments><journal-ref>IEEE Journal of Selected Topics in Signal Processing, vol. 4, pp.
  504-513, 2010</journal-ref><doi>10.1109/JSTSP.2010.2048070</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rank estimation is a classical model order selection problem that arises in a
variety of important statistical signal and array processing systems, yet is
addressed relatively infrequently in the extant literature. Here we present
sample covariance asymptotics stemming from random matrix theory, and bring
them to bear on the problem of optimal rank estimation in the context of the
standard array observation model with additive white Gaussian noise. The most
significant of these results demonstrates the existence of a phase transition
threshold, below which eigenvalues and associated eigenvectors of the sample
covariance fail to provide any information on population eigenvalues. We then
develop a decision-theoretic rank estimation framework that leads to a simple
ordered selection rule based on thresholding; in contrast to competing
approaches, however, it admits asymptotic minimax optimality and is free of
tuning parameters. We analyze the asymptotic performance of our rank selection
procedure and conclude with a brief simulation study demonstrating its
practical efficacy in the context of subspace tracking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.3204</identifier>
 <datestamp>2012-01-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.3204</id><created>2009-06-17</created><updated>2009-10-07</updated><authors><author><keyname>Bhlmann</keyname><forenames>Peter</forenames></author><author><keyname>Kalisch</keyname><forenames>Markus</forenames></author><author><keyname>Maathuis</keyname><forenames>Marloes H.</forenames></author></authors><title>Variable selection in high-dimensional linear models: partially faithful
  distributions and the PC-simple algorithm</title><categories>stat.ME</categories><comments>20 pages, 3 figures</comments><journal-ref>Biometrika 2010, Vol. 97, No. 2, 261-278</journal-ref><doi>10.1093/biomet/asq008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider variable selection in high-dimensional linear models where the
number of covariates greatly exceeds the sample size. We introduce the new
concept of partial faithfulness and use it to infer associations between the
covariates and the response. Under partial faithfulness, we develop a
simplified version of the PC algorithm (Spirtes et al., 2000), the PC-simple
algorithm, which is computationally feasible even with thousands of covariates
and provides consistent variable selection under conditions on the random
design matrix that are of a different nature than coherence conditions for
penalty-based approaches like the Lasso. Simulations and application to real
data show that our method is competitive compared to penalty-based approaches.
We provide an efficient implementation of the algorithm in the R-package pcalg.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.3215</identifier>
 <datestamp>2012-01-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.3215</id><created>2009-06-17</created><authors><author><keyname>Maathuis</keyname><forenames>Marloes H.</forenames></author></authors><title>Reduction algorithm for the NPMLE for the distribution function of
  bivariate interval censored data</title><categories>stat.CO</categories><comments>12 pages, 3 figures</comments><journal-ref>Journal of Computational and Graphical Statistics 2005, Vol. 14,
  No. 2, 352-362</journal-ref><doi>10.1198/106186005X48470</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study computational aspects of the nonparametric maximum likelihood
estimator (NPMLE) for the distribution function of bivariate interval censored
data. The computation of the NPMLE consists of two steps: a parameter reduction
step and an optimization step. In this paper we focus on the reduction step. We
introduce two new reduction algorithms: the Tree algorithm and the HeightMap
algorithm. The Tree algorithm is only mentioned briefly. The HeightMap
algorithm is discussed in detail and also given in pseudo code. It is a very
fast and simple algorithm of time complexity O(n^2). This is an order faster
than the best known algorithm thus far, the O(n^3) algorithm of Bogaerts and
Lesaffre (2003). We compare our algorithms with the algorithms of Gentleman and
Vandal (2001), Song (2001) and Bogaerts and Lesaffre (2003), using simulated
data. We show that our algorithms, and especially the HeightMap algorithm, are
significantly faster. Finally, we point out that the HeightMap algorithm can be
easily generalized to d-dimensional data with d&gt;2. Such a multivariate version
of the HeightMap algorithm has time complexity O(n^d).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.3287</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.3287</id><created>2009-06-17</created><updated>2012-11-22</updated><authors><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author><author><keyname>Gleditsch</keyname><forenames>Kristian Skrede</forenames></author></authors><title>The developmental dynamics of terrorist organizations</title><categories>physics.soc-ph nlin.AO physics.data-an stat.AP</categories><comments>28 pages, 8 figures, 4 tables, supplementary material</comments><journal-ref>PLOS ONE 7(11): e48633 (2012)</journal-ref><doi>10.1371/journal.pone.0048633</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We identify robust statistical patterns in the frequency and severity of
violent attacks by terrorist organizations as they grow and age. Using
group-level static and dynamic analyses of terrorist events worldwide from
1968-2008 and a simulation model of organizational dynamics, we show that the
production of violent events tends to accelerate with increasing size and
experience. This coupling of frequency, experience and size arises from a
fundamental positive feedback loop in which attacks lead to growth which leads
to increased production of new attacks. In contrast, event severity is
independent of both size and experience. Thus larger, more experienced
organizations are more deadly because they attack more frequently, not because
their attacks are more deadly, and large events are equally likely to come from
large and small organizations. These results hold across political ideologies
and time, suggesting that the frequency and severity of terrorism may be
constrained by fundamental processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.3465</identifier>
 <datestamp>2010-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.3465</id><created>2009-06-18</created><updated>2010-11-09</updated><authors><author><keyname>Allen</keyname><forenames>Genevera I.</forenames></author><author><keyname>Tibshirani</keyname><forenames>Robert</forenames></author></authors><title>Transposable regularized covariance models with an application to
  missing data imputation</title><categories>stat.AP stat.ME stat.ML</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOAS314 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS314</report-no><journal-ref>Annals of Applied Statistics 2010, Vol. 4, No. 2, 764-790</journal-ref><doi>10.1214/09-AOAS314</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Missing data estimation is an important challenge with high-dimensional data
arranged in the form of a matrix. Typically this data matrix is transposable,
meaning that either the rows, columns or both can be treated as features. To
model transposable data, we present a modification of the matrix-variate
normal, the mean-restricted matrix-variate normal, in which the rows and
columns each have a separate mean vector and covariance matrix. By placing
additive penalties on the inverse covariance matrices of the rows and columns,
these so-called transposable regularized covariance models allow for maximum
likelihood estimation of the mean and nonsingular covariance matrices. Using
these models, we formulate EM-type algorithms for missing data imputation in
both the multivariate and transposable frameworks. We present theoretical
results exploiting the structure of our transposable models that allow these
models and imputation methods to be applied to high-dimensional data.
Simulations and results on microarray data and the Netflix data show that these
imputation techniques often outperform existing methods and offer a greater
degree of flexibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.3558</identifier>
 <datestamp>2011-03-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.3558</id><created>2009-06-18</created><updated>2011-03-15</updated><authors><author><keyname>Simkin</keyname><forenames>M. V.</forenames></author><author><keyname>Roychowdhury</keyname><forenames>V. P.</forenames></author></authors><title>Estimating achievement from fame</title><categories>physics.soc-ph cs.CY physics.hist-ph stat.AP</categories><journal-ref>Significance 8 (2011) 22-26</journal-ref><doi>10.1111/j.1740-9713.2011.00473.x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report a method for estimating people's achievement based on their fame.
Earlier we discovered (cond-mat/0310049) that fame of fighter pilot aces
(measured as number of Google hits) grows exponentially with their achievement
(number of victories). We hypothesize that the same functional relation between
achievement and fame holds for other professions. This allows us to estimate
achievement for professions where an unquestionable and universally accepted
measure of achievement does not exist. We apply the method to Nobel Prize
winners in Physics. For example, we obtain that Paul Dirac, who is hundred
times less famous than Einstein contributed to physics only two times less. We
compare our results with Landau's ranking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.3849</identifier>
 <datestamp>2010-06-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.3849</id><created>2009-06-21</created><authors><author><keyname>Yu</keyname><forenames>Yaming</forenames></author></authors><title>Squeezing the Arimoto-Blahut algorithm for faster convergence</title><categories>cs.IT math.IT stat.CO</categories><journal-ref>IEEE Trans. Inform. Theory 56 (2010) 3149-3157</journal-ref><doi>10.1109/TIT.2010.2048452</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Arimoto--Blahut algorithm for computing the capacity of a discrete
memoryless channel is revisited. A so-called ``squeezing'' strategy is used to
design algorithms that preserve its simplicity and monotonic convergence
properties, but have provably better rates of convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.4073</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.4073</id><created>2009-06-22</created><updated>2010-07-29</updated><authors><author><keyname>Bryc</keyname><forenames>Wlodzimierz</forenames></author><author><keyname>Hassairi</keyname><forenames>Abdelhamid</forenames></author></authors><title>One-sided Cauchy-Stieltjes Kernel Families</title><categories>math.PR math.ST stat.TH</categories><msc-class>60E10, 46L54</msc-class><journal-ref>Journ. Theoret. Probab. 24 (2011) 577--594</journal-ref><doi>10.1007/s10959-010-0303-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper continues the study of a kernel family which uses the
Cauchy-Stieltjes kernel in place of the celebrated exponential kernel of the
exponential families theory. We extend the theory to cover generating measures
with support that is unbounded on one side. We illustrate the need for such an
extension by showing that cubic pseudo-variance functions correspond to
free-infinitely divisible laws without the first moment. We also determine the
domain of means, advancing the understanding of Cauchy-Stieltjes kernel
families also for compactly supported generating measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.4205</identifier>
 <datestamp>2011-02-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.4205</id><created>2009-06-23</created><updated>2011-02-23</updated><authors><author><keyname>Catellier</keyname><forenames>Rmi</forenames><affiliation>CMLA</affiliation></author><author><keyname>Mazliak</keyname><forenames>Laurent</forenames><affiliation>PMA</affiliation></author></authors><title>The emergence of French statistics. How mathematics entered the world of
  statistics in France during the 1920s</title><categories>math.HO math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns the emergence of modern mathematical statistics in France
after the First World War. Emile Borel's achievements are presented, and
especially his creation of two institutions where mathematical statistics was
developed: the {\it Statistical Institute of Paris University}, (ISUP) in 1922
and above all the {\it Henri Poincar\'e Institute} (IHP) in 1928. At the IHP, a
new journal {\it Annales de l'Institut Henri Poincar\'e} was created in 1931.
We discuss the first papers in that journal dealing with mathematical
statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.4258</identifier>
 <datestamp>2010-08-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.4258</id><created>2009-06-23</created><authors><author><keyname>Zien</keyname><forenames>Alexander</forenames></author><author><keyname>Kraemer</keyname><forenames>Nicole</forenames></author><author><keyname>Sonnenburg</keyname><forenames>Soeren</forenames></author><author><keyname>Raetsch</keyname><forenames>Gunnar</forenames></author></authors><title>The Feature Importance Ranking Measure</title><categories>stat.ML</categories><comments>15 pages, 3 figures. to appear in the Proceedings of the European
  Conference on Machine Learning and Principles and Practice of Knowledge
  Discovery in Databases (ECML/PKDD), 2009</comments><journal-ref>Proceedings of the European Conference on Machine Learning and
  Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD),
  Lecture Notes in Computer Science 5782, 694 - 709, 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most accurate predictions are typically obtained by learning machines with
complex feature spaces (as e.g. induced by kernels). Unfortunately, such
decision rules are hardly accessible to humans and cannot easily be used to
gain insights about the application domain. Therefore, one often resorts to
linear models in combination with variable selection, thereby sacrificing some
predictive power for presumptive interpretability. Here, we introduce the
Feature Importance Ranking Measure (FIRM), which by retrospective analysis of
arbitrary learning machines allows to achieve both excellent predictive
performance and superior interpretation. In contrast to standard raw feature
weighting, FIRM takes the underlying correlation structure of the features into
account. Thereby, it is able to discover the most relevant features, even if
their appearance in the training data is entirely prevented by noise. The
desirable properties of FIRM are investigated analytically and illustrated in
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.4329</identifier>
 <datestamp>2012-06-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.4329</id><created>2009-06-23</created><updated>2012-06-13</updated><authors><author><keyname>Maruyama</keyname><forenames>Yuzo</forenames></author></authors><title>A Bayes factor with reasonable model selection consistency for ANOVA
  model</title><categories>stat.ME</categories><comments>a major revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the ANOVA model, we propose a new g-prior based Bayes factor without
integral representation, with reasonable model selection consistency for any
asymptotic situations (either number of levels of the factor and/or number of
replication in each level goes to infinity). Exact analytic calculation of the
marginal density under a special choice of the priors enables such a Bayes
factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.4582</identifier>
 <datestamp>2010-04-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.4582</id><created>2009-06-24</created><authors><author><keyname>Belabbas</keyname><forenames>Mohamed-Ali</forenames></author><author><keyname>Wolfe</keyname><forenames>Patrick J.</forenames></author></authors><title>On landmark selection and sampling in high-dimensional data analysis</title><categories>stat.ML cs.CV cs.LG</categories><comments>18 pages, 6 figures, submitted for publication</comments><journal-ref>Philosophical Transactions of the Royal Society, Series A, vol.
  367, pp. 4295-4312, 2009</journal-ref><doi>10.1098/rsta.2009.0161</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, the spectral analysis of appropriately defined kernel
matrices has emerged as a principled way to extract the low-dimensional
structure often prevalent in high-dimensional data. Here we provide an
introduction to spectral methods for linear and nonlinear dimension reduction,
emphasizing ways to overcome the computational limitations currently faced by
practitioners with massive datasets. In particular, a data subsampling or
landmark selection process is often employed to construct a kernel based on
partial information, followed by an approximate spectral analysis termed the
Nystrom extension. We provide a quantitative framework to analyse this
procedure, and use it to demonstrate algorithmic performance bounds on a range
of practical approaches designed to optimize the landmark selection process. We
compare the practical implications of these bounds by way of real-world
examples drawn from the field of computer vision, whereby low-dimensional
manifold structure is shown to emerge from high-dimensional video data streams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.4754</identifier>
 <datestamp>2010-08-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.4754</id><created>2009-06-25</created><updated>2009-09-23</updated><authors><author><keyname>Dobigeon</keyname><forenames>Nicolas</forenames></author><author><keyname>Moussaoui</keyname><forenames>Said</forenames></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames></author><author><keyname>Carteret</keyname><forenames>Cedric</forenames></author></authors><title>Bayesian separation of spectral sources under non-negativity and full
  additivity constraints</title><categories>stat.ME</categories><comments>v4: minor grammatical changes; Signal Processing, 2009</comments><journal-ref>Signal Processing, vol. 89, no. 12, pp. 2657-2669, Dec. 2009</journal-ref><doi>10.1016/j.sigpro.2009.05.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of separating spectral sources which are
linearly mixed with unknown proportions. The main difficulty of the problem is
to ensure the full additivity (sum-to-one) of the mixing coefficients and
non-negativity of sources and mixing coefficients. A Bayesian estimation
approach based on Gamma priors was recently proposed to handle the
non-negativity constraints in a linear mixture model. However, incorporating
the full additivity constraint requires further developments. This paper
studies a new hierarchical Bayesian model appropriate to the non-negativity and
sum-to-one constraints associated to the regressors and regression coefficients
of linear mixtures. The estimation of the unknown parameters of this model is
performed using samples generated using an appropriate Gibbs sampler. The
performance of the proposed algorithm is evaluated through simulation results
conducted on synthetic mixture models. The proposed approach is also applied to
the processing of multicomponent chemical mixtures resulting from Raman
spectroscopy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.4980</identifier>
 <datestamp>2014-08-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.4980</id><created>2009-06-26</created><authors><author><keyname>Olding</keyname><forenames>Benjamin P.</forenames></author><author><keyname>Wolfe</keyname><forenames>Patrick J.</forenames></author></authors><title>Inference for graphs and networks: Extending classical tools to modern
  data</title><categories>stat.ME stat.AP</categories><comments>16 pages, 6 figures; submitted for publication</comments><doi>10.1142/9781783263752_0001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs and networks provide a canonical representation of relational data,
with massive network data sets becoming increasingly prevalent across a variety
of scientific fields. Although tools from mathematics and computer science have
been eagerly adopted by practitioners in the service of network inference, they
do not yet comprise a unified and coherent framework for the statistical
analysis of large-scale network data. This paper serves as both an introduction
to the topic and a first step toward formal inference procedures. We develop
and illustrate our arguments using the example of hypothesis testing for
network structure. We invoke a generalized likelihood ratio framework and use
it to highlight the growing number of topics in this area that require strong
contributions from statistical science. We frame our discussion in the context
of previous work from across a variety of disciplines, and conclude by
outlining fundamental statistical challenges whose solutions will in turn serve
to advance the science of network inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.4982</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.4982</id><created>2009-06-26</created><authors><author><keyname>Ignatov</keyname><forenames>Dmitry I.</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Sergei O.</forenames></author></authors><title>Concept-based Recommendations for Internet Advertisement</title><categories>cs.AI cs.CY cs.IR stat.ML</categories><comments>D.I.Ignatov, S.O. Kuznetsov. Concept-based Recommendations for
  Internet Advertisement//In proceedings of The Sixth International Conference
  Concept Lattices and Their Applications (CLA'08), Olomouc, Czech Republic,
  2008 ISBN 978-80-244-2111-7</comments><acm-class>I.2.1; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of detecting terms that can be interesting to the advertiser is
considered. If a company has already bought some advertising terms which
describe certain services, it is reasonable to find out the terms bought by
competing companies. A part of them can be recommended as future advertising
terms to the company. The goal of this work is to propose better interpretable
recommendations based on FCA and association rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.5045</identifier>
 <datestamp>2010-07-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.5045</id><created>2009-06-27</created><updated>2010-05-24</updated><authors><author><keyname>Srivastava</keyname><forenames>Radhendushka</forenames></author><author><keyname>Sengupta</keyname><forenames>Debasis</forenames></author></authors><title>Consistent estimation of non-bandlimited spectral density from uniformly
  spaced samples</title><categories>math.ST stat.TH</categories><journal-ref>IEEE Trans. Inform. Theory vol 56 issue 8 (2010), 3642 - 3659</journal-ref><doi>10.1109/TIT.2010.2050807</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the matter of selection of sample time points for the estimation of the
power spectral density of a continuous time stationary stochastic process,
irregular sampling schemes such as Poisson sampling are often preferred over
regular (uniform) sampling. A major reason for this preference is the
well-known problem of inconsistency of estimators based on regular sampling,
when the underlying power spectral density is not bandlimited. It is argued in
this paper that, in consideration of a large sample property like consistency,
it is natural to allow the sampling rate to go to infinity as the sample size
goes to infinity. Through appropriate asymptotic calculations under this
scenario, it is shown that the smoothed periodogram based on regularly spaced
data is a consistent estimator of the spectral density, even when the latter is
not band-limited. It transpires that, under similar assumptions, the estimators
based on uniformly sampled and Poisson-sampled data have about the same rate of
convergence. Apart from providing this reassuring message, the paper also gives
a guideline for practitioners regarding appropriate choice of the sampling
rate. Theoretical calculations for large samples and Monte-Carlo simulations
for small samples indicate that the smoothed periodogram based on uniformly
sampled data have less variance and more bias than its counterpart based on
Poisson sampled data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.5462</identifier>
 <datestamp>2011-09-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.5462</id><created>2009-06-30</created><updated>2011-09-15</updated><authors><author><keyname>Rauh</keyname><forenames>Johannes</forenames></author><author><keyname>Kahle</keyname><forenames>Thomas</forenames></author><author><keyname>Ay</keyname><forenames>Nihat</forenames></author></authors><title>Support Sets in Exponential Families and Oriented Matroid Theory</title><categories>math.ST stat.TH</categories><comments>27 pages, extended version published in IJAR</comments><msc-class>52C40, 62B05, 14P15</msc-class><journal-ref>International Journal of Approximate Reasoning Volume 52, Issue 5,
  July 2011, Pages 613-626</journal-ref><doi>10.1016/j.ijar.2011.01.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The closure of a discrete exponential family is described by a finite set of
equations corresponding to the circuits of an underlying oriented matroid.
These equations are similar to the equations used in algebraic statistics,
although they need not be polynomial in the general case. This description
allows for a combinatorial study of the possible support sets in the closure of
an exponential family. If two exponential families induce the same oriented
matroid, then their closures have the same support sets. Furthermore, the
positive cocircuits give a parameterization of the closure of the exponential
family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0906.5546</identifier>
 <datestamp>2011-10-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0906.5546</id><created>2009-06-30</created><updated>2011-10-08</updated><authors><author><keyname>Vellaisamy</keyname><forenames>P.</forenames></author></authors><title>A-Collapsibility of Distribution Dependence and Quantile Regression
  Coefficients</title><categories>stat.ME math.ST stat.AP stat.TH</categories><comments>The paper has fifteen pages and has been accepted for publication in
  Scandinavian Journal of Statistics</comments><msc-class>Primary: 62H20, Secondary: 62J05, 62J12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Yule-Simpson paradox notes that an association between random variables
can be reversed when averaged over a background variable. Cox and Wermuth
(2003) introduced the concept of distribution dependence between two random
variables X and Y, and developed two dependence conditions, each of which
guarantees that reversal cannot occur. Ma, Xie and Geng (2006) studied the
collapsibility of distribution dependence over a background variable W, under a
rather strong homogeneity condition. Collapsibility ensures the association
remains the same for conditional and marginal models, so that Yule-Simpson
reversal cannot occur. In this paper, we investigate a more general condition
for avoiding effect reversal: A-collapsibility. The conditions of Cox and
Wermuth imply A-collapsibility, without assuming homogeneity. In fact, we show
that, when W is a binary variable, collapsibility is equivalent to
A-collapsibility plus homogeneity, and A-collapsibility is equivalent to the
conditions of Cox and Wermuth. Recently, Cox (2007) extended Cochran's result
on regression coefficients of conditional and marginal models, to quantile
regression coefficients. The conditions of Cox and Wermuth are sufficient for
A-collapsibility of quantile regression coefficients. If the conditional
distribution of W, given Y = y and X = x, belong to one-dimensional natural
exponential family, they are also necessary. Some applications of
A-collapsibility include the analysis of a contingency table, linear regression
models and quantile regression models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.0077</identifier>
 <datestamp>2011-08-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.0077</id><created>2009-07-01</created><updated>2011-08-09</updated><authors><author><keyname>Davydov</keyname><forenames>Youri</forenames></author><author><keyname>Molchanov</keyname><forenames>Ilya</forenames></author><author><keyname>Zuyev</keyname><forenames>Sergei</forenames></author></authors><title>Stability for random measures, point processes and discrete semigroups</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ301 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ301</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 3, 1015-1043</journal-ref><doi>10.3150/10-BEJ301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrete stability extends the classical notion of stability to random
elements in discrete spaces by defining a scaling operation in a randomised
way: an integer is transformed into the corresponding binomial distribution.
Similarly defining the scaling operation as thinning of counting measures we
characterise the corresponding discrete stability property of point processes.
It is shown that these processes are exactly Cox (doubly stochastic Poisson)
processes with strictly stable random intensity measures. We give spectral and
LePage representations for general strictly stable random measures without
assuming their independent scattering. As a consequence, spectral
representations are obtained for the probability generating functional and void
probabilities of discrete stable processes. An alternative cluster
representation for such processes is also derived using the so-called Sibuya
point processes, which constitute a new family of purely random point
processes. The obtained results are then applied to explore stable random
elements in discrete semigroups, where the scaling is defined by means of
thinning of a point process on the basis of the semigroup. Particular examples
include discrete stable vectors that generalise discrete stable random
variables and the family of natural numbers with the multiplication operation,
where the primes form the basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.0079</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.0079</id><created>2009-07-01</created><updated>2012-05-09</updated><authors><author><keyname>Cator</keyname><forenames>Eric A.</forenames></author><author><keyname>Lopuha</keyname><forenames>Hendrik P.</forenames></author></authors><title>Central limit theorem and influence function for the MCD estimators at
  general multivariate distributions</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/11-BEJ353 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ353</report-no><journal-ref>Bernoulli 2012, Vol. 18, No. 2, 520-551</journal-ref><doi>10.3150/11-BEJ353</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define the minimum covariance determinant functionals for multivariate
location and scatter through trimming functions and establish their existence
at any multivariate distribution. We provide a precise characterization
including a separating ellipsoid property and prove that the functionals are
continuous. Moreover, we establish asymptotic normality for both the location
and covariance estimator and derive the influence function. These results are
obtained in a very general multivariate setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.0139</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.0139</id><created>2009-07-01</created><updated>2009-10-28</updated><authors><author><keyname>Bickel</keyname><forenames>David R.</forenames></author></authors><title>Coherent frequentism</title><categories>math.ST math.PR stat.ME stat.TH</categories><comments>The confidence-measure theory of inference and decision is explicitly
  extended to vector parameters of interest. The derivation of upper and lower
  confidence levels from valid and nonconservative set estimators is formalized</comments><journal-ref>Bickel, D. R. (2012). Coherent Frequentism: A Decision Theory
  Based on Confidence Sets. Communications in Statistics - Theory and Methods,
  41(8), 1478-1496</journal-ref><doi>10.1080/03610926.2010.543302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By representing the range of fair betting odds according to a pair of
confidence set estimators, dual probability measures on parameter space called
frequentist posteriors secure the coherence of subjective inference without any
prior distribution. The closure of the set of expected losses corresponding to
the dual frequentist posteriors constrains decisions without arbitrarily
forcing optimization under all circumstances. This decision theory reduces to
those that maximize expected utility when the pair of frequentist posteriors is
induced by an exact or approximate confidence set estimator or when an
automatic reduction rule is applied to the pair. In such cases, the resulting
frequentist posterior is coherent in the sense that, as a probability
distribution of the parameter of interest, it satisfies the axioms of the
decision-theoretic and logic-theoretic systems typically cited in support of
the Bayesian posterior. Unlike the p-value, the confidence level of an interval
hypothesis derived from such a measure is suitable as an estimator of the
indicator of hypothesis truth since it converges in sample-space probability to
1 if the hypothesis is true or to 0 otherwise under general conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.0250</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.0250</id><created>2009-07-01</created><updated>2011-04-21</updated><authors><author><keyname>Schuhmacher</keyname><forenames>Dominic</forenames></author><author><keyname>Huesler</keyname><forenames>Andre</forenames></author><author><keyname>Duembgen</keyname><forenames>Lutz</forenames></author></authors><title>Multivariate Log-Concave Distributions as a Nearly Parametric Model</title><categories>math.PR math.ST stat.TH</categories><comments>updated two references, changed the local technical report number</comments><report-no>Technical report 74, IMSV, University of Bern</report-no><msc-class>62A01, 62G05, 62G07, 62G15, 62G35</msc-class><journal-ref>Statistics and Risk Modeling, Volume 28, Number 3 (2011), pp.
  277-295</journal-ref><doi>10.1524/stnd.2011.1073</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that the family P_d of probability distributions on R^d
with log-concave densities satisfies a strong continuity condition. In
particular, it turns out that weak convergence within this family entails (i)
convergence in total variation distance, (ii) convergence of arbitrary moments,
and (iii) pointwise convergence of Laplace transforms. Hence the nonparametric
model P_d has similar properties as parametric models such as, for instance,
the family of all d-variate Gaussian distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.0440</identifier>
 <datestamp>2010-04-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.0440</id><created>2009-07-02</created><authors><author><keyname>Dachian</keyname><forenames>Serguei</forenames></author></authors><title>On Limiting Likelihood Ratio Processes of some Change-Point Type
  Statistical Models</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00401299</proxy><msc-class>62F99, 62M99</msc-class><journal-ref>Journal of Statistical Planning and Inference (2010) to appear</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different change-point type models encountered in statistical inference for
stochastic processes give rise to different limiting likelihood ratio
processes. In this paper we consider two such likelihood ratios. The first one
is an exponential functional of a two-sided Poisson process driven by some
parameter, while the second one is an exponential functional of a two-sided
Brownian motion. We establish that for sufficiently small values of the
parameter, the Poisson type likelihood ratio can be approximated by the
Brownian type one. As a consequence, several statistically interesting
quantities (such as limiting variances of different estimators) related to the
first likelihood ratio can also be approximated by those related to the second
one. Finally, we discuss the asymptotics of the large values of the parameter
and illustrate the results by numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.0619</identifier>
 <datestamp>2012-02-17</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.0619</id><created>2009-07-03</created><updated>2012-02-15</updated><authors><author><keyname>Giraud</keyname><forenames>Christophe</forenames><affiliation>MIAJ, CMAP</affiliation></author><author><keyname>Huet</keyname><forenames>Sylvie</forenames><affiliation>MIAJ</affiliation></author><author><keyname>Verzelen</keyname><forenames>Nicolas</forenames><affiliation>LM-Orsay</affiliation></author></authors><title>Graph selection with GGMselect</title><categories>math.ST stat.TH</categories><comments>44 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications on inference of biological networks have raised a strong
interest in the problem of graph estimation in high-dimensional Gaussian
graphical models. To handle this problem, we propose a two-stage procedure
which first builds a family of candidate graphs from the data, and then selects
one graph among this family according to a dedicated criterion. This estimation
procedure is shown to be consistent in a high-dimensional setting, and its risk
is controlled by a non-asymptotic oracle-like inequality. The procedure is
tested on a real data set concerning gene expression data, and its performances
are assessed on the basis of a large numerical study. The procedure is
implemented in the R-package GGMselect available on the CRAN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.1020</identifier>
 <datestamp>2013-09-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.1020</id><created>2009-07-06</created><updated>2013-09-17</updated><authors><author><keyname>Tadic</keyname><forenames>Vladislav B.</forenames></author></authors><title>Convergence and Convergence Rate of Stochastic Gradient Search in the
  Case of Multiple and Non-Isolated Extrema</title><categories>math.OC math.ST stat.TH</categories><msc-class>62L20, 90C15, 93E12, 93E35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The asymptotic behavior of stochastic gradient algorithms is studied. Relying
on results from differential geometry (Lojasiewicz gradient inequality), the
single limit-point convergence of the algorithm iterates is demonstrated and
relatively tight bounds on the convergence rate are derived. In sharp contrast
to the existing asymptotic results, the new results presented here allow the
objective function to have multiple and non-isolated minima. The new results
also offer new insights into the asymptotic properties of several classes of
recursive algorithms which are routinely used in engineering, statistics,
machine learning and operations research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.1100</identifier>
 <datestamp>2013-09-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.1100</id><created>2009-07-06</created><updated>2013-09-20</updated><authors><author><keyname>Girolami</keyname><forenames>Mark</forenames></author><author><keyname>Calderhead</keyname><forenames>Ben</forenames></author><author><keyname>Chin</keyname><forenames>Siu A.</forenames></author></authors><title>Riemannian Manifold Hamiltonian Monte Carlo</title><categories>stat.CO math.NA math.ST stat.TH</categories><comments>This paper has been withdrawn by the posting author because he is no
  longer a co-author of this work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proposes a Riemannian Manifold Hamiltonian Monte Carlo sampler to
resolve the shortcomings of existing Monte Carlo algorithms when sampling from
target densities that may be high dimensional and exhibit strong correlations.
The method provides a fully automated adaptation mechanism that circumvents the
costly pilot runs required to tune proposal densities for Metropolis-Hastings
or indeed Hybrid Monte Carlo and Metropolis Adjusted Langevin Algorithms. This
allows for highly efficient sampling even in very high dimensions where
different scalings may be required for the transient and stationary phases of
the Markov chain. The proposed method exploits the Riemannian structure of the
parameter space of statistical models and thus automatically adapts to the
local manifold structure at each step based on the metric tensor. A
semi-explicit second order symplectic integrator for non-separable Hamiltonians
is derived for simulating paths across this manifold which provides highly
efficient convergence and exploration of the target density. The performance of
the Riemannian Manifold Hamiltonian Monte Carlo method is assessed by
performing posterior inference on logistic regression models, log-Gaussian Cox
point processes, stochastic volatility models, and Bayesian estimation of
parameter posteriors of dynamical systems described by nonlinear differential
equations. Substantial improvements in the time normalised Effective Sample
Size are reported when compared to alternative sampling approaches. Matlab code
at \url{http://www.dcs.gla.ac.uk/inference/rmhmc} allows replication of all
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.1254</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.1254</id><created>2009-07-07</created><updated>2011-10-03</updated><authors><author><keyname>Cornuet</keyname><forenames>Jean-Marie</forenames><affiliation>CBGP, INRA, Montpellier</affiliation></author><author><keyname>Marin</keyname><forenames>Jean-Michel</forenames><affiliation>I3M, Montpellier</affiliation></author><author><keyname>Mira</keyname><forenames>Antonietta</forenames><affiliation>University of Lugano</affiliation></author><author><keyname>Robert</keyname><forenames>Christian P.</forenames><affiliation>Universite Paris Dauphine</affiliation></author></authors><title>Adaptive Multiple Importance Sampling</title><categories>stat.CO stat.AP</categories><comments>20 pages, 3 figures, revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Adaptive Multiple Importance Sampling (AMIS) algorithm is aimed at an
optimal recycling of past simulations in an iterated importance sampling
scheme. The difference with earlier adaptive importance sampling
implementations like Population Monte Carlo is that the importance weights of
all simulated values, past as well as present, are recomputed at each
iteration, following the technique of the deterministic multiple mixture
estimator of Owen and Zhou (2000). Although the convergence properties of the
algorithm cannot be fully investigated, we demonstrate through a challenging
banana shape target distribution and a population genetics example that the
improvement brought by this technique is substantial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.1787</identifier>
 <datestamp>2010-02-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.1787</id><created>2009-07-10</created><updated>2010-02-16</updated><authors><author><keyname>Lavancier</keyname><forenames>Frdric</forenames><affiliation>LMJL</affiliation></author><author><keyname>Philippe</keyname><forenames>Anne</forenames><affiliation>LMJL</affiliation></author><author><keyname>Surgailis</keyname><forenames>Donatas</forenames></author></authors><title>A two-sample test for comparison of long memory parameters</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00403445</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a two-sample test for comparison of long memory parameters based
on ratios of two rescaled variance (V/S) statistics studied in [Giraitis L.,
Leipus, R., Philippe, A., 2006. A test for stationarity versus trends and unit
roots for a wide class of dependent errors. Econometric Theory 21, 989--1029].
The two samples have the same length and can be mutually independent or
dependent. In the latter case, the test statistic is modified to make it
asymptotically free of the long-run correlation coefficient between the
samples. To diminish the sensitivity of the test on the choice of the bandwidth
parameter, an adaptive formula for the bandwidth parameter is derived using the
asymptotic expansion in [Abadir, K., Distaso, W., Giraitis, L., 2009. Two
estimators of the long-run variance: Beyond short memory. Journal of
Econometrics 150, 56--70]. A simulation study shows that the above choice of
bandwidth leads to a good size of our comparison test for most values of
fractional and ARMA parameters of the simulated series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.1997</identifier>
 <datestamp>2011-10-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.1997</id><created>2009-07-12</created><updated>2011-10-27</updated><authors><author><keyname>Leonid</keyname><affiliation>Aryeh</affiliation></author><author><keyname>Kontorovich</keyname></author></authors><title>Statistical estimation requires unbounded memory</title><categories>stat.CO</categories><comments>this is an old version, with a mistake in the proof of Thm. 6.1.
  Please see my homepage for an updated version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the existence of bounded-memory consistent estimators of
various statistical functionals. This question is resolved in the negative in a
rather strong sense. We propose various bounded-memory approximations, using
techniques from automata theory and stochastic processes. Some questions of
potential interest are raised for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.2135</identifier>
 <datestamp>2010-02-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.2135</id><created>2009-07-13</created><updated>2010-02-27</updated><authors><author><keyname>Gramacy</keyname><forenames>Robert B.</forenames></author><author><keyname>Pantaleo</keyname><forenames>Ester</forenames></author></authors><title>Shrinkage regression for multivariate inference with missing data, and
  an application to portfolio balancing</title><categories>stat.ME stat.AP</categories><comments>25 pages, 4 figures, 2 tables, to appear in BA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Portfolio balancing requires estimates of covariance between asset returns.
Returns data have histories which greatly vary in length, since assets begin
public trading at different times. This can lead to a huge amount of missing
data--too much for the conventional imputation-based approach. Fortunately, a
well-known factorization of the MVN likelihood under the prevailing historical
missingness pattern leads to a simple algorithm of OLS regressions that is much
more reliable. When there are more assets than returns, however, OLS becomes
unstable. Gramacy, et al. (2008), showed how classical shrinkage regression may
be used instead, thus extending the state of the art to much bigger asset
collections, with further accuracy and interpretation advantages. In this
paper, we detail a fully Bayesian hierarchical formulation that extends the
framework further by allowing for heavy-tailed errors, relaxing the historical
missingness assumption, and accounting for estimation risk. We illustrate how
this approach compares favorably to the classical one using synthetic data and
an investment exercise with real returns. An accompanying R package is on CRAN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.2337</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.2337</id><created>2009-07-14</created><updated>2013-04-02</updated><authors><author><keyname>Kolar</keyname><forenames>Mladen</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Sparsistent Estimation of Time-Varying Discrete Markov Random Fields</title><categories>stat.ML</categories><comments>Updated references. Reorganized proofs. Added simulation studies</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network models have been popular for modeling and representing complex
relationships and dependencies between observed variables. When data comes from
a dynamic stochastic process, a single static network model cannot adequately
capture transient dependencies, such as, gene regulatory dependencies
throughout a developmental cycle of an organism. Kolar et al (2010b) proposed a
method based on kernel-smoothing l1-penalized logistic regression for
estimating time-varying networks from nodal observations collected from a
time-series of observational data. In this paper, we establish conditions under
which the proposed method consistently recovers the structure of a time-varying
network. This work complements previous empirical findings by providing sound
theoretical guarantees for the proposed estimation procedure. For completeness,
we include numerical simulations in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.2451</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.2451</id><created>2009-07-14</created><updated>2011-09-02</updated><authors><author><keyname>Gautier</keyname><forenames>Eric</forenames><affiliation>CREST, ENSAE</affiliation></author><author><keyname>Kitamura</keyname><forenames>Yuichi</forenames></author></authors><title>Nonparametric estimation in random coefficients binary choice models</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nous consid\'erons dans cet article des mod\`eles \`a choix binaires et
coefficients al\'eatoires. Le but est d'estimer de mani\`ere nonparam\'etrique
la densit\'e du coefficient al\'eatoire. Il s'agit d'un probl\`eme inverse mal
pos\'e caract\'eris\'e par une transformation int\'egrale. Un nouvel estimateur
de la densit\'e du coefficient al\'eatoire est propos\'e. Il est bas\'e sur les
d\'eveloppements en s\'eries de Fourier-Laplace sur la sph\`ere. Cette approche
permet une \'etude fine du probl\`eme d'identification mais aussi d'obtenir un
estimateur par injection ayant une expression explicite et ne n\'ecessitant
aucun optimisation num\'erique. Le nouvel estimateur est donc tr\`es facile \`a
obtenir num\'eriquement, tout en \'etant souple sur le traitement de
l'h\'et\'erog\'en\'eit\'e inobserv\'ee. Nous pr\'esentons des extensions parmi
lesquellesle traitement de coefficients non al\'eatoires et de mod\`eles avec
endog\'en\'eit\'e.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.2601</identifier>
 <datestamp>2012-02-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.2601</id><created>2009-07-15</created><authors><author><keyname>Said</keyname><forenames>Salem</forenames></author><author><keyname>Lageman</keyname><forenames>Christian</forenames></author><author><keyname>Bihan</keyname><forenames>Nicolas Le</forenames></author><author><keyname>Manton</keyname><forenames>Jonathan H.</forenames></author></authors><title>Decompounding on compact Lie groups</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>26 pages, 3 figures, 25 references</comments><journal-ref>IEEE Transactions on Information Theory, Vol. 56, Issue 6, pp.
  2766-2777, 2010</journal-ref><doi>10.1109/TIT.2010.2046216</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Noncommutative harmonic analysis is used to solve a nonparametric estimation
problem stated in terms of compound Poisson processes on compact Lie groups.
This problem of decompounding is a generalization of a similar classical
problem. The proposed solution is based on a char- acteristic function method.
The treated problem is important to recent models of the physical inverse
problem of multiple scattering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.2770</identifier>
 <datestamp>2011-04-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.2770</id><created>2009-07-16</created><updated>2011-04-14</updated><authors><author><keyname>Xu</keyname><forenames>Lizhen</forenames></author><author><keyname>Craiu</keyname><forenames>Radu V.</forenames></author><author><keyname>Sun</keyname><forenames>Lei</forenames></author></authors><title>Bayesian methods to overcome the winner's curse in genetic studies</title><categories>stat.AP stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOAS373 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS373</report-no><journal-ref>Annals of Applied Statistics 2011, Vol. 5, No. 1, 201-231</journal-ref><doi>10.1214/10-AOAS373</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parameter estimates for associated genetic variants, report ed in the initial
discovery samples, are often grossly inflated compared to the values observed
in the follow-up replication samples. This type of bias is a consequence of the
sequential procedure in which the estimated effect of an associated genetic
marker must first pass a stringent significance threshold. We propose a
hierarchical Bayes method in which a spike-and-slab prior is used to account
for the possibility that the significant test result may be due to chance. We
examine the robustness of the method using different priors corresponding to
different degrees of confidence in the testing results and propose a Bayesian
model averaging procedure to combine estimates produced by different models.
The Bayesian estimators yield smaller variance compared to the conditional
likelihood estimator and outperform the latter in studies with low power. We
investigate the performance of the method with simulations and applications to
four real data examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.3369</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.3369</id><created>2009-07-20</created><authors><author><keyname>Geller</keyname><forenames>Daryl</forenames></author><author><keyname>Lan</keyname><forenames>Xiaohong</forenames></author><author><keyname>Marinucci</keyname><forenames>Domenico</forenames></author></authors><title>Spin Needlets Spectral Estimation</title><categories>math.ST astro-ph.CO math.PR stat.TH</categories><msc-class>60G60, 62M15, 42C40, 33C55, 58J05</msc-class><journal-ref>Electronic Journal of Statistics, Vol. 3, (2009), pp.1497-1530</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the statistical analysis of random sections of a spin fibre
bundle over the sphere. These may be thought of as random fields that at each
point p in $S^2$ take as a value a curve (e.g. an ellipse) living in the
tangent plane at that point $T_{p}S^2$, rather than a number as in ordinary
situations. The analysis of such fields is strongly motivated by applications,
for instance polarization experiments in Cosmology. To investigate such fields,
spin needlets were recently introduced by Geller and Marinucci (2008) and
Geller et al. (2008). We consider the use of spin needlets for spin angular
power spectrum estimation, in the presence of noise and missing observations,
and we provide Central Limit Theorem results, in the high frequency sense; we
discuss also tests for bias and asymmetries with an asymptotic justification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.3454</identifier>
 <datestamp>2010-11-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.3454</id><created>2009-07-20</created><updated>2010-11-10</updated><authors><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Generalized density clustering</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS797 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS797</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 5, 2678-2722</journal-ref><doi>10.1214/10-AOS797</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study generalized density-based clustering in which sharply defined
clusters such as clusters on lower-dimensional manifolds are allowed. We show
that accurate clustering is possible even in high dimensions. We propose two
data-based methods for choosing the bandwidth and we study the stability
properties of density clusters. We show that a simple graph-based algorithm
successfully approximates the high density clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.3503</identifier>
 <datestamp>2013-05-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.3503</id><created>2009-07-20</created><updated>2013-05-03</updated><authors><author><keyname>Chernozhukov</keyname><forenames>Victor</forenames></author><author><keyname>Lee</keyname><forenames>Sokbae</forenames></author><author><keyname>Rosen</keyname><forenames>Adam M.</forenames></author></authors><title>Intersection Bounds: Estimation and Inference</title><categories>math.ST stat.TH</categories><msc-class>62G05, 62G15, 62G32</msc-class><journal-ref>Chernozhukov, V., Lee, S., and Rosen, A. M. (2013) Intersection
  Bounds: Estimation and Inference. Econometrica. Volume 81, Issue 2, pages
  667-737</journal-ref><doi>10.3982/ECTA8718</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a practical and novel method for inference on intersection bounds,
namely bounds defined by either the infimum or supremum of a parametric or
nonparametric function, or equivalently, the value of a linear programming
problem with a potentially infinite constraint set. We show that many bounds
characterizations in econometrics, for instance bounds on parameters under
conditional moment inequalities, can be formulated as intersection bounds. Our
approach is especially convenient for models comprised of a continuum of
inequalities that are separable in parameters, and also applies to models with
inequalities that are non-separable in parameters. Since analog estimators for
intersection bounds can be severely biased in finite samples, routinely
underestimating the size of the identified set, we also offer a
median-bias-corrected estimator of such bounds as a by-product of our
inferential procedures. We develop theory for large sample inference based on
the strong approximation of a sequence of series or kernel-based empirical
processes by a sequence of "penultimate" Gaussian processes. These penultimate
processes are generally not weakly convergent, and thus non-Donsker. Our
theoretical results establish that we can nonetheless perform asymptotically
valid inference based on these processes. Our construction also provides new
adaptive inequality/moment selection methods. We provide conditions for the use
of nonparametric kernel and series estimators, including a novel result that
establishes strong approximation for any general series estimator admitting
linearization, which may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.3521</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.3521</id><created>2009-07-20</created><updated>2009-12-08</updated><authors><author><keyname>Moustakides</keyname><forenames>George V.</forenames></author><author><keyname>Polunchenko</keyname><forenames>Aleksey S.</forenames></author><author><keyname>Tartakovsky</keyname><forenames>Alexander G.</forenames></author></authors><title>A Numerical Approach to Performance Analysis of Quickest Change-Point
  Detection Procedures</title><categories>stat.CO</categories><comments>32 pages, to appear in Statistica Sinica</comments><journal-ref>Statistica Sinica, vol. 21, no. 2, pp. 571--596, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the most popular sequential change detection rules such as CUSUM, EWMA,
and the Shiryaev-Roberts test, we develop integral equations and a concise
numerical method to compute a number of performance metrics, including average
detection delay and average time to false alarm. We pay special attention to
the Shiryaev-Roberts procedure and evaluate its performance for various
initialization strategies. Regarding the randomized initialization variant
proposed by Pollak, known to be asymptotically optimal of order-3, we offer a
means for numerically computing the quasi-stationary distribution of the
Shiryaev-Roberts statistic that is the distribution of the initializing random
variable, thus making this test applicable in practice. A significant
side-product of our computational technique is the observation that
deterministic initializations of the Shiryaev-Roberts procedure can also enjoy
the same order-3 optimality property as Pollak's randomized test and, after
careful selection, even uniformly outperform it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.3708</identifier>
 <datestamp>2010-04-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.3708</id><created>2009-07-21</created><updated>2010-04-20</updated><authors><author><keyname>Lancichinetti</keyname><forenames>Andrea</forenames></author><author><keyname>Radicchi</keyname><forenames>Filippo</forenames></author><author><keyname>Ramasco</keyname><forenames>Jose J.</forenames></author></authors><title>Statistical significance of communities in networks</title><categories>physics.soc-ph cond-mat.stat-mech stat.AP</categories><comments>9 pages, 8 figures, 2 tables. The software to calculate the C-score
  can be found at http://filrad.homelinux.org/cscore</comments><journal-ref>Phys. Rev. E 81, 046110 (2010)</journal-ref><doi>10.1103/PhysRevE.81.046110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nodes in real-world networks are usually organized in local modules. These
groups, called communities, are intuitively defined as sub-graphs with a larger
density of internal connections than of external links. In this work, we
introduce a new measure aimed at quantifying the statistical significance of
single communities. Extreme and Order Statistics are used to predict the
statistics associated with individual clusters in random graphs. These
distributions allows us to define one community significance as the probability
that a generic clustering algorithm finds such a group in a random graph. The
method is successfully applied in the case of real-world networks for the
evaluation of the significance of their communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.3837</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.3837</id><created>2009-07-22</created><updated>2012-11-09</updated><authors><author><keyname>Newton</keyname><forenames>Michael A.</forenames></author><author><keyname>Chung</keyname><forenames>Lisa M.</forenames></author></authors><title>Gamma-based clustering via ordered means with application to
  gene-expression analysis</title><categories>stat.ME math.ST stat.CO stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS805 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS805</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 6, 3217-3244</journal-ref><doi>10.1214/10-AOS805</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrete mixture models provide a well-known basis for effective clustering
algorithms, although technical challenges have limited their scope. In the
context of gene-expression data analysis, a model is presented that mixes over
a finite catalog of structures, each one representing equality and inequality
constraints among latent expected values. Computations depend on the
probability that independent gamma-distributed variables attain each of their
possible orderings. Each ordering event is equivalent to an event in
independent negative-binomial random variables, and this finding guides a
dynamic-programming calculation. The structuring of mixture-model components
according to constraints among latent means leads to strict concavity of the
mixture log likelihood. In addition to its beneficial numerical properties, the
clustering method shows promising results in an empirical study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.4000</identifier>
 <datestamp>2011-04-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.4000</id><created>2009-07-23</created><updated>2011-04-05</updated><authors><author><keyname>Goeyvaerts</keyname><forenames>Nele</forenames></author><author><keyname>Hens</keyname><forenames>Niel</forenames></author><author><keyname>Ogunjimi</keyname><forenames>Benson</forenames></author><author><keyname>Aerts</keyname><forenames>Marc</forenames></author><author><keyname>Shkedy</keyname><forenames>Ziv</forenames></author><author><keyname>Van Damme</keyname><forenames>Pierre</forenames></author><author><keyname>Beutels</keyname><forenames>Philippe</forenames></author></authors><title>Estimating infectious disease parameters from data on social contacts
  and serological status</title><categories>stat.AP</categories><comments>25 pages, 6 figures</comments><journal-ref>Journal of the Royal Statistical Society, Series C, 2010, Vol. 59,
  Part 2, p. 255-277, www3.interscience.wiley.com/journal/117997424/home</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In dynamic models of infectious disease transmission, typically various
mixing patterns are imposed on the so-called Who-Acquires-Infection-From-Whom
matrix (WAIFW). These imposed mixing patterns are based on prior knowledge of
age-related social mixing behavior rather than observations. Alternatively, one
can assume that transmission rates for infections transmitted predominantly
through non-sexual social contacts, are proportional to rates of conversational
contact which can be estimated from a contact survey. In general, however,
contacts reported in social contact surveys are proxies of those events by
which transmission may occur and there may exist age-specific characteristics
related to susceptibility and infectiousness which are not captured by the
contact rates. Therefore, in this paper, transmission is modeled as the product
of two age-specific variables: the age-specific contact rate and an
age-specific proportionality factor, which entails an improvement of fit for
the seroprevalence of the varicella-zoster virus (VZV) in Belgium. Furthermore,
we address the impact on the estimation of the basic reproduction number, using
non-parametric bootstrapping to account for different sources of variability
and using multi-model inference to deal with model selection uncertainty. The
proposed method makes it possible to obtain important information on
transmission dynamics that cannot be inferred from approaches traditionally
applied hitherto.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.4160</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.4160</id><created>2009-07-24</created><updated>2010-05-04</updated><authors><author><keyname>Kontoyiannis</keyname><forenames>Ioannis</forenames></author><author><keyname>Dellaportas</keyname><forenames>Petros</forenames></author></authors><title>Notes on Using Control Variates for Estimation with Reversible MCMC
  Samplers</title><categories>stat.CO math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general methodology is presented for the construction and effective use of
control variates for reversible MCMC samplers. The values of the coefficients
of the optimal linear combination of the control variates are computed, and
adaptive, consistent MCMC estimators are derived for these optimal
coefficients. All methodological and asymptotic arguments are rigorously
justified. Numerous MCMC simulation examples from Bayesian inference
applications demonstrate that the resulting variance reduction can be quite
dramatic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.4383</identifier>
 <datestamp>2010-10-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.4383</id><created>2009-07-24</created><updated>2010-10-28</updated><authors><author><keyname>Hao</keyname><forenames>Jiangang</forenames></author><author><keyname>Koester</keyname><forenames>Benjamin P.</forenames></author><author><keyname>Mckay</keyname><forenames>Timothy A.</forenames></author><author><keyname>Rykoff</keyname><forenames>Eli S.</forenames></author><author><keyname>Rozo</keyname><forenames>Eduardo</forenames></author><author><keyname>Evrard</keyname><forenames>August</forenames></author><author><keyname>Annis</keyname><forenames>James</forenames></author><author><keyname>Becker</keyname><forenames>Matthew</forenames></author><author><keyname>Busha</keyname><forenames>Michael</forenames></author><author><keyname>Gerdes</keyname><forenames>David</forenames></author><author><keyname>Johnston</keyname><forenames>David E.</forenames></author><author><keyname>Sheldon</keyname><forenames>Erin</forenames></author><author><keyname>Wechsler</keyname><forenames>Risa H.</forenames></author></authors><title>Precision Measurements of the Cluster Red Sequence using an Error
  Corrected Gaussian Mixture Model</title><categories>astro-ph.CO astro-ph.GA stat.CO</categories><comments>33 pages, 14 Figures; A typo in Eq.A11 is fixed. The C++/Python codes
  for ECGMM can be downloaded from:
  https://sites.google.com/site/jiangangecgmm/</comments><report-no>FERMILAB-PUB-09-339-A</report-no><journal-ref>Astrophys.J.702:745-758,2009</journal-ref><doi>10.1088/0004-637X/702/1/745</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The red sequence is an important feature of galaxy clusters and plays a
crucial role in optical cluster detection. Measurement of the slope and scatter
of the red sequence are affected both by selection of red sequence galaxies and
measurement errors. In this paper, we describe a new error corrected Gaussian
Mixture Model for red sequence galaxy identification. Using this technique, we
can remove the effects of measurement error and extract unbiased information
about the intrinsic properties of the red sequence. We use this method to
select red sequence galaxies in each of the 13,823 clusters in the maxBCG
catalog, and measure the red sequence ridgeline location and scatter of each.
These measurements provide precise constraints on the variation of the average
red galaxy populations in the observed frame with redshift. We find that the
scatter of the red sequence ridgeline increases mildly with redshift, and that
the slope decreases with redshift. We also observe that the slope does not
strongly depend on cluster richness. Using similar methods, we show that this
behavior is mirrored in a spectroscopic sample of field galaxies, further
emphasizing that ridgeline properties are independent of environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.4728</identifier>
 <datestamp>2011-02-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.4728</id><created>2009-07-27</created><authors><author><keyname>Arlot</keyname><forenames>Sylvain</forenames><affiliation>LIENS</affiliation></author><author><keyname>Celisse</keyname><forenames>Alain</forenames><affiliation>MIA</affiliation></author></authors><title>A survey of cross-validation procedures for model selection</title><categories>math.ST stat.AP stat.ME stat.ML stat.TH</categories><proxy>ccsd hal-00407906</proxy><msc-class>62G08, 62G05, 62G09</msc-class><journal-ref>Statistics Surveys 4 (2010) 40--79</journal-ref><doi>10.1214/09-SS054</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Used to estimate the risk of an estimator or to perform model selection,
cross-validation is a widespread strategy because of its simplicity and its
apparent universality. Many results exist on the model selection performances
of cross-validation procedures. This survey intends to relate these results to
the most recent advances of model selection theory, with a particular emphasis
on distinguishing empirical statements from rigorous theoretical results. As a
conclusion, guidelines are provided for choosing the best cross-validation
procedure according to the particular features of the problem in hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.5024</identifier>
 <datestamp>2011-04-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.5024</id><created>2009-07-28</created><updated>2010-10-14</updated><authors><author><keyname>Kazakopoulos</keyname><forenames>P.</forenames></author><author><keyname>Mertikopoulos</keyname><forenames>P.</forenames></author><author><keyname>Moustakas</keyname><forenames>A. L.</forenames></author><author><keyname>Caire</keyname><forenames>G.</forenames></author></authors><title>Living at the Edge: A Large Deviations Approach to the Outage MIMO
  Capacity</title><categories>cs.IT cond-mat.stat-mech math.IT stat.AP</categories><comments>Accepted for publication, IEEE Transactions on Information Theory
  (2010). Part of this work appears in the Proc. IEEE Information Theory
  Workshop, June 2009, Volos, Greece</comments><journal-ref>IEEE Transactions on Information Theory, vol. 57, no 4, p. 1984,
  April 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using a large deviations approach we calculate the probability distribution
of the mutual information of MIMO channels in the limit of large antenna
numbers. In contrast to previous methods that only focused at the distribution
close to its mean (thus obtaining an asymptotically Gaussian distribution), we
calculate the full distribution, including its tails which strongly deviate
from the Gaussian behavior near the mean. The resulting distribution
interpolates seamlessly between the Gaussian approximation for rates $R$ close
to the ergodic value of the mutual information and the approach of Zheng and
Tse for large signal to noise ratios $\rho$. This calculation provides us with
a tool to obtain outage probabilities analytically at any point in the $(R,
\rho, N)$ parameter space, as long as the number of antennas $N$ is not too
small. In addition, this method also yields the probability distribution of
eigenvalues constrained in the subspace where the mutual information per
antenna is fixed to $R$ for a given $\rho$. Quite remarkably, this eigenvalue
density is of the form of the Marcenko-Pastur distribution with square-root
singularities, and it depends on the values of $R$ and $\rho$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.5151</identifier>
 <datestamp>2010-07-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.5151</id><created>2009-07-29</created><updated>2010-05-31</updated><authors><author><keyname>Roueff</keyname><forenames>Franois</forenames><affiliation>LTCI</affiliation></author><author><keyname>Von Sachs</keyname><forenames>Rainer</forenames><affiliation>STAT</affiliation></author></authors><title>Locally stationary long memory estimation</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There exists a wide literature on modelling strongly dependent time series
using a longmemory parameter d, including more recent work on semiparametric
wavelet estimation. As a generalization of these latter approaches, in this
work we allow the long-memory parameter d to be varying over time. We embed our
approach into the framework of locally stationary processes. We show weak
consistency and a central limit theorem for our log-regression wavelet
estimator of the time-dependent d in a Gaussian context. Both simulations and a
real data example complete our work on providing a fairly general approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.5236</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.5236</id><created>2009-07-30</created><updated>2010-06-02</updated><authors><author><keyname>Ghosh</keyname><forenames>Souvik</forenames></author><author><keyname>Resnick</keyname><forenames>Sidney I</forenames></author></authors><title>A Discussion on Mean Excess Plots</title><categories>math.PR math.ST stat.TH</categories><comments>26 pages, 9 figures</comments><msc-class>60F05, 60F17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A widely used tool in the study of risk, insurance and extreme values is the
mean excess plot. One use is for validating a generalized Pareto model for the
excess distribution. This paper investigates some theoretical and practical
aspects of the use of the mean excess plot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0907.5309</identifier>
 <datestamp>2010-01-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0907.5309</id><created>2009-07-30</created><updated>2010-01-29</updated><authors><author><keyname>Sriperumbudur</keyname><forenames>Bharath K.</forenames></author><author><keyname>Gretton</keyname><forenames>Arthur</forenames></author><author><keyname>Fukumizu</keyname><forenames>Kenji</forenames></author><author><keyname>Schlkopf</keyname><forenames>Bernhard</forenames></author><author><keyname>Lanckriet</keyname><forenames>Gert R. G.</forenames></author></authors><title>Hilbert space embeddings and metrics on probability measures</title><categories>stat.ML math.ST stat.TH</categories><comments>48 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Hilbert space embedding for probability measures has recently been
proposed, with applications including dimensionality reduction, homogeneity
testing, and independence testing. This embedding represents any probability
measure as a mean element in a reproducing kernel Hilbert space (RKHS). A
pseudometric on the space of probability measures can be defined as the
distance between distribution embeddings: we denote this as $\gamma_k$, indexed
by the kernel function $k$ that defines the inner product in the RKHS.
  We present three theoretical properties of $\gamma_k$. First, we consider the
question of determining the conditions on the kernel $k$ for which $\gamma_k$
is a metric: such $k$ are denoted {\em characteristic kernels}. Unlike
pseudometrics, a metric is zero only when two distributions coincide, thus
ensuring the RKHS embedding maps all distributions uniquely (i.e., the
embedding is injective). While previously published conditions may apply only
in restricted circumstances (e.g. on compact domains), and are difficult to
check, our conditions are straightforward and intuitive: bounded continuous
strictly positive definite kernels are characteristic. Alternatively, if a
bounded continuous kernel is translation-invariant on $\bb{R}^d$, then it is
characteristic if and only if the support of its Fourier transform is the
entire $\bb{R}^d$. Second, we show that there exist distinct distributions that
are arbitrarily close in $\gamma_k$. Third, to understand the nature of the
topology induced by $\gamma_k$, we relate $\gamma_k$ to other popular metrics
on probability measures, and present conditions on the kernel $k$ under which
$\gamma_k$ metrizes the weak topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.0050</identifier>
 <datestamp>2010-02-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.0050</id><created>2009-08-01</created><updated>2010-02-11</updated><authors><author><keyname>Mairal</keyname><forenames>Julien</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Ponce</keyname><forenames>Jean</forenames><affiliation>INRIA Rocquencourt, LIENS</affiliation></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Online Learning for Matrix Factorization and Sparse Coding</title><categories>stat.ML cs.LG math.OC</categories><comments>revised version</comments><proxy>ccsd inria-00408716</proxy><journal-ref>Journal of Machine Learning Research 11 (2010) 19--60</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse coding--that is, modelling data vectors as sparse linear combinations
of basis elements--is widely used in machine learning, neuroscience, signal
processing, and statistics. This paper focuses on the large-scale matrix
factorization problem that consists of learning the basis set, adapting it to
specific data. Variations of this problem include dictionary learning in signal
processing, non-negative matrix factorization and sparse principal component
analysis. In this paper, we propose to address these tasks with a new online
optimization algorithm, based on stochastic approximations, which scales up
gracefully to large datasets with millions of training samples, and extends
naturally to various matrix factorization formulations, making it suitable for
a wide range of learning problems. A proof of convergence is presented, along
with experiments with natural images and genomic data demonstrating that it
leads to state-of-the-art performance in terms of speed and optimization for
both small and large datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.0067</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.0067</id><created>2009-08-01</created><updated>2012-11-12</updated><authors><author><keyname>Wood</keyname><forenames>Michael</forenames></author></authors><title>Making statistical methods in management research more useful: some
  suggestions from a case study</title><categories>stat.AP stat.ME</categories><comments>27 pages, 2 figures. New version has amended title, revised abstract,
  and the rest of the paper has been simplified</comments><journal-ref>Slightly revised version published in Sage Open, vol 3, no 1, 2013</journal-ref><doi>10.1177/2158244013476873</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I present a critique of the methods used in a typical paper. This leads to
three broad conclusions about the conventional use of statistical methods.
First, results are often reported in an unnecessarily obscure manner. Second,
the null hypothesis testing paradigm is deeply flawed: estimating the size of
effects and citing confidence intervals or levels is usually better. Third,
there are several issues, independent of the particular statistical concepts
employed, which limit the value of any statistical approach: e.g. difficulties
of generalizing to different contexts, and the weakness of some research in
terms of the size of the effects found. The first two of these are easily
remedied: I illustrate some of the possibilities by re-analyzing the data from
the case study article. The third means that in some contexts a statistical
approach may not be worthwhile. My case study is a management paper, but
similar problems arise in other social sciences. Keywords: Confidence,
Hypothesis testing, Null hypothesis significance tests, Philosophy of
statistics, Statistical methods, User-friendliness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.0128</identifier>
 <datestamp>2011-03-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.0128</id><created>2009-08-02</created><updated>2011-02-28</updated><authors><author><keyname>Hengartner</keyname><forenames>Nicolas</forenames><affiliation>LANL</affiliation></author><author><keyname>Matzner-Lber</keyname><forenames>Eric</forenames><affiliation>IRMAR</affiliation></author><author><keyname>Rouvire</keyname><forenames>Laurent</forenames><affiliation>IRMAR, CREST</affiliation></author><author><keyname>Burr</keyname><forenames>Thomas</forenames><affiliation>LANL</affiliation></author></authors><title>Multiplicative Bias Corrected Nonparametric Smoothers</title><categories>math.ST stat.TH</categories><comments>29 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a multiplicative bias reduction estimator for
nonparametric regression. The approach consists to apply a multiplicative bias
correction to an oversmooth pilot estimator. In Burr et al. [2010], this method
has been tested to estimate energy spectra. For such data set, it was observed
that the method allows to decrease bias with negligible increase in variance.
In this paper, we study the asymptotic properties of the resulting estimate and
prove that this estimate has zero asymptotic bias and the same asymptotic
variance as the local linear estimate. Simulations show that our asymptotic
results are available for modest sample sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.0433</identifier>
 <datestamp>2011-01-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.0433</id><created>2009-08-04</created><updated>2010-06-28</updated><authors><author><keyname>Nickl</keyname><forenames>Richard</forenames></author><author><keyname>Ptscher</keyname><forenames>Benedikt M.</forenames></author></authors><title>Efficient Simulation-Based Minimum Distance Estimation and Indirect
  Inference</title><categories>math.ST stat.TH</categories><comments>Minor revision, some references and remarks added</comments><journal-ref>Mathematical Methods of Statistics 19 (2010), 327-364</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a random sample from a parametric model, we show how indirect inference
estimators based on appropriate nonparametric density estimators (i.e.,
simulation-based minimum distance estimators) can be constructed that, under
mild assumptions, are asymptotically normal with variance-covarince matrix
equal to the Cramer-Rao bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.0618</identifier>
 <datestamp>2012-11-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.0618</id><created>2009-08-05</created><updated>2012-11-27</updated><authors><author><keyname>Lian</keyname><forenames>Heng</forenames></author></authors><title>Functional Partial Linear Model</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When predicting scalar responses in the situation where the explanatory
variables are functions, it is sometimes the case that some functional
variables are related to responses linearly while other variables have more
complicated relationships with the responses. In this paper, we propose a new
semi-parametric model to take advantage of both parametric and nonparametric
functional modeling. Asymptotic properties of the proposed estimators are
established and finite sample behavior is investigated through a small
simulation experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.1144</identifier>
 <datestamp>2010-11-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.1144</id><created>2009-08-07</created><updated>2010-11-12</updated><authors><author><keyname>Wilson</keyname><forenames>Melanie A.</forenames></author><author><keyname>Iversen</keyname><forenames>Edwin S.</forenames></author><author><keyname>Clyde</keyname><forenames>Merlise A.</forenames></author><author><keyname>Schmidler</keyname><forenames>Scott C.</forenames></author><author><keyname>Schildkraut</keyname><forenames>Joellen M.</forenames></author></authors><title>Bayesian model search and multilevel inference for SNP association
  studies</title><categories>stat.AP</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOAS322 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS322</report-no><journal-ref>Annals of Applied Statistics 2010, Vol. 4, No. 3, 1342-1364</journal-ref><doi>10.1214/09-AOAS322</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Technological advances in genotyping have given rise to hypothesis-based
association studies of increasing scope. As a result, the scientific hypotheses
addressed by these studies have become more complex and more difficult to
address using existing analytic methodologies. Obstacles to analysis include
inference in the face of multiple comparisons, complications arising from
correlations among the SNPs (single nucleotide polymorphisms), choice of their
genetic parametrization and missing data. In this paper we present an efficient
Bayesian model search strategy that searches over the space of genetic markers
and their genetic parametrization. The resulting method for Multilevel
Inference of SNP Associations, MISA, allows computation of multilevel posterior
probabilities and Bayes factors at the global, gene and SNP level, with the
prior distribution on SNP inclusion in the model providing an intrinsic
multiplicity correction. We use simulated data sets to characterize MISA's
statistical power, and show that MISA has higher power to detect association
than standard procedures. Using data from the North Carolina Ovarian Cancer
Study (NCOCS), MISA identifies variants that were not identified by standard
methods and have been externally ``validated'' in independent studies. We
examine sensitivity of the NCOCS results to prior choice and method for
imputing missing data. MISA is available in an R package on CRAN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.1518</identifier>
 <datestamp>2011-04-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.1518</id><created>2009-08-11</created><updated>2011-04-13</updated><authors><author><keyname>Rodriguez</keyname><forenames>D. E.</forenames></author><author><keyname>Bab</keyname><forenames>M. A.</forenames></author><author><keyname>Albano</keyname><forenames>E. V.</forenames></author></authors><title>Study of the Nonequilibrium Critical Quenching and Annealing Dynamics
  for the Long-Range Ising Model</title><categories>cond-mat.stat-mech stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extensive Monte Carlo simulations are employed in order to study the dynamic
critical behavior of the one-dimensional Ising magnet, with algebraically
decaying long-range interactions of the form $\frac{1}{r^{d+\sigma}}$, with
$\sigma=0.75$. The critical temperature, as well as the critical exponents, is
evaluated from the power-law behavior of suitable physical observables when the
system is quenched from uncorrelated states, corresponding to infinite
temperature, to the critical point. These results are compared with those
obtained from the dynamic evolution of the system when it is suddenly annealed
at the critical point from the ordered state. Also, the critical temperature in
the infinite interaction limit is obtained by means of a finite-range scaling
analysis of data measured with different cutoffs of the interaction range. All
the estimated static critical exponents ($\gamma /\nu $, $\beta /\nu $, and
$1/\nu $) are in good agreement with Renormalization Group (RG) predictions and
previously reported numerical data obtained under equilibrium conditions. It is
found that the dynamic exponent $z$ is different for quenching and annealing
experiments, most likely due to the influence of the Kosterlitz-Thouless
transition occurring at relatively similar algebraic decay of the interactions
with $\sigma =1$. However, for annealing experiments the measured exponent $z$
is close to the RG predictions. On the other hand, the relevant exponents of
the dynamic behavior ($z$ and $\theta$) are slightly different than the RG
predictions, most likely due to the fact that they may depend on the especific
dynamics used (Metropolis in the present paper).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.1530</identifier>
 <datestamp>2010-01-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.1530</id><created>2009-08-11</created><updated>2010-01-06</updated><authors><author><keyname>Brouwer</keyname><forenames>Andries E.</forenames></author><author><keyname>Draisma</keyname><forenames>Jan</forenames></author></authors><title>Equivariant Groebner bases and the Gaussian two-factor model</title><categories>math.AC math.ST stat.TH</categories><comments>11 pages, improved exposition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exploiting symmetry in Groebner basis computations is difficult when the
symmetry takes the form of a group acting by automorphisms on monomials in
finitely many variables. This is largely due to the fact that the group
elements, being invertible, cannot preserve a term order. By contrast, inspired
by work of Aschenbrenner and Hillar, we introduce the concept of equivariant
Groebner basis in a setting where a_monoid_ acts by_homomorphisms_ on monomials
in potentially infinitely many variables. We require that the action be
compatible with a term order, and under some further assumptions derive a
Buchberger-type algorithm for computing equivariant Groebner bases. Using this
algorithm and the monoid of strictly increasing functions N -&gt; N we prove that
the kernel of the ring homomorphism R[y_{ij} | i,j in N, i &gt; j] -&gt; R[s_i,t_i |
i in N], y_{ij} -&gt; s_i s_j + t_i t_j is generated by two types of polynomials:
off-diagonal 3x3-minors and pentads. This confirms a conjecture by Drton,
Sturmfels, and Sullivant on the Gaussian two-factor model from algebraic
statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.1701</identifier>
 <datestamp>2011-01-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.1701</id><created>2009-08-12</created><updated>2011-01-13</updated><authors><author><keyname>Sheena</keyname><forenames>Yo</forenames></author><author><keyname>Takemura</keyname><forenames>Akimichi</forenames></author></authors><title>Admissible Estimator of the Eigenvalues of Variance-Covariance Matrix
  for Multivariate Normal Distributions--Detailed Proof--</title><categories>math.ST stat.TH</categories><comments>34 pages, 1 figure</comments><msc-class>62C15, 62F10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An admissible estimator of the eigenvalues of the variance-covariance matrix
is given for multivariate normal distributions with respect to the
scale-invariant squared error loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.1733</identifier>
 <datestamp>2010-01-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.1733</id><created>2009-08-12</created><updated>2010-01-08</updated><authors><author><keyname>Fill</keyname><forenames>James Allen</forenames></author><author><keyname>Huber</keyname><forenames>Mark</forenames></author></authors><title>Perfect simulation of Vervaat perpetuities</title><categories>math.PR math.ST stat.TH</categories><comments>14 pages, no figures</comments><msc-class>68U20;65C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use coupling into and from the past to sample perfectly in a simple and
provably fast fashion from the Vervaat family of perpetuities. The family
includes the Dickman distribution, which arises both in number theory and in
the analysis of the Quickselect algorithm, which was the motivation for our
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.1767</identifier>
 <datestamp>2011-03-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.1767</id><created>2009-08-12</created><updated>2011-03-09</updated><authors><author><keyname>Pea</keyname><forenames>Edsel A.</forenames></author><author><keyname>Habiger</keyname><forenames>Joshua D.</forenames></author><author><keyname>Wu</keyname><forenames>Wensong</forenames></author></authors><title>Power-enhanced multiple decision functions controlling family-wise error
  and false discovery rates</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS844 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS844</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 1, 556-583</journal-ref><doi>10.1214/10-AOS844</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improved procedures, in terms of smaller missed discovery rates (MDR), for
performing multiple hypotheses testing with weak and strong control of the
family-wise error rate (FWER) or the false discovery rate (FDR) are developed
and studied. The improvement over existing procedures such as the \v{S}id\'ak
procedure for FWER control and the Benjamini--Hochberg (BH) procedure for FDR
control is achieved by exploiting possible differences in the powers of the
individual tests. Results signal the need to take into account the powers of
the individual tests and to have multiple hypotheses decision functions which
are not limited to simply using the individual $p$-values, as is the case, for
example, with the \v{S}id\'ak, Bonferroni, or BH procedures. They also enhance
understanding of the role of the powers of individual tests, or more precisely
the receiver operating characteristic (ROC) functions of decision processes, in
the search for better multiple hypotheses testing procedures. A
decision-theoretic framework is utilized, and through auxiliary randomizers the
procedures could be used with discrete or mixed-type data or with rank-based
nonparametric tests. This is in contrast to existing $p$-value based procedures
whose theoretical validity is contingent on each of these $p$-value statistics
being stochastically equal to or greater than a standard uniform variable under
the null hypothesis. Proposed procedures are relevant in the analysis of
high-dimensional "large $M$, small $n$" data sets arising in the natural,
physical, medical, economic and social sciences, whose generation and creation
is accelerated by advances in high-throughput technology, notably, but not
limited to, microarray technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.1952</identifier>
 <datestamp>2011-06-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.1952</id><created>2009-08-13</created><updated>2011-06-13</updated><authors><author><keyname>Kerkyacharian</keyname><forenames>Grard</forenames></author><author><keyname>Ngoc</keyname><forenames>Thanh Mai Pham</forenames></author><author><keyname>Picard</keyname><forenames>Dominique</forenames></author></authors><title>Localized spherical deconvolution</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS858 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS858</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 2, 1042-1068</journal-ref><doi>10.1214/10-AOS858</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a new algorithm for the treatment of the deconvolution problem on
the sphere which combines the traditional SVD inversion with an appropriate
thresholding technique in a well chosen new basis. We establish upper bounds
for the behavior of our procedure for any $\mathbb {L}_p$ loss. It is important
to emphasize the adaptation properties of our procedures with respect to the
regularity (sparsity) of the object to recover as well as to inhomogeneous
smoothness. We also perform a numerical study which proves that the procedure
shows very promising properties in practice as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.2056</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.2056</id><created>2009-08-14</created><authors><author><keyname>Peres</keyname><forenames>Yuval</forenames></author><author><keyname>Roch</keyname><forenames>Sebastien</forenames></author></authors><title>Reconstruction on Trees: Exponential Moment Bounds for Linear Estimators</title><categories>math.PR cs.DS math.ST q-bio.PE stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a Markov chain $(\xi_v)_{v \in V} \in [k]^V$ on the infinite $b$-ary
tree $T = (V,E)$ with irreducible edge transition matrix $M$, where $b \geq 2$,
$k \geq 2$ and $[k] = \{1,...,k\}$. We denote by $L_n$ the level-$n$ vertices
of $T$. Assume $M$ has a real second-largest (in absolute value) eigenvalue
$\lambda$ with corresponding real eigenvector $\nu \neq 0$. Letting $\sigma_v =
\nu_{\xi_v}$, we consider the following root-state estimator, which was
introduced by Mossel and Peres (2003) in the context of the "recontruction
problem" on trees: \begin{equation*} S_n = (b\lambda)^{-n} \sum_{x\in L_n}
\sigma_x. \end{equation*} As noted by Mossel and Peres, when $b\lambda^2 &gt; 1$
(the so-called Kesten-Stigum reconstruction phase) the quantity $S_n$ has
uniformly bounded variance. Here, we give bounds on the moment-generating
functions of $S_n$ and $S_n^2$ when $b\lambda^2 &gt; 1$. Our results have
implications for the inference of evolutionary trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.2061</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.2061</id><created>2009-08-14</created><authors><author><keyname>Roch</keyname><forenames>Sebastien</forenames></author></authors><title>Sequence-Length Requirement of Distance-Based Phylogeny Reconstruction:
  Breaking the Polynomial Barrier</title><categories>math.PR cs.CE cs.DS math.ST q-bio.PE q-bio.QM stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new distance-based phylogeny reconstruction technique which
provably achieves, at sufficiently short branch lengths, a polylogarithmic
sequence-length requirement -- improving significantly over previous polynomial
bounds for distance-based methods. The technique is based on an averaging
procedure that implicitly reconstructs ancestral sequences.
  In the same token, we extend previous results on phase transitions in
phylogeny reconstruction to general time-reversible models. More precisely, we
show that in the so-called Kesten-Stigum zone (roughly, a region of the
parameter space where ancestral sequences are well approximated by ``linear
combinations'' of the observed sequences) sequences of length $\poly(\log n)$
suffice for reconstruction when branch lengths are discretized. Here $n$ is the
number of extant species.
  Our results challenge, to some extent, the conventional wisdom that estimates
of evolutionary distances alone carry significantly less information about
phylogenies than full sequence datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.2098</identifier>
 <datestamp>2011-02-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.2098</id><created>2009-08-14</created><authors><author><keyname>Latuszynski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Niemiro</keyname><forenames>Wojciech</forenames></author></authors><title>Rigorous confidence bounds for MCMC under a geometric drift condition</title><categories>stat.ME stat.CO</categories><report-no>CRiSM research report 09</report-no><journal-ref>Journal of Complexity, 27(1), pp. 23-38, (2011)</journal-ref><doi>10.1016/j.jco.2010.07.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We assume a drift condition towards a small set and bound the mean square
error of estimators obtained by taking averages along a single trajectory of a
Markov chain Monte Carlo algorithm. We use these bounds to construct
fixed-width nonasymptotic confidence intervals. For a possibly unbounded
function $f:\stany \to R,$ let $I=\int_{\stany} f(x) \pi(x) dx$ be the value of
interest and $\hat{I}_{t,n}=(1/n)\sum_{i=t}^{t+n-1}f(X_i)$ its MCMC estimate.
Precisely, we derive lower bounds for the length of the trajectory $n$ and
burn-in time $t$ which ensure that $$P(|\hat{I}_{t,n}-I|\leq \varepsilon)\geq
1-\alpha.$$ The bounds depend only and explicitly on drift parameters, on the
$V-$norm of $f,$ where $V$ is the drift function and on precision and
confidence parameters $\varepsilon, \alpha.$ Next we analyse an MCMC estimator
based on the median of multiple shorter runs that allows for sharper bounds for
the required total simulation cost. In particular the methodology can be
applied for computing Bayesian estimators in practically relevant models. We
illustrate our bounds numerically in a simple example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.2359</identifier>
 <datestamp>2011-02-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.2359</id><created>2009-08-17</created><updated>2011-02-15</updated><authors><author><keyname>Capp</keyname><forenames>Olivier</forenames><affiliation>LTCI</affiliation></author></authors><title>Online EM Algorithm for Hidden Markov Models</title><categories>stat.CO stat.ML</categories><comments>Revised version, to appear in J. Comput. Graph. Statist</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online (also called "recursive" or "adaptive") estimation of fixed model
parameters in hidden Markov models is a topic of much interest in times series
modelling. In this work, we propose an online parameter estimation algorithm
that combines two key ideas. The first one, which is deeply rooted in the
Expectation-Maximization (EM) methodology consists in reparameterizing the
problem using complete-data sufficient statistics. The second ingredient
consists in exploiting a purely recursive form of smoothing in HMMs based on an
auxiliary recursion. Although the proposed online EM algorithm resembles a
classical stochastic approximation (or Robbins-Monro) algorithm, it is
sufficiently different to resist conventional analysis of convergence. We thus
provide limited results which identify the potential limiting points of the
recursion as well as the large-sample behavior of the quantities involved in
the algorithm. The performance of the proposed algorithm is numerically
evaluated through simulations in the case of a noisily observed Markov chain.
In this case, the algorithm reaches estimation results that are comparable to
that of the maximum likelihood estimator for large sample sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.2372</identifier>
 <datestamp>2012-07-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.2372</id><created>2009-08-17</created><updated>2012-07-03</updated><authors><author><keyname>Guillotte</keyname><forenames>Simon</forenames></author><author><keyname>Perron</keyname><forenames>Franois</forenames></author></authors><title>Bayesian estimation of a bivariate copula using the Jeffreys prior</title><categories>stat.ME math.ST stat.CO stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ345 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ345</report-no><journal-ref>Bernoulli 2012, Vol. 18, No. 2, 496-519</journal-ref><doi>10.3150/10-BEJ345</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A bivariate distribution with continuous margins can be uniquely decomposed
via a copula and its marginal distributions. We consider the problem of
estimating the copula function and adopt a Bayesian approach. On the space of
copula functions, we construct a finite-dimensional approximation subspace that
is parametrized by a doubly stochastic matrix. A major problem here is the
selection of a prior distribution on the space of doubly stochastic matrices
also known as the Birkhoff polytope. The main contributions of this paper are
the derivation of a simple formula for the Jeffreys prior and showing that it
is proper. It is known in the literature that for a complex problem like the
one treated here, the above results are difficult to obtain. The Bayes
estimator resulting from the Jeffreys prior is then evaluated numerically via
Markov chain Monte Carlo methodology. A rather extensive simulation experiment
is carried out. In many cases, the results favour the Bayes estimator over
frequentist estimators such as the standard kernel estimator and Deheuvels'
estimator in terms of mean integrated squared error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.2409</identifier>
 <datestamp>2010-07-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.2409</id><created>2009-08-17</created><authors><author><keyname>Lee</keyname><forenames>Ann B.</forenames></author><author><keyname>Luca</keyname><forenames>Diana</forenames></author><author><keyname>Roeder</keyname><forenames>Kathryn</forenames></author></authors><title>A Spectral Graph Approach to Discovering Genetic Ancestry</title><categories>stat.AP</categories><comments>6 figures</comments><journal-ref>Annals of Applied Statistics, 4(1), 179-201, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mapping human genetic variation is fundamentally interesting in fields such
as anthropology and forensic inference. At the same time patterns of genetic
diversity confound efforts to determine the genetic basis of complex disease.
Due to technological advances it is now possible to measure hundreds of
thousands of genetic variants per individual across the genome. Principal
component analysis (PCA) is routinely used to summarize the genetic similarity
between subjects. The eigenvectors are interpreted as dimensions of ancestry.
We build on this idea using a spectral graph approach. In the process we draw
on connections between multidimensional scaling and spectral kernel methods.
Our approach, based on a spectral embedding derived from the normalized
Laplacian of a graph, can produce more meaningful delineation of ancestry than
by using PCA. The method is stable to outliers and can more easily incorporate
different similarity measures of genetic data than PCA. We illustrate a new
algorithm for genetic clustering and association analysis on a large,
genetically heterogeneous sample.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.2503</identifier>
 <datestamp>2010-06-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.2503</id><created>2009-08-18</created><updated>2010-05-31</updated><authors><author><keyname>Biau</keyname><forenames>Grard</forenames><affiliation>LSTA, PMA</affiliation></author><author><keyname>Patra</keyname><forenames>Benot</forenames><affiliation>LSTA</affiliation></author></authors><title>Sequential Quantile Prediction of Time Series</title><categories>stat.ME math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by a broad range of potential applications, we address the quantile
prediction problem of real-valued time series. We present a sequential quantile
forecasting model based on the combination of a set of elementary nearest
neighbor-type predictors called "experts" and show its consistency under a
minimum of conditions. Our approach builds on the methodology developed in
recent years for prediction of individual sequences and exploits the quantile
structure as a minimizer of the so-called pinball loss function. We perform an
in-depth analysis of real-world data sets and show that this nonparametric
strategy generally outperforms standard quantile prediction methods
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.2514</identifier>
 <datestamp>2012-05-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.2514</id><created>2009-08-18</created><updated>2012-05-09</updated><authors><author><keyname>Kerkyacharian</keyname><forenames>Gerard</forenames><affiliation>LPMA</affiliation></author><author><keyname>Pennec</keyname><forenames>Erwan Le</forenames><affiliation>LPMA, INRIA Saclay - Ile de France, LM-Orsay</affiliation></author><author><keyname>Picard</keyname><forenames>Dominique</forenames><affiliation>LPMA</affiliation></author></authors><title>Radon needlet thresholding</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><journal-ref>Bernoulli 18, 2 (2012) 391-433</journal-ref><doi>10.3150/10-BEJ340</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a new algorithm for the treatment of the noisy inversion of the
Radon transform using an appropriate thresholding technique adapted to a
well-chosen new localized basis. We establish minimax results and prove their
optimality. In particular, we prove that the procedures provided here are able
to attain minimax bounds for any $\mathbb {L}_p$ loss. It s important to notice
that most of the minimax bounds obtained here are new to our knowledge. It is
also important to emphasize the adaptation properties of our procedures with
respect to the regularity (sparsity) of the object to recover and to
inhomogeneous smoothness. We perform a numerical study that is of importance
since we especially have to discuss the cubature problems and propose an
averaging procedure that is mostly in the spirit of the cycle spinning
performed for periodic signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.2616</identifier>
 <datestamp>2012-02-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.2616</id><created>2009-08-18</created><updated>2010-06-14</updated><authors><author><keyname>Oron</keyname><forenames>Assaf P.</forenames></author><author><keyname>Azriel</keyname><forenames>David</forenames></author><author><keyname>Hoff</keyname><forenames>Peter D.</forenames></author></authors><title>Convergence of Nonparametric Long-Memory Phase I Designs</title><categories>stat.ME math.ST stat.AP stat.TH</categories><comments>New version uploaded. Lemma added that proves convergence of running
  point estimates based on martingale theory. Also simulation study added</comments><journal-ref>International Journal of Biostatistics, Volume 7, Issue 1, Article
  39, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine nonparametric dose-finding designs that use toxicity estimates
based on all available data at each dose allocation decision. We prove that one
such design family, called here "interval design", converges almost surely to
the maximum tolerated dose (MTD), if the MTD is the only dose level whose
toxicity rate falls within the pre-specified interval around the desired target
rate. Another nonparametric family, called "point design", has a positive
probability of not converging. In a numerical sensitivity study, a diverse
sample of dose-toxicity scenarios was randomly generated. On this sample, the
"interval design" convergence conditions are met far more often than the
conditions for one-parameter design convergence (the Shen-O'Quigley
conditions), suggesting that the interval-design conditions are less
restrictive. Implications of these theoretical and numerical results for
small-sample behavior of the designs, and for future research, are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.2644</identifier>
 <datestamp>2010-02-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.2644</id><created>2009-08-18</created><authors><author><keyname>Weinstein</keyname><forenames>Marvin</forenames></author><author><keyname>Horn</keyname><forenames>David</forenames></author></authors><title>Dynamic quantum clustering: a method for visual exploration of
  structures in data</title><categories>physics.data-an cs.DS hep-ex physics.comp-ph stat.ML</categories><comments>15 pages, 9 figures</comments><report-no>SLAC-PUB-13759</report-no><journal-ref>Phys.Rev.E80:066117,2009</journal-ref><doi>10.1103/PhysRevE.80.066117</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A given set of data-points in some feature space may be associated with a
Schrodinger equation whose potential is determined by the data. This is known
to lead to good clustering solutions. Here we extend this approach into a
full-fledged dynamical scheme using a time-dependent Schrodinger equation.
Moreover, we approximate this Hamiltonian formalism by a truncated calculation
within a set of Gaussian wave functions (coherent states) centered around the
original points. This allows for analytic evaluation of the time evolution of
all such states, opening up the possibility of exploration of relationships
among data-points through observation of varying dynamical-distances among
points and convergence of points into clusters. This formalism may be further
supplemented by preprocessing, such as dimensional reduction through singular
value decomposition or feature filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.2926</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.2926</id><created>2009-08-20</created><updated>2012-02-24</updated><authors><author><keyname>Oreshkin</keyname><forenames>Boris N.</forenames></author><author><keyname>Coates</keyname><forenames>Mark J.</forenames></author></authors><title>Analysis of error propagation in particle filters with approximation</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/11-AAP760 the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP760</report-no><journal-ref>Annals of Applied Probability 2011, Vol. 21, No. 6, 2343-2378</journal-ref><doi>10.1214/11-AAP760</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the impact of approximation steps that become necessary
when particle filters are implemented on resource-constrained platforms. We
consider particle filters that perform intermittent approximation, either by
subsampling the particles or by generating a parametric approximation. For such
algorithms, we derive time-uniform bounds on the weak-sense $L_p$ error and
present associated exponential inequalities. We motivate the theoretical
analysis by considering the leader node particle filter and present numerical
experiments exploring its performance and the relationship to the error bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.3047</identifier>
 <datestamp>2010-02-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.3047</id><created>2009-08-23</created><authors><author><keyname>Vongehr</keyname><forenames>Sascha</forenames></author><author><keyname>Tang</keyname><forenames>Shaochun</forenames></author><author><keyname>Meng</keyname><forenames>Xiangkang</forenames></author></authors><title>Collision statistics of clusters: From Poisson model to Poisson mixtures</title><categories>physics.chem-ph physics.atm-clus physics.data-an stat.AP</categories><comments>22 pages, 4 figures, to appear in Chin Phys B</comments><journal-ref>Chin. Phys. B Vol. 19, No. 2 (2010) 023602</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clusters traverse a gas and collide with gas particles. The gas particles are
adsorbed and the clusters become hosts. If the clusters are size selected, the
number of guests will be Poisson distributed. We review this by showcasing four
laboratory procedures that all rely on the validity of the Poisson model. The
effects of a statistical distribution of the cluster sizes in a beam of
clusters are discussed. We derive the average collision rates. Additionally, we
present Poisson mixture models that involve also standard deviations. We derive
the collision statistics for common size distributions of hosts and also for
some generalizations thereof. The models can be applied to large noble gas
clusters traversing doping gas. While outlining how to fit a generalized
Poisson to the statistics, we still find even these Poisson models to be often
insufficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.3121</identifier>
 <datestamp>2011-05-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.3121</id><created>2009-08-21</created><updated>2011-05-25</updated><authors><author><keyname>Gugushvili</keyname><forenames>Shota</forenames></author></authors><title>Nonparametric inference for discretely sampled L\'evy processes</title><categories>math.ST stat.TH</categories><comments>38 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a sample from a discretely observed L\'evy process $X=(X_t)_{t\geq 0}$
of the finite jump activity, the problem of nonparametric estimation of the
L\'evy density $\rho$ corresponding to the process $X$ is studied. An estimator
of $\rho$ is proposed that is based on a suitable inversion of the
L\'evy-Khintchine formula and a plug-in device. The main results of the paper
deal with upper risk bounds for estimation of $\rho$ over suitable classes of
L\'evy triplets. The corresponding lower bounds are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.3163</identifier>
 <datestamp>2010-04-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.3163</id><created>2009-08-21</created><updated>2010-04-06</updated><authors><author><keyname>Munk</keyname><forenames>Axel</forenames></author><author><keyname>Schmidt-Hieber</keyname><forenames>Johannes</forenames></author></authors><title>Nonparametric estimation of the volatility function in a high-frequency
  model corrupted by noise</title><categories>stat.ME math.ST stat.TH</categories><comments>5 figures, corrected references, minor changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the models Y_{i,n}=\int_0^{i/n}
\sigma(s)dW_s+\tau(i/n)\epsilon_{i,n}, and \tilde
Y_{i,n}=\sigma(i/n)W_{i/n}+\tau(i/n)\epsilon_{i,n}, i=1,...,n, where W_t
denotes a standard Brownian motion and \epsilon_{i,n} are centered i.i.d.
random variables with E(\epsilon_{i,n}^2)=1 and finite fourth moment.
Furthermore, \sigma and \tau are unknown deterministic functions and W_t and
(\epsilon_{1,n},...,\epsilon_{n,n}) are assumed to be independent processes.
Based on a spectral decomposition of the covariance structures we derive series
estimators for \sigma^2 and \tau^2 and investigate their rate of convergence of
the MISE in dependence of their smoothness. To this end specific basis
functions and their corresponding Sobolev ellipsoids are introduced and we show
that our estimators are optimal in minimax sense. Our work is motivated by
microstructure noise models. Our major finding is that the microstructure noise
\epsilon_{i,n} introduces an additionally degree of ill-posedness of 1/2;
irrespectively of the tail behavior of \epsilon_{i,n}. The method is
illustrated by a small numerical study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.3392</identifier>
 <datestamp>2010-10-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.3392</id><created>2009-08-24</created><authors><author><keyname>Comte</keyname><forenames>Fabienne</forenames><affiliation>MAP5</affiliation></author><author><keyname>Johannes</keyname><forenames>Jan</forenames></author></authors><title>Adaptive estimation in circular functional linear models</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00410729</proxy><journal-ref>Mathematical Methods of Statistics 19, 1 (2010) 42-63</journal-ref><doi>10.3103/S1066530710010035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the slope parameter in circular
functional linear regression, where scalar responses Y1,...,Yn are modeled in
dependence of 1-periodic, second order stationary random functions X1,...,Xn.
We consider an orthogonal series estimator of the slope function, by replacing
the first m theoretical coefficients of its development in the trigonometric
basis by adequate estimators. Wepropose a model selection procedure for m in a
set of admissible values, by defining a contrast function minimized by our
estimator and a theoretical penalty function; this first step assumes the
degree of ill posedness to be known. Then we generalize the procedure to a
random set of admissible m's and a random penalty function. The resulting
estimator is completely data driven and reaches automatically what is known to
be the optimal minimax rate of convergence, in term of a general weighted
L2-risk. This means that we provide adaptive estimators of both the slope
function and its derivatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.3437</identifier>
 <datestamp>2010-11-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.3437</id><created>2009-08-24</created><updated>2010-11-19</updated><authors><author><keyname>Addario-Berry</keyname><forenames>Louigi</forenames></author><author><keyname>Broutin</keyname><forenames>Nicolas</forenames></author><author><keyname>Devroye</keyname><forenames>Luc</forenames></author><author><keyname>Lugosi</keyname><forenames>Gbor</forenames></author></authors><title>On combinatorial testing problems</title><categories>math.ST math.CO stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS817 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS817</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 5, 3063-3092</journal-ref><doi>10.1214/10-AOS817</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a class of hypothesis testing problems in which, upon observing the
realization of an $n$-dimensional Gaussian vector, one has to decide whether
the vector was drawn from a standard normal distribution or, alternatively,
whether there is a subset of the components belonging to a certain given class
of sets whose elements have been ``contaminated,'' that is, have a mean
different from zero. We establish some general conditions under which testing
is possible and others under which testing is hopeless with a small risk. The
combinatorial and geometric structure of the class of sets is shown to play a
crucial role. The bounds are illustrated on various examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.3666</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.3666</id><created>2009-08-25</created><authors><author><keyname>van Handel</keyname><forenames>Ramon</forenames></author></authors><title>On the minimal penalty for Markov order estimation</title><categories>math.PR cs.IT math.IT math.ST stat.TH</categories><comments>29 pages</comments><msc-class>62M05, 60E15, 60F15, 60G42, 60J10</msc-class><journal-ref>Probab. Th. Rel. Fields 150 (2011), pp. 709-738</journal-ref><doi>10.1007/s00440-010-0290-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that large-scale typicality of Markov sample paths implies that the
likelihood ratio statistic satisfies a law of iterated logarithm uniformly to
the same scale. As a consequence, the penalized likelihood Markov order
estimator is strongly consistent for penalties growing as slowly as log log n
when an upper bound is imposed on the order which may grow as rapidly as log n.
Our method of proof, using techniques from empirical process theory, does not
rely on the explicit expression for the maximum likelihood estimator in the
Markov case and could therefore be applicable in other settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.3668</identifier>
 <datestamp>2010-12-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.3668</id><created>2009-08-25</created><updated>2010-03-04</updated><authors><author><keyname>Bubenik</keyname><forenames>Peter</forenames></author><author><keyname>Carlsson</keyname><forenames>Gunnar</forenames></author><author><keyname>Kim</keyname><forenames>Peter T.</forenames></author><author><keyname>Luo</keyname><forenames>Zhiming</forenames></author></authors><title>Statistical topology via Morse theory, persistence and nonparametric
  estimation</title><categories>math.ST math.AT stat.TH</categories><comments>23 pages, 7 figures, minor corrections</comments><msc-class>62C10 (Primary), 62G08, 41A15 (Secondary), 55N99, 58J90</msc-class><journal-ref>Contemporary Mathematics, Vol. 516 (2010), pp. 75-92</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we examine the use of topological methods for multivariate
statistics. Using persistent homology from computational algebraic topology, a
random sample is used to construct estimators of persistent homology. This
estimation procedure can then be evaluated using the bottleneck distance
between the estimated persistent homology and the true persistent homology. The
connection to statistics comes from the fact that when viewed as a
nonparametric regression problem, the bottleneck distance is bounded by the
sup-norm loss. Consequently, a sharp asymptotic minimax bound is determined
under the sup-norm risk over Holder classes of functions for the nonparametric
regression problem on manifolds. This provides good convergence properties for
the persistent homology estimator in terms of the expected bottleneck distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.3817</identifier>
 <datestamp>2010-07-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.3817</id><created>2009-08-26</created><updated>2010-07-10</updated><authors><author><keyname>Scutari</keyname><forenames>Marco</forenames></author></authors><title>Learning Bayesian Networks with the bnlearn R Package</title><categories>stat.ML</categories><comments>22 pages, 4 pictures</comments><journal-ref>Journal of Statistical Software (2010), 35(3), 1-22</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  bnlearn is an R package which includes several algorithms for learning the
structure of Bayesian networks with either discrete or continuous variables.
Both constraint-based and score-based algorithms are implemented, and can use
the functionality provided by the snow package to improve their performance via
parallel computing. Several network scores and conditional independence
algorithms are available for both the learning algorithms and independent use.
Advanced plotting options are provided by the Rgraphviz package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.3961</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.3961</id><created>2009-08-27</created><updated>2013-04-17</updated><authors><author><keyname>Clifford</keyname><forenames>Peter</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Cosma</keyname><forenames>Ioana Ada</forenames><affiliation>University of Ottawa</affiliation></author></authors><title>A simple sketching algorithm for entropy estimation</title><categories>stat.CO math.ST stat.TH</categories><comments>11 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of approximating the empirical Shannon entropy of a
high-frequency data stream under the relaxed strict-turnstile model, when space
limitations make exact computation infeasible. An equivalent measure of entropy
is the Renyi entropy that depends on a constant alpha. This quantity can be
estimated efficiently and unbiasedly from a low-dimensional synopsis called an
alpha-stable data sketch via the method of compressed counting. An
approximation to the Shannon entropy can be obtained from the Renyi entropy by
taking alpha sufficiently close to 1. However, practical guidelines for
parameter calibration with respect to alpha are lacking. We avoid this problem
by showing that the random variables used in estimating the Renyi entropy can
be transformed to have a proper distributional limit as alpha approaches 1: the
maximally skewed, strictly stable distribution with alpha = 1 defined on the
entire real line. We propose a family of asymptotically unbiased log-mean
estimators of the Shannon entropy, indexed by a constant zeta &gt; 0, that can be
computed in a single-pass algorithm to provide an additive approximation. We
recommend the log-mean estimator with zeta = 1 that has exponentially
decreasing tail bounds on the error probability, asymptotic relative efficiency
of 0.932, and near-optimal computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.4119</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.4119</id><created>2009-08-27</created><authors><author><keyname>Moustakides</keyname><forenames>George V.</forenames></author><author><keyname>Polunchenko</keyname><forenames>Aleksey S.</forenames></author><author><keyname>Tartakovsky</keyname><forenames>Alexander G.</forenames></author></authors><title>Numerical Comparison of Cusum and Shiryaev-Roberts Procedures for
  Detecting Changes in Distributions</title><categories>stat.CO</categories><comments>21 pages, 8 figures, to appear in Communications in Statistics -
  Theory and Methods</comments><journal-ref>Communications in Statistics - Theory and Methods, vol. 38, no.
  16-17, pp. 3225-3239, 2009</journal-ref><doi>10.1080/03610920902947774</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The CUSUM procedure is known to be optimal for detecting a change in
distribution under a minimax scenario, whereas the Shiryaev-Roberts procedure
is optimal for detecting a change that occurs at a distant time horizon. As a
simpler alternative to the conventional Monte Carlo approach, we propose a
numerical method for the systematic comparison of the two detection schemes in
both settings, i.e., minimax and for detecting changes that occur in the
distant future. Our goal is accomplished by deriving a set of exact integral
equations for the performance metrics, which are then solved numerically. We
present detailed numerical results for the problem of detecting a change in the
mean of a Gaussian sequence, which show that the difference between the two
procedures is significant only when detecting small changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.4310</identifier>
 <datestamp>2011-12-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.4310</id><created>2009-08-28</created><updated>2011-12-06</updated><authors><author><keyname>Marron</keyname><forenames>Beatriz</forenames></author></authors><title>Co-occurrence Matrix and Fractal Dimension for Image Segmentation</title><categories>stat.AP cs.CV</categories><comments>This paper has been withdrawn by the author</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important tasks in image processing problem and machine
vision is object recognition, and the success of many proposed methods relies
on a suitable choice of algorithm for the segmentation of an image. This paper
focuses on how to apply texture operators based on the concept of fractal
dimension and cooccurence matrix, to the problem of object recognition and a
new method based on fractal dimension is introduced. Several images, in which
the result of the segmentation can be shown, are used to illustrate the use of
each method and a comparative study of each operator is made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.4334</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.4334</id><created>2009-08-29</created><authors><author><keyname>Queiros</keyname><forenames>Silvio M. Duarte</forenames></author></authors><title>One and two side generalisations of the log-Normal distribution by means
  of a new product definition</title><categories>math.ST physics.data-an stat.AP stat.TH</categories><comments>25 pages, 7 figures</comments><journal-ref>Physica A 391, 3594 (2012)</journal-ref><doi>10.1016/j.physa.2012.01.050</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this manuscript we introduce a generalisation of the log-Normal
distribution that is inspired by a modification of the Kaypten multiplicative
process using the $q$-product of Borges [Physica A \textbf{340}, 95 (2004)].
Depending on the value of q the distribution increases the tail for small (when
$q&lt;1$) or large (when $q&gt;1$) values of the variable upon analysis. The usual
log-Normal distribution is retrieved when $q=1$. The main statistical features
of this distribution are presented as well as a related random number
generators and tables of quantiles of the Kolmogorov-Smirnov. Lastly, we
illustrate the application of this distribution studying the adjustment of a
set of variables of biological and financial origin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.4461</identifier>
 <datestamp>2010-07-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.4461</id><created>2009-08-31</created><updated>2009-11-04</updated><authors><author><keyname>Hara</keyname><forenames>Hisayuki</forenames></author><author><keyname>Takemura</keyname><forenames>Akimichi</forenames></author></authors><title>Connecting tables with zero-one entries by a subset of a Markov basis</title><categories>math.ST math.CO stat.ME stat.TH</categories><comments>18 pages</comments><msc-class>62G17, 62H15</msc-class><journal-ref>in Algebraic Methods in Statistics and Probability II,
  Contemporary Mathematics, vol. 516, Amer. Math. Soc., Providence, RI, pp.
  199-213, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss connecting tables with zero-one entries by a subset of a Markov
basis. In this paper, as a Markov basis we consider the Graver basis, which
corresponds to the unique minimal Markov basis for the Lawrence lifting of the
original configuration. Since the Graver basis tends to be large, it is of
interest to clarify conditions such that a subset of the Graver basis, in
particular a minimal Markov basis itself, connects tables with zero-one
entries. We give some theoretical results on the connectivity of tables with
zero-one entries. We also study some common models, where a minimal Markov
basis for tables without the zero-one restriction does not connect tables with
zero-one entries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0908.4489</identifier>
 <datestamp>2010-08-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0908.4489</id><created>2009-08-31</created><updated>2010-01-04</updated><authors><author><keyname>Dobigeon</keyname><forenames>Nicolas</forenames></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames></author></authors><title>Bayesian orthogonal component analysis for sparse representation</title><categories>stat.ME stat.ML</categories><comments>Revised version. Accepted to IEEE Trans. Signal Processing</comments><journal-ref>IEEE Trans. Signal Processing, vol. 58. no. 5, pp. 2675-2685, May
  2010</journal-ref><doi>10.1109/TSP.2010.2041594</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of identifying a lower dimensional space
where observed data can be sparsely represented. This under-complete dictionary
learning task can be formulated as a blind separation problem of sparse sources
linearly mixed with an unknown orthogonal mixing matrix. This issue is
formulated in a Bayesian framework. First, the unknown sparse sources are
modeled as Bernoulli-Gaussian processes. To promote sparsity, a weighted
mixture of an atom at zero and a Gaussian distribution is proposed as prior
distribution for the unobserved sources. A non-informative prior distribution
defined on an appropriate Stiefel manifold is elected for the mixing matrix.
The Bayesian inference on the unknown parameters is conducted using a Markov
chain Monte Carlo (MCMC) method. A partially collapsed Gibbs sampler is
designed to generate samples asymptotically distributed according to the joint
posterior distribution of the unknown model parameters and hyperparameters.
These samples are then used to approximate the joint maximum a posteriori
estimator of the sources and mixing matrix. Simulations conducted on synthetic
data are reported to illustrate the performance of the method for recovering
sparse representations. An application to sparse coding on under-complete
dictionary is finally investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.0329</identifier>
 <datestamp>2011-04-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.0329</id><created>2009-09-02</created><updated>2010-09-23</updated><authors><author><keyname>Petelet</keyname><forenames>Matthieu</forenames><affiliation>CEA-DEN</affiliation></author><author><keyname>Iooss</keyname><forenames>Bertrand</forenames><affiliation>Mthodes d'Analyse Stochastique des Codes et Traitements Numriques</affiliation></author><author><keyname>Asserin</keyname><forenames>Olivier</forenames><affiliation>CEA-DEN</affiliation></author><author><keyname>Loredo</keyname><forenames>Alexandre</forenames><affiliation>EA1859</affiliation></author></authors><title>Latin hypercube sampling with inequality constraints</title><categories>stat.CO math.ST stat.TH</categories><proxy>ccsd</proxy><journal-ref>AStA Advances in Statistical Analysis 3 (2010) 11-21</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In some studies requiring predictive and CPU-time consuming numerical models,
the sampling design of the model input variables has to be chosen with caution.
For this purpose, Latin hypercube sampling has a long history and has shown its
robustness capabilities. In this paper we propose and discuss a new algorithm
to build a Latin hypercube sample (LHS) taking into account inequality
constraints between the sampled variables. This technique, called constrained
Latin hypercube sampling (cLHS), consists in doing permutations on an initial
LHS to honor the desired monotonic constraints. The relevance of this approach
is shown on a real example concerning the numerical welding simulation, where
the inequality constraints are caused by the physical decreasing of some
material properties in function of the temperature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.0639</identifier>
 <datestamp>2012-02-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.0639</id><created>2009-09-03</created><authors><author><keyname>Arribas-Gil</keyname><forenames>Ana</forenames></author></authors><title>Parameter Estimation in multiple-hidden i.i.d. models from biological
  multiple alignment</title><categories>stat.AP q-bio.GN</categories><journal-ref>Statistical Applications in Genetics and Molecular Biology: 9(1),
  Article 10, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we deal with parameter estimation in a latent variable model,
namely the multiple-hidden i.i.d. model, which is derived from multiple
alignment algorithms. We first provide a rigorous formalism for the homology
structure of k sequences related by a star-shaped phylogenetic tree in the
context of multiple alignment based on indel evolution models. We discuss
possible definitions of likelihoods and compare them to the criterion used in
multiple alignment algorithms. Existence of two different Information
divergence rates is established and a divergence property is shown under
additional assumptions. This would yield consistency for the parameter in
parametrization schemes for which the divergence property holds. We finally
extend the definition of the multiple-hidden i.i.d. model and the results
obtained to the case in which the sequences are related by an arbitrary
phylogenetic tree. Simulations illustrate different cases which are not covered
by our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.1008</identifier>
 <datestamp>2010-10-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.1008</id><created>2009-09-05</created><updated>2010-01-18</updated><authors><author><keyname>Robert</keyname><forenames>Christian P.</forenames></author><author><keyname>Chopin</keyname><forenames>Nicolas</forenames></author><author><keyname>Rousseau</keyname><forenames>Judith</forenames></author></authors><title>Rejoinder: Harold Jeffreys's Theory of Probability Revisited</title><categories>stat.ME math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-STS284REJ the
  Statistical Science (http://www.imstat.org/sts/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><report-no>IMS-STS-STS284REJ</report-no><journal-ref>Statistical Science (2009), Vol. 24, No. 2, 191-194</journal-ref><doi>10.1214/09-STS284REJ</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are grateful to all discussants of our re-visitation for their strong
support in our enterprise and for their overall agreement with our perspective.
Further discussions with them and other leading statisticians showed that the
legacy of Theory of Probability is alive and lasting. [arXiv:0804.3173]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.1046</identifier>
 <datestamp>2012-01-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.1046</id><created>2009-09-05</created><updated>2012-01-02</updated><authors><author><keyname>Girard</keyname><forenames>Didier A.</forenames><affiliation>LJK</affiliation></author></authors><title>Asymptotic near-efficiency of the "Gibbs-energy and empirical-variance"
  estimating functions for fitting Matern models to a dense (noisy) series</title><categories>math.ST stat.ME stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let us call as "Gaussian Gibbs energy" the quadratic form appearing in the
maximum likelihood (ML) criterion when fitting a zero-mean multidimensional
Gaussian distribution to one realization. We consider a continuous-time
Gaussian process $Z$ which belongs to the Mat\'ern family with known
"regularity" index $\nu \geq 1/2$. For estimating the range and the variance of
$Z$ from observations on a "dense" regular grid, corrupted by a Gaussian white
noise of known variance, we propose two simple estimating functions based on
the conditional Gibbs energy mean (CGEM) and the empirical variance (EV). We
show that the ratio of the large sample mean squared error of the resulting
CGEM-EV estimate of the range-parameter to the one of its ML estimate, and the
analog ratio for the variance-parameter, both converge (when the grid-step
tends to 0) toward a constant, only function of $\nu$, surprisingly close to 1
provided $\nu$ is not too large. This latter condition on $\nu$ has not to be
imposed to obtain the convergence to 1 of the analog ratio for the
microergodic-parameter. Possible extensions of this approach, which may benefit
from very easy numerical implementations, are briefly discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.1234</identifier>
 <datestamp>2010-11-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.1234</id><created>2009-09-07</created><updated>2010-09-22</updated><authors><author><keyname>de Abreu</keyname><forenames>Gabriel C. G.</forenames></author><author><keyname>Labouriau</keyname><forenames>Rodrigo</forenames></author><author><keyname>Edwards</keyname><forenames>David</forenames></author></authors><title>High-dimensional Graphical Model Search with gRapHD R Package</title><categories>stat.ML stat.CO</categories><comments>20 pages with 8 figures</comments><journal-ref>Journal of Statistical Software, Vol. 37, Issue 1, Nov. 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the R package gRapHD for efficient selection of
high-dimensional undirected graphical models. The package provides tools for
selecting trees, forests and decomposable models minimizing information
criteria such as AIC or BIC, and for displaying the independence graphs of the
models. It has also some useful tools for analysing graphical structures. It
supports the use of discrete, continuous, or both types of variables
simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.1373</identifier>
 <datestamp>2012-10-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.1373</id><created>2009-09-07</created><updated>2012-09-28</updated><authors><author><keyname>Kim</keyname><forenames>Seyoung</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Tree-guided group lasso for multi-response regression with structured
  sparsity, with an application to eQTL mapping</title><categories>stat.ML q-bio.GN q-bio.QM stat.AP stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOAS549 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS549</report-no><journal-ref>Annals of Applied Statistics 2012, Vol. 6, No. 3, 1095-1117</journal-ref><doi>10.1214/12-AOAS549</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating a sparse multi-response regression
function, with an application to expression quantitative trait locus (eQTL)
mapping, where the goal is to discover genetic variations that influence
gene-expression levels. In particular, we investigate a shrinkage technique
capable of capturing a given hierarchical structure over the responses, such as
a hierarchical clustering tree with leaf nodes for responses and internal nodes
for clusters of related responses at multiple granularity, and we seek to
leverage this structure to recover covariates relevant to each
hierarchically-defined cluster of responses. We propose a tree-guided group
lasso, or tree lasso, for estimating such structured sparsity under
multi-response regression by employing a novel penalty function constructed
from the tree. We describe a systematic weighting scheme for the overlapping
groups in the tree-penalty such that each regression coefficient is penalized
in a balanced manner despite the inhomogeneous multiplicity of group
memberships of the regression coefficients due to overlaps among groups. For
efficient optimization, we employ a smoothing proximal gradient method that was
originally developed for a general class of structured-sparsity-inducing
penalties. Using simulated and yeast data sets, we demonstrate that our method
shows a superior performance in terms of both prediction errors and recovery of
true sparsity patterns, compared to other methods for learning a
multivariate-response regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.1685</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.1685</id><created>2009-09-09</created><updated>2010-05-16</updated><authors><author><keyname>Scutari</keyname><forenames>Marco</forenames></author></authors><title>Structure Variability in Bayesian Networks</title><categories>stat.ME math.ST stat.ML stat.TH</categories><comments>21 pages, 4 figures</comments><report-no>Working Paper 13 - 2009, Department of Statistical Sciences,
  University of Padova</report-no><journal-ref>merged and published as part of Bayesian Analysis 2013, 8(3),
  505-532</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structure of a Bayesian network encodes most of the information about the
probability distribution of the data, which is uniquely identified given some
general distributional assumptions. Therefore it's important to study the
variability of its network structure, which can be used to compare the
performance of different learning algorithms and to measure the strength of any
arbitrary subset of arcs.
  In this paper we will introduce some descriptive statistics and the
corresponding parametric and Monte Carlo tests on the undirected graph
underlying the structure of a Bayesian network, modeled as a multivariate
Bernoulli random variable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.1884</identifier>
 <datestamp>2011-09-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.1884</id><created>2009-09-10</created><updated>2011-09-13</updated><authors><author><keyname>Arlot</keyname><forenames>Sylvain</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author></authors><title>Data-driven calibration of linear estimators with minimal penalties</title><categories>math.ST stat.ME stat.ML stat.TH</categories><comments>Advances in Neural Information Processing Systems (NIPS 2009),
  Vancouver : Canada (2009)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper tackles the problem of selecting among several linear estimators
in non-parametric regression; this includes model selection for linear
regression, the choice of a regularization parameter in kernel ridge
regression, spline smoothing or locally weighted regression, and the choice of
a kernel in multiple kernel learning. We propose a new algorithm which first
estimates consistently the variance of the noise, based upon the concept of
minimal penalty, which was previously introduced in the context of model
selection. Then, plugging our variance estimate in Mallows' $C_L$ penalty is
proved to lead to an algorithm satisfying an oracle inequality. Simulation
experiments with kernel ridge regression and multiple kernel learning show that
the proposed algorithm often improves significantly existing calibration
procedures such as generalized cross-validation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.1933</identifier>
 <datestamp>2010-06-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.1933</id><created>2009-09-10</created><updated>2010-06-04</updated><authors><author><keyname>Ralaivola</keyname><forenames>Liva</forenames><affiliation>LIF</affiliation></author><author><keyname>Szafranski</keyname><forenames>Marie</forenames><affiliation>IBISC</affiliation></author><author><keyname>Stempfel</keyname><forenames>Guillaume</forenames><affiliation>LIF</affiliation></author></authors><title>Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and
  Stationary $\beta$-Mixing Processes</title><categories>cs.LG math.ST stat.ML stat.TH</categories><comments>Long version of the AISTATS 09 paper:
  http://jmlr.csail.mit.edu/proceedings/papers/v5/ralaivola09a/ralaivola09a.pdf</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pac-Bayes bounds are among the most accurate generalization bounds for
classifiers learned from independently and identically distributed (IID) data,
and it is particularly so for margin classifiers: there have been recent
contributions showing how practical these bounds can be either to perform model
selection (Ambroladze et al., 2007) or even to directly guide the learning of
linear classifiers (Germain et al., 2009). However, there are many practical
situations where the training data show some dependencies and where the
traditional IID assumption does not hold. Stating generalization bounds for
such frameworks is therefore of the utmost interest, both from theoretical and
practical standpoints. In this work, we propose the first - to the best of our
knowledge - Pac-Bayes generalization bounds for classifiers trained on data
exhibiting interdependencies. The approach undertaken to establish our results
is based on the decomposition of a so-called dependency graph that encodes the
dependencies within the data, in sets of independent data, thanks to graph
fractional covers. Our bounds are very general, since being able to find an
upper bound on the fractional chromatic number of the dependency graph is
sufficient to get new Pac-Bayes bounds for specific settings. We show how our
results can be used to derive bounds for ranking statistics (such as Auc) and
classifiers trained on data distributed according to a stationary {\ss}-mixing
process. In the way, we show how our approach seemlessly allows us to deal with
U-processes. As a side note, we also provide a Pac-Bayes generalization bound
for classifiers learned on data from stationary $\varphi$-mixing distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.2139</identifier>
 <datestamp>2012-07-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.2139</id><created>2009-09-11</created><updated>2012-07-24</updated><authors><author><keyname>Chigansky</keyname><forenames>Pavel</forenames></author><author><keyname>Ritov</keyname><forenames>Yaacov</forenames></author></authors><title>On the Viterbi process with continuous state space</title><categories>math.ST math.PR stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ294 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ294</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 2, 609-627</journal-ref><doi>10.3150/10-BEJ294</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with convergence of the maximum a posterior probability path
estimator in hidden Markov models. We show that when the state space of the
hidden process is continuous, the optimal path may stabilize in a way which is
essentially different from the previously considered finite-state setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.2191</identifier>
 <datestamp>2010-02-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.2191</id><created>2009-09-11</created><updated>2010-02-19</updated><authors><author><keyname>Girard</keyname><forenames>Robin</forenames></author></authors><title>Fast rate of convergence in high dimensional linear discriminant
  analysis</title><categories>math.ST stat.TH</categories><msc-class>62C99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives a theoretical analysis of high dimensional linear
discrimination of Gaussian data. We study the excess risk of linear
discriminant rules. We emphasis on the poor performances of standard procedures
in the case when dimension p is larger than sample size n. The corresponding
theoretical results are non asymptotic lower bounds. On the other hand, we
propose two discrimination procedures based on dimensionality reduction and
provide associated rates of convergence which can be O(log(p)/n) under sparsity
assumptions. Finally all our results rely on a theorem that provides simple
sharp relations between the excess risk and an estimation error associated to
the geometric parameters defining the used discrimination rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.2234</identifier>
 <datestamp>2010-09-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.2234</id><created>2009-09-11</created><updated>2010-09-09</updated><authors><author><keyname>Unnikrishnan</keyname><forenames>Jayakrishnan</forenames></author><author><keyname>Huang</keyname><forenames>Dayu</forenames></author><author><keyname>Meyn</keyname><forenames>Sean</forenames></author><author><keyname>Surana</keyname><forenames>Amit</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal</forenames></author></authors><title>Universal and Composite Hypothesis Testing via Mismatched Divergence</title><categories>cs.IT cs.LG math.IT math.ST stat.TH</categories><comments>Accepted to IEEE Transactions on Information Theory, July 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the universal hypothesis testing problem, where the goal is to decide
between the known null hypothesis distribution and some other unknown
distribution, Hoeffding proposed a universal test in the nineteen sixties.
Hoeffding's universal test statistic can be written in terms of
Kullback-Leibler (K-L) divergence between the empirical distribution of the
observations and the null hypothesis distribution. In this paper a modification
of Hoeffding's test is considered based on a relaxation of the K-L divergence
test statistic, referred to as the mismatched divergence. The resulting
mismatched test is shown to be a generalized likelihood-ratio test (GLRT) for
the case where the alternate distribution lies in a parametric family of the
distributions characterized by a finite dimensional parameter, i.e., it is a
solution to the corresponding composite hypothesis testing problem. For certain
choices of the alternate distribution, it is shown that both the Hoeffding test
and the mismatched test have the same asymptotic performance in terms of error
exponents. A consequence of this result is that the GLRT is optimal in
differentiating a particular distribution from others in an exponential family.
It is also shown that the mismatched test has a significant advantage over the
Hoeffding test in terms of finite sample size performance. This advantage is
due to the difference in the asymptotic variances of the two test statistics
under the null hypothesis. In particular, the variance of the K-L divergence
grows linearly with the alphabet size, making the test impractical for
applications involving large alphabet distributions. The variance of the
mismatched divergence on the other hand grows linearly with the dimension of
the parameter space, and can hence be controlled through a prudent choice of
the function class defining the mismatched divergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.2479</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.2479</id><created>2009-09-14</created><updated>2013-02-04</updated><authors><author><keyname>Simkin</keyname><forenames>M. V.</forenames></author></authors><title>Scientific evaluation of Charles Dickens</title><categories>physics.soc-ph physics.data-an stat.AP</categories><journal-ref>Journal of Quantitative Linguistics, volume 20, issue 1, pp 68-73,
  2013</journal-ref><doi>10.1080/09296174.2012.754602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I report the results of the test, where the takers had to tell the prose of
Charles Dickens from that of Edward Bulwer-Lytton, who is considered by many to
be the worst writer in history of letters. The average score is about 50%,
which is on the level of random guessing. This suggests that the quality of
Dickens' prose is the same as of that of Bulwer-Lytton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.2904</identifier>
 <datestamp>2010-06-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.2904</id><created>2009-09-15</created><updated>2010-06-22</updated><authors><author><keyname>Komatsu</keyname><forenames>Yusuke</forenames></author><author><keyname>Shimizu</keyname><forenames>Shohei</forenames></author><author><keyname>Shimodaira</keyname><forenames>Hidetoshi</forenames></author></authors><title>Computing p-values of LiNGAM outputs via Multiscale Bootstrap</title><categories>stat.ML stat.ME</categories><comments>11 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structural equation models and Bayesian networks have been widely used to
study causal relationships between continuous variables. Recently, a
non-Gaussian method called LiNGAM was proposed to discover such causal models
and has been extended in various directions. An important problem with LiNGAM
is that the results are affected by the random sampling of the data as with any
statistical method. Thus, some analysis of the statistical reliability or
confidence level should be conducted. A common method to evaluate a confidence
level is a bootstrap method. However, a confidence level computed by ordinary
bootstrap method is known to be biased as a probability-value ($p$-value) of
hypothesis testing. In this paper, we propose a new procedure to apply an
advanced bootstrap method called multiscale bootstrap to compute confidence
levels, i.e., p-values, of LiNGAM outputs. The multiscale bootstrap method
gives unbiased $p$-values with asymptotic much higher accuracy. Experiments on
artificial data demonstrate the utility of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.3667</identifier>
 <datestamp>2011-02-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.3667</id><created>2009-09-20</created><updated>2011-02-15</updated><authors><author><keyname>Arendarczyk</keyname><forenames>Marek</forenames></author><author><keyname>Dbicki</keyname><forenames>Krzysztof</forenames></author></authors><title>Asymptotics of supremum distribution of a Gaussian process over a
  Weibullian time</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ266 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ266</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 1, 194-210</journal-ref><doi>10.3150/10-BEJ266</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\{X(t):t\in[0,\infty)\}$ be a centered Gaussian process with stationary
increments and variance function $\sigma^2_X(t)$. We study the exact
asymptotics of ${\mathbb{P}}(\sup_{t\in[0,T]}X(t)&gt;u)$ as $u\to\infty$, where
$T$ is an independent of $\{X(t)\}$ non-negative Weibullian random variable. As
an illustration, we work out the asymptotics of the supremum distribution of
fractional Laplace motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.3704</identifier>
 <datestamp>2011-08-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.3704</id><created>2009-09-21</created><updated>2011-08-10</updated><authors><author><keyname>Zeisel</keyname><forenames>Amit</forenames></author><author><keyname>Zuk</keyname><forenames>Or</forenames></author><author><keyname>Domany</keyname><forenames>Eytan</forenames></author></authors><title>FDR control with adaptive procedures and FDR monotonicity</title><categories>stat.ME stat.AP</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOAS399 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS399</report-no><journal-ref>Annals of Applied Statistics 2011, Vol. 5, No. 2A, 943-968</journal-ref><doi>10.1214/10-AOAS399</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The steep rise in availability and usage of high-throughput technologies in
biology brought with it a clear need for methods to control the False Discovery
Rate (FDR) in multiple tests. Benjamini and Hochberg (BH) introduced in 1995 a
simple procedure and proved that it provided a bound on the expected value,
$\mathit{FDR}\leq q$. Since then, many authors tried to improve the BH bound,
with one approach being designing adaptive procedures, which aim at estimating
the number of true null hypothesis in order to get a better FDR bound. Our two
main rigorous results are the following: (i) a theorem that provides a bound on
the FDR for adaptive procedures that use any estimator for the number of true
hypotheses ($m_0$), (ii) a theorem that proves a monotonicity property of
general BH-like procedures, both for the case where the hypotheses are
independent. We also propose two improved procedures for which we prove FDR
control for the independent case, and demonstrate their advantages over several
available bounds, on simulated data and on a large number of gene expression
data sets. Both applications are simple and involve a similar amount of
computation as the original BH procedure. We compare the performance of our
proposed procedures with BH and other procedures and find that in most cases we
get more power for the same level of statistical significance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.3764</identifier>
 <datestamp>2012-01-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.3764</id><created>2009-09-21</created><updated>2012-01-05</updated><authors><author><keyname>Haas</keyname><forenames>Bndicte</forenames></author><author><keyname>Miermont</keyname><forenames>Grgory</forenames></author></authors><title>Self-similar scaling limits of non-increasing Markov chains</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ312 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ312</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 4, 1217-1247</journal-ref><doi>10.3150/10-BEJ312</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study scaling limits of non-increasing Markov chains with values in the
set of non-negative integers, under the assumption that the large jump events
are rare and happen at rates that behave like a negative power of the current
state. We show that the chain starting from $n$ and appropriately rescaled,
converges in distribution, as $n\rightarrow \infty$, to a non-increasing
self-similar Markov process. This convergence holds jointly with that of the
rescaled absorption time to the time at which the self-similar Markov process
reaches first 0. We discuss various applications to the study of random walks
with a barrier, of the number of collisions in $\Lambda$-coalescents that do
not descend from infinity and of non-consistent regenerative compositions.
Further applications to the scaling limits of Markov branching trees are
developed in our paper, Scaling limits of Markov branching trees, with
applications to Galton--Watson and random unordered trees (2010).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.4331</identifier>
 <datestamp>2010-10-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.4331</id><created>2009-09-23</created><updated>2010-10-06</updated><authors><author><keyname>Chang</keyname><forenames>Jonathan</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author></authors><title>Hierarchical relational models for document networks</title><categories>stat.AP</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOAS309 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS309</report-no><journal-ref>Annals of Applied Statistics 2010, Vol. 4, No. 1, 124-150</journal-ref><doi>10.1214/09-AOAS309</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop the relational topic model (RTM), a hierarchical model of both
network structure and node attributes. We focus on document networks, where the
attributes of each document are its words, that is, discrete observations taken
from a fixed vocabulary. For each pair of documents, the RTM models their link
as a binary random variable that is conditioned on their contents. The model
can be used to summarize a network of documents, predict links between them,
and predict words within them. We derive efficient inference and estimation
algorithms based on variational methods that take advantage of sparsity and
scale with the number of links. We evaluate the predictive performance of the
RTM for large networks of scientific abstracts, web documents, and
geographically tagged news.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.4370</identifier>
 <datestamp>2010-11-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.4370</id><created>2009-09-24</created><updated>2010-10-28</updated><authors><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author><author><keyname>Zaman</keyname><forenames>Tauhid</forenames></author></authors><title>Rumors in a Network: Who's the Culprit?</title><categories>stat.ML stat.AP</categories><comments>43 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a systematic study of the problem of finding the source of a rumor
in a network. We model rumor spreading in a network with a variant of the
popular SIR model and then construct an estimator for the rumor source. This
estimator is based upon a novel topological quantity which we term
\textbf{rumor centrality}. We establish that this is an ML estimator for a
class of graphs. We find the following surprising threshold phenomenon: on
trees which grow faster than a line, the estimator always has non-trivial
detection probability, whereas on trees that grow like a line, the detection
probability will go to 0 as the network grows. Simulations performed on
synthetic networks such as the popular small-world and scale-free networks, and
on real networks such as an internet AS network and the U.S. electric power
grid network, show that the estimator either finds the source exactly or within
a few hops of the true source across different network topologies. We compare
rumor centrality to another common network centrality notion known as distance
centrality. We prove that on trees, the rumor center and distance center are
equivalent, but on general networks, they may differ. Indeed, simulations show
that rumor centrality outperforms distance centrality in finding rumor sources
in networks which are not tree-like.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.4551</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.4551</id><created>2009-09-24</created><authors><author><keyname>Malinovsky</keyname><forenames>Yaakov</forenames></author><author><keyname>Rinott</keyname><forenames>Yosef</forenames></author></authors><title>Prediction of Ordered Random Effects in a Simple Small Area Model</title><categories>stat.ME math.ST stat.TH</categories><comments>30 pages, 6 figures</comments><journal-ref>Statistica Sinica 20 (2010), 697-714</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prediction of a vector of ordered parameters or part of it arises naturally
in the context of Small Area Estimation (SAE). For example, one may want to
estimate the parameters associated with the top ten areas, the best or worst
area, or a certain percentile. We use a simple SAE model to show that
estimation of ordered parameters by the corresponding ordered estimates of each
area separately does not yield good results with respect to MSE. Shrinkage-type
predictors, with an appropriate amount of shrinkage for the particular problem
of ordered parameters, are considerably better, and their performance is close
to that of the optimal predictors, which cannot in general be computed
explicitly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.4588</identifier>
 <datestamp>2010-12-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.4588</id><created>2009-09-24</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Discrete MDL Predicts in Total Variation</title><categories>math.PR cs.IT cs.LG math.IT math.ST stat.ML stat.TH</categories><comments>15 LaTeX pages</comments><journal-ref>Advances in Neural Information Processing Systems 22 (NIPS 2009)
  pages 817-825</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Minimum Description Length (MDL) principle selects the model that has the
shortest code for data plus model. We show that for a countable class of
models, MDL predictions are close to the true distribution in a strong sense.
The result is completely general. No independence, ergodicity, stationarity,
identifiability, or other assumption on the model class need to be made. More
formally, we show that for any countable class of models, the distributions
selected by MDL (or MAP) asymptotically predict (merge with) the true measure
in the class in total variation distance. Implications for non-i.i.d. domains
like time-series forecasting, discriminative learning, and reinforcement
learning are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.4821</identifier>
 <datestamp>2011-08-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.4821</id><created>2009-09-25</created><updated>2011-06-09</updated><authors><author><keyname>Hara</keyname><forenames>Hisayuki</forenames></author><author><keyname>Sei</keyname><forenames>Tomonari</forenames></author><author><keyname>Takemura</keyname><forenames>Akimichi</forenames></author></authors><title>Hierarchical subspace models for contingency tables</title><categories>math.ST stat.TH</categories><comments>26 pages</comments><msc-class>62H17, 62H99</msc-class><journal-ref>J. Multivariate Anal. 103(2012), 19-34</journal-ref><doi>10.1016/j.jmva.2011.06.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For statistical analysis of multiway contingency tables we propose modeling
interaction terms in each maximal compact component of a hierarchical model. By
this approach we can search for parsimonious models with smaller degrees of
freedom than the usual hierarchical model, while preserving conditional
independence structures in the hierarchical model. We discuss estimation and
exacts tests of the proposed model and illustrate the advantage of the proposed
modeling with some data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.4856</identifier>
 <datestamp>2012-01-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.4856</id><created>2009-09-26</created><updated>2010-12-20</updated><authors><author><keyname>Maathuis</keyname><forenames>Marloes H.</forenames></author><author><keyname>Hudgens</keyname><forenames>Michael G.</forenames></author></authors><title>Nonparametric inference for competing risks current status data with
  continuous, discrete or grouped observation times</title><categories>stat.ME</categories><comments>16 pages, 3 figures</comments><journal-ref>Biometrika 2011, Vol. 98, No. 2, 325-340</journal-ref><doi>10.1093/biomet/asq083</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New methods and theory have recently been developed to nonparametrically
estimate cumulative incidence functions for competing risks survival data
subject to current status censoring. In particular, the limiting distribution
of the nonparametric maximum likelihood estimator and a simplified "naive
estimator" have been established under certain smoothness conditions. In this
paper, we establish the large-sample behavior of these estimators in two
additional models, namely when the observation time distribution has discrete
support and when the observation times are grouped. These asymptotic results
are applied to the construction of confidence intervals in the three different
models. The methods are illustrated on two data sets regarding the cumulative
incidence of (i) different types of menopause from a cross-sectional sample of
women in the United States and (ii) subtype-specific HIV infection from a
sero-prevalence study in injecting drug users in Thailand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.4961</identifier>
 <datestamp>2012-02-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.4961</id><created>2009-09-28</created><authors><author><keyname>Bartolucci</keyname><forenames>Francesco</forenames></author><author><keyname>Pennoni</keyname><forenames>Fulvia</forenames></author><author><keyname>Vittadini</keyname><forenames>Giorgio</forenames></author></authors><title>Assessment of school performance through a multilevel latent Markov
  Rasch model</title><categories>stat.AP</categories><doi>10.3102/1076998610381396</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An extension of the latent Markov Rasch model is described for the analysis
of binary longitudinal data with covariates when subjects are collected in
clusters, e.g. students clustered in classes. For each subject, the latent
process is used to represent the characteristic of interest (e.g. ability)
conditional on the effect of the cluster to which he/she belongs. The latter
effect is modeled by a discrete latent variable associated with each cluster.
For the maximum likelihood estimation of the model parameters we outline an EM
algorithm. We show how the proposed model may be used for assessing the
development of cognitive Math achievement. This approach is applied to the
analysis of a dataset collected in the Lombardy Region (Italy) and based on
test scores over three years of middle-school students attending public and
private schools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.5026</identifier>
 <datestamp>2011-05-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.5026</id><created>2009-09-28</created><updated>2011-05-08</updated><authors><author><keyname>Suzuki</keyname><forenames>Taiji</forenames></author><author><keyname>Tomioka</keyname><forenames>Ryota</forenames></author></authors><title>SpicyMKL</title><categories>stat.ML stat.CO</categories><comments>30 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new optimization algorithm for Multiple Kernel Learning (MKL)
called SpicyMKL, which is applicable to general convex loss functions and
general types of regularization. The proposed SpicyMKL iteratively solves
smooth minimization problems. Thus, there is no need of solving SVM, LP, or QP
internally. SpicyMKL can be viewed as a proximal minimization method and
converges super-linearly. The cost of inner minimization is roughly
proportional to the number of active kernels. Therefore, when we aim for a
sparse kernel combination, our algorithm scales well against increasing number
of kernels. Moreover, we give a general block-norm formulation of MKL that
includes non-sparse regularizations, such as elastic-net and \ellp -norm
regularizations. Extending SpicyMKL, we propose an efficient optimization
method for the general regularization framework. Experimental results show that
our algorithm is faster than existing methods especially when the number of
kernels is large (&gt; 1000).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.5194</identifier>
 <datestamp>2010-07-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.5194</id><created>2009-09-28</created><updated>2010-07-15</updated><authors><author><keyname>Hannah</keyname><forenames>Lauren A.</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author><author><keyname>Powell</keyname><forenames>Warren B.</forenames></author></authors><title>Dirichlet Process Mixtures of Generalized Linear Models</title><categories>stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLM),
a new method of nonparametric regression that accommodates continuous and
categorical inputs, and responses that can be modeled by a generalized linear
model. We prove conditions for the asymptotic unbiasedness of the DP-GLM
regression mean function estimate. We also give examples for when those
conditions hold, including models for compactly supported continuous
distributions and a model with continuous covariates and categorical response.
We empirically analyze the properties of the DP-GLM and why it provides better
results than existing Dirichlet process mixture regression models. We evaluate
DP-GLM on several data sets, comparing it to modern methods of nonparametric
regression like CART, Bayesian trees and Gaussian processes. Compared to
existing techniques, the DP-GLM provides a single model (and corresponding
inference algorithms) that performs well in many regression settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.5216</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.5216</id><created>2009-09-28</created><updated>2010-01-04</updated><authors><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Willsky</keyname><forenames>Alan S.</forenames></author></authors><title>Learning Gaussian Tree Models: Analysis of Error Exponents and Extremal
  Structures</title><categories>stat.ML cs.IT math.IT math.ST stat.TH</categories><comments>Submitted to Transactions on Signal Processing</comments><journal-ref>IEEE Transactions on Signal Processing, May 2010, Volume: 58
  Issue:5, pages 2701 - 2714</journal-ref><doi>10.1109/TSP.2010.2042478</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of learning tree-structured Gaussian graphical models from
independent and identically distributed (i.i.d.) samples is considered. The
influence of the tree structure and the parameters of the Gaussian distribution
on the learning rate as the number of samples increases is discussed.
Specifically, the error exponent corresponding to the event that the estimated
tree structure differs from the actual unknown tree structure of the
distribution is analyzed. Finding the error exponent reduces to a least-squares
problem in the very noisy learning regime. In this regime, it is shown that the
extremal tree structure that minimizes the error exponent is the star for any
fixed set of correlation coefficients on the edges of the tree. If the
magnitudes of all the correlation coefficients are less than 0.63, it is also
shown that the tree structure that maximizes the error exponent is the Markov
chain. In other words, the star and the chain graphs represent the hardest and
the easiest structures to learn in the class of tree-structured Gaussian
graphical models. This result can also be intuitively explained by correlation
decay: pairs of nodes which are far apart, in terms of graph distance, are
unlikely to be mistaken as edges by the maximum-likelihood estimator in the
asymptotic regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.5262</identifier>
 <datestamp>2010-07-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.5262</id><created>2009-09-29</created><updated>2010-07-06</updated><authors><author><keyname>Gramacy</keyname><forenames>Robert B.</forenames></author><author><keyname>Polson</keyname><forenames>Nicholas G.</forenames></author></authors><title>Particle learning of Gaussian process models for sequential design and
  optimization</title><categories>stat.CO stat.ME</categories><comments>18 pages, 5 figures, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a simulation-based method for the online updating of Gaussian
process regression and classification models. Our method exploits sequential
Monte Carlo to produce a fast sequential design algorithm for these models
relative to the established MCMC alternative. The latter is less ideal for
sequential design since it must be restarted and iterated to convergence with
the inclusion of each new design point. We illustrate some attractive ensemble
aspects of our SMC approach, and show how active learning heuristics may be
implemented via particles to optimize a noisy function or to explore
classification boundaries online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.5289</identifier>
 <datestamp>2011-01-17</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.5289</id><created>2009-09-29</created><authors><author><keyname>Sapatinas</keyname><forenames>Theofanis</forenames></author><author><keyname>Shanbhag</keyname><forenames>Damodar N.</forenames></author></authors><title>Moment properties of multivariate infinitely divisible laws and criteria
  for self-decomposability</title><categories>math.ST stat.TH</categories><comments>22 pages (To appear in: Journal of Multivariate Analysis)</comments><msc-class>60E07 (primary), 60E05, 60G51, 62H10 (secondary)</msc-class><journal-ref>Journal of Multivariate Analysis, Vol. 101, 500-511, (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ramachandran (1969, Theorem 8) has shown that for any univariate infinitely
divisible distribution and any positive real number $\alpha$, an absolute
moment of order $\alpha$ relative to the distribution exists (as a finite
number) if and only if this is so for a certain truncated version of the
corresponding L$\acute{\rm e}$vy measure. A generalized version of this result
in the case of multivariate infinitely divisible distributions, involving the
concept of g-moments, is given by Sato (1999, Theorem 25.3). We extend
Ramachandran's theorem to the multivariate case, keeping in mind the immediate
requirements under appropriate assumptions of cumulant studies of the
distributions referred to; the format of Sato's theorem just referred to
obviously varies from ours and seems to be having a different agenda. Also,
appealing to a further criterion based on the L$\acute{\rm e}$vy measure, we
identify in a certain class of multivariate infinitely divisible distributions
the distributions that are self-decomposable; this throws new light on
structural aspects of certain multivariate distributions such as the
multivariate generalized hyperbolic distributions studied by Barndorff-Nielsen
(1977) and others. Various points of relevance to the study are also addressed
through specific examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.5369</identifier>
 <datestamp>2010-03-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.5369</id><created>2009-09-29</created><updated>2010-03-25</updated><authors><author><keyname>Robert</keyname><forenames>Christian P.</forenames></author></authors><title>On the relevance of the Bayesian approach to Statistics</title><categories>stat.ME stat.CO</categories><comments>This paper is written in conjunction with the 3rd Bayesian
  econometrics meeting that took place at the Rimini Centre for Economic
  Analysis on July 01-02, 2009. A version will eventually be published in the
  Review of Economic Analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We argue here about the relevance and the ultimate unity of the Bayesian
approach in a neutral and agnostic manner. Our main theme is that Bayesian data
analysis is an effective tool for handling complex models, as proven by the
increasing proportion of Bayesian studies in the applied sciences. We disregard
in this essay the philosophical debates on the deeper meaning of probability
and on the random nature of parameters as things of the past that do a
disservice to the approach and are incomprehensible to most bystanders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.5438</identifier>
 <datestamp>2011-10-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.5438</id><created>2009-09-29</created><authors><author><keyname>Rudoy</keyname><forenames>Daniel</forenames></author><author><keyname>Yuen</keyname><forenames>Shelten G.</forenames></author><author><keyname>Howe</keyname><forenames>Robert D.</forenames></author><author><keyname>Wolfe</keyname><forenames>Patrick J.</forenames></author></authors><title>Bayesian changepoint analysis for atomic force microscopy and soft
  material indentation</title><categories>stat.AP cond-mat.mtrl-sci physics.ins-det</categories><comments>20 pages, 6 figures; submitted for publication</comments><journal-ref>Journal of the Royal Statistical Society, Series C, 59,
  573-593,2010</journal-ref><doi>10.1111/j.1467-9876.2010.00715.x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Material indentation studies, in which a probe is brought into controlled
physical contact with an experimental sample, have long been a primary means by
which scientists characterize the mechanical properties of materials. More
recently, the advent of atomic force microscopy, which operates on the same
fundamental principle, has in turn revolutionized the nanoscale analysis of
soft biomaterials such as cells and tissues. This paper addresses the
inferential problems associated with material indentation and atomic force
microscopy, through a framework for the changepoint analysis of pre- and
post-contact data that is applicable to experiments across a variety of
physical scales. A hierarchical Bayesian model is proposed to account for
experimentally observed changepoint smoothness constraints and measurement
error variability, with efficient Monte Carlo methods developed and employed to
realize inference via posterior sampling for parameters such as Young's
modulus, a key quantifier of material stiffness. These results are the first to
provide the materials science community with rigorous inference procedures and
uncertainty quantification, via optimized and fully automated high-throughput
algorithms, implemented as the publicly available software package BayesCP. To
demonstrate the consistent accuracy and wide applicability of this approach,
results are shown for a variety of data sets from both macro- and
micro-materials experiments--including silicone, neurons, and red blood
cells--conducted by the authors and others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0909.5524</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0909.5524</id><created>2009-09-30</created><updated>2011-09-20</updated><authors><author><keyname>Lung-Yut-Fong</keyname><forenames>Alexandre</forenames><affiliation>LTCI</affiliation></author><author><keyname>Lvy-Leduc</keyname><forenames>Cline</forenames><affiliation>LTCI</affiliation></author><author><keyname>Capp</keyname><forenames>Olivier</forenames><affiliation>LTCI</affiliation></author></authors><title>Distributed detection/localization of change-points in high-dimensional
  network traffic data</title><categories>stat.AP cs.NI math.ST stat.TH</categories><comments>Statistics and Computing (2011) 1-12</comments><proxy>ccsd</proxy><doi>10.1007/s11222-011-9240-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel approach for distributed statistical detection of
change-points in high-volume network traffic. We consider more specifically the
task of detecting and identifying the targets of Distributed Denial of Service
(DDoS) attacks. The proposed algorithm, called DTopRank, performs distributed
network anomaly detection by aggregating the partial information gathered in a
set of network monitors. In order to address massive data while limiting the
communication overhead within the network, the approach combines record
filtering at the monitor level and a nonparametric rank test for doubly
censored time series at the central decision site. The performance of the
DTopRank algorithm is illustrated both on synthetic data as well as from a
traffic trace provided by a major Internet service provider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.0063</identifier>
 <datestamp>2011-06-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.0063</id><created>2009-09-30</created><updated>2011-06-21</updated><authors><author><keyname>Farias</keyname><forenames>Vivek F.</forenames></author><author><keyname>Jagabathula</keyname><forenames>Srikanth</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author></authors><title>A Nonparametric Approach to Modeling Choice with Limited Data</title><categories>stat.AP</categories><comments>44 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central push in operations models over the last decade has been the
incorporation of models of customer choice. Real world implementations of many
of these models face the formidable stumbling block of simply identifying the
`right' model of choice to use. Thus motivated, we visit the following problem:
For a `generic' model of consumer choice (namely, distributions over preference
lists) and a limited amount of data on how consumers actually make decisions
(such as marginal information about these distributions), how may one predict
revenues from offering a particular assortment of choices? We present a
framework to answer such questions and design a number of tractable algorithms
from a data and computational standpoint for the same. This paper thus takes a
significant step towards `automating' the crucial task of choice model
selection in the context of operational decision problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.0264</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.0264</id><created>2009-10-01</created><updated>2012-06-19</updated><authors><author><keyname>Baigorri</keyname><forenames>A. R.</forenames></author><author><keyname>Goncalves</keyname><forenames>C. R.</forenames></author><author><keyname>Resende</keyname><forenames>P. A. A.</forenames></author></authors><title>Markov Chain Order Estimation and Relative Entropy</title><categories>math.ST stat.ME stat.TH</categories><comments>Revised for better and shorter proof, new numerical simulations as
  well as improved references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use the $f-divergence$ also called relative entropy as a measure of
diversity between probability densities and review its basic properties. In the
sequence we define a few objects which capture relevant information from the
sample of a Markov Chain to be used in the definition of a couple of estimators
i.e. the Local Dependency Level and Global Dependency Level for a Markov chain
sample. After exploring their properties we propose a new estimator for the
Markov chain order. Finally we show a few tables containing numerical
simulation results, comparing the performance of the new estimator with the
well known and already established AIC and BIC estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.0343</identifier>
 <datestamp>2010-10-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.0343</id><created>2009-10-02</created><updated>2010-10-19</updated><authors><author><keyname>Drees</keyname><forenames>Holger</forenames></author><author><keyname>Rootzn</keyname><forenames>Holger</forenames></author></authors><title>Limit theorems for empirical processes of cluster functionals</title><categories>math.ST math.PR stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS788 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS788</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 4, 2145-2186</journal-ref><doi>10.1214/09-AOS788</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $(X_{n,i})_{1\le i\le n,n\in\mathbb{N}}$ be a triangular array of
row-wise stationary $\mathbb{R}^d$-valued random variables. We use a "blocks
method" to define clusters of extreme values: the rows of $(X_{n,i})$ are
divided into $m_n$ blocks $(Y_{n,j})$, and if a block contains at least one
extreme value, the block is considered to contain a cluster. The cluster starts
at the first extreme value in the block and ends at the last one. The main
results are uniform central limit theorems for empirical processes
$Z_n(f):=\frac{1}{\sqrt {nv_n}}\sum_{j=1}^{m_n}(f(Y_{n,j})-Ef(Y_{n,j})),$ for
$v_n=P\{X_{n,i}\neq0\}$ and $f$ belonging to classes of cluster functionals,
that is, functions of the blocks $Y_{n,j}$ which only depend on the cluster
values and which are equal to 0 if $Y_{n,j}$ does not contain a cluster.
Conditions for finite-dimensional convergence include $\beta$-mixing, suitable
Lindeberg conditions and convergence of covariances. To obtain full uniform
convergence, we use either "bracketing entropy" or bounds on covering numbers
with respect to a random semi-metric. The latter makes it possible to bring the
powerful Vapnik--\v{C}ervonenkis theory to bear. Applications include
multivariate tail empirical processes and empirical processes of cluster values
and of order statistics in clusters. Although our main field of applications is
the analysis of extreme values, the theory can be applied more generally to
rare events occurring, for example, in nonparametric curve estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.0544</identifier>
 <datestamp>2011-07-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.0544</id><created>2009-10-04</created><updated>2011-07-18</updated><authors><author><keyname>Yu</keyname><forenames>Yaming</forenames></author></authors><title>Some stochastic inequalities for weighted sums</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ302 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ302</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 3, 1044-1053</journal-ref><doi>10.3150/10-BEJ302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare weighted sums of i.i.d. positive random variables according to the
usual stochastic order. The main inequalities are derived using majorization
techniques under certain log-concavity assumptions. Specifically, let $Y_i$ be
i.i.d. random variables on $\mathbf{R}_+$. Assuming that $\log Y_i$ has a
log-concave density, we show that $\sum a_iY_i$ is stochastically smaller than
$\sum b_iY_i$, if $(\log a_1,...,\log a_n)$ is majorized by $(\log b_1,...,\log
b_n)$. On the other hand, assuming that $Y_i^p$ has a log-concave density for
some $p&gt;1$, we show that $\sum a_iY_i$ is stochastically larger than $\sum
b_iY_i$, if $(a_1^q,...,a_n^q)$ is majorized by $(b_1^q,...,b_n^q)$, where
$p^{-1}+q^{-1}=1$. These unify several stochastic ordering results for specific
distributions. In particular, a conjecture of Hitczenko [Sankhy\={a} A 60
(1998) 171--175] on Weibull variables is proved. Potential applications in
reliability and wireless communications are mentioned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.0610</identifier>
 <datestamp>2010-10-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.0610</id><created>2009-10-04</created><updated>2010-10-17</updated><authors><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Regularization Techniques for Learning with Matrices</title><categories>cs.LG stat.ML</categories><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is growing body of learning problems for which it is natural to
organize the parameters into matrix, so as to appropriately regularize the
parameters under some matrix norm (in order to impose some more sophisticated
prior knowledge). This work describes and analyzes a systematic method for
constructing such matrix-based, regularization methods. In particular, we focus
on how the underlying statistical properties of a given problem can help us
decide which regularization function is appropriate.
  Our methodology is based on the known duality fact: that a function is
strongly convex with respect to some norm if and only if its conjugate function
is strongly smooth with respect to the dual norm. This result has already been
found to be a key component in deriving and analyzing several learning
algorithms. We demonstrate the potential of this framework by deriving novel
generalization and regret bounds for multi-task learning, multi-class learning,
and kernel learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.0722</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.0722</id><created>2009-10-05</created><authors><author><keyname>van de Geer</keyname><forenames>Sara A.</forenames></author><author><keyname>Bhlmann</keyname><forenames>Peter</forenames></author></authors><title>On the conditions used to prove oracle results for the Lasso</title><categories>math.ST stat.ML stat.TH</categories><comments>33 pages, 1 figure</comments><journal-ref>Electronic Journal of Statistics, 3, (2009), 1360-1392</journal-ref><doi>10.1214/09-EJS506</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oracle inequalities and variable selection properties for the Lasso in linear
models have been established under a variety of different assumptions on the
design matrix. We show in this paper how the different conditions and concepts
relate to each other. The restricted eigenvalue condition (Bickel et al., 2009)
or the slightly weaker compatibility condition (van de Geer, 2007) are
sufficient for oracle results. We argue that both these conditions allow for a
fairly general class of design matrices. Hence, optimality of the Lasso for
prediction and estimation holds for more general situations than what it
appears from coherence (Bunea et al, 2007b,c) or restricted isometry (Candes
and Tao, 2005) assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.0745</identifier>
 <datestamp>2012-10-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.0745</id><created>2009-10-05</created><authors><author><keyname>Bickel</keyname><forenames>David R.</forenames></author></authors><title>Estimating the null distribution for conditional inference and
  genome-scale screening</title><categories>stat.ME math.ST stat.TH</categories><journal-ref>D. R. Bickel, Estimating the null distribution to adjust observed
  confidence levels for genome-scale screening, Biometrics 67, 363-370 (2011)</journal-ref><doi>10.1111/j.1541-0420.2010.01491.x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a novel approach to the multiple testing problem, Efron (2004; 2007)
formulated estimators of the distribution of test statistics or nominal
p-values under a null distribution suitable for modeling the data of thousands
of unaffected genes, non-associated single-nucleotide polymorphisms, or other
biological features. Estimators of the null distribution can improve not only
the empirical Bayes procedure for which it was originally intended, but also
many other multiple comparison procedures. Such estimators serve as the
groundwork for the proposed multiple comparison procedure based on a recent
frequentist method of minimizing posterior expected loss, exemplified with a
non-additive loss function designed for genomic screening rather than for
validation.
  The merit of estimating the null distribution is examined from the vantage
point of conditional inference in the remainder of the paper. In a simulation
study of genome-scale multiple testing, conditioning the observed confidence
level on the estimated null distribution as an approximate ancillary statistic
markedly improved conditional inference. To enable researchers to determine
whether to rely on a particular estimated null distribution for inference or
decision making, an information-theoretic score is provided that quantifies the
benefit of conditioning. As the sum of the degree of ancillarity and the degree
of inferential relevance, the score reflects the balance conditioning would
strike between the two conflicting terms.
  Applications to gene expression microarray data illustrate the methods
introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.0827</identifier>
 <datestamp>2010-06-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.0827</id><created>2009-10-05</created><updated>2010-05-31</updated><authors><author><keyname>Bianchi</keyname><forenames>Pascal</forenames><affiliation>LTCI</affiliation></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames><affiliation>Chaire Radio Flexible</affiliation></author><author><keyname>Mada</keyname><forenames>Mylne</forenames><affiliation>LM-Orsay</affiliation></author><author><keyname>Najim</keyname><forenames>Jamal</forenames><affiliation>LTCI</affiliation></author></authors><title>Performance of Statistical Tests for Single Source Detection using
  Random Matrix Theory</title><categories>math.PR cs.IT math.IT math.ST stat.TH</categories><comments>45 p. improved presentation; more proofs provided</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a unified framework for the detection of a source with
a sensor array in the context where the noise variance and the channel between
the source and the sensors are unknown at the receiver. The Generalized Maximum
Likelihood Test is studied and yields the analysis of the ratio between the
maximum eigenvalue of the sampled covariance matrix and its normalized trace.
Using recent results of random matrix theory, a practical way to evaluate the
threshold and the $p$-value of the test is provided in the asymptotic regime
where the number $K$ of sensors and the number $N$ of observations per sensor
are large but have the same order of magnitude. The theoretical performance of
the test is then analyzed in terms of Receiver Operating Characteristic (ROC)
curve. It is in particular proved that both Type I and Type II error
probabilities converge to zero exponentially as the dimensions increase at the
same rate, and closed-form expressions are provided for the error exponents.
These theoretical results rely on a precise description of the large deviations
of the largest eigenvalue of spiked random matrix models, and establish that
the presented test asymptotically outperforms the popular test based on the
condition number of the sampled covariance matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.0895</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.0895</id><created>2009-10-06</created><updated>2011-06-19</updated><authors><author><keyname>Jagabathula</keyname><forenames>Srikanth</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author></authors><title>Inferring Rankings Using Constrained Sensing</title><categories>math.ST stat.TH</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering a function over the space of
permutations (or, the symmetric group) over $n$ elements from given partial
information; the partial information we consider is related to the group
theoretic Fourier Transform of the function. This problem naturally arises in
several settings such as ranked elections, multi-object tracking, ranking
systems, and recommendation systems. Inspired by the work of Donoho and Stark
in the context of discrete-time functions, we focus on non-negative functions
with a sparse support (support size $\ll$ domain size). Our recovery method is
based on finding the sparsest solution (through $\ell_0$ optimization) that is
consistent with the available information. As the main result, we derive
sufficient conditions for functions that can be recovered exactly from partial
information through $\ell_0$ optimization. Under a natural random model for the
generation of functions, we quantify the recoverability conditions by deriving
bounds on the sparsity (support size) for which the function satisfies the
sufficient conditions with a high probability as $n \to \infty$. $\ell_0$
optimization is computationally hard. Therefore, the popular compressive
sensing literature considers solving the convex relaxation, $\ell_1$
optimization, to find the sparsest solution. However, we show that $\ell_1$
optimization fails to recover a function (even with constant sparsity)
generated using the random model with a high probability as $n \to \infty$. In
order to overcome this problem, we propose a novel iterative algorithm for the
recovery of functions that satisfy the sufficient conditions. Finally, using an
Information Theoretic framework, we study necessary conditions for exact
recovery to be possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.0936</identifier>
 <datestamp>2011-01-17</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.0936</id><created>2009-10-06</created><authors><author><keyname>Ingster</keyname><forenames>Yuri I.</forenames></author><author><keyname>Sapatinas</keyname><forenames>Theofanis</forenames></author></authors><title>Minimax Goodness-of-Fit Testing in Multivariate Nonparametric Regression</title><categories>math.ST stat.TH</categories><comments>36 pages (to appear in: Mathematical Methods of Statistics)</comments><msc-class>62G08, 62G10, 62G20</msc-class><journal-ref>Mathematical Methods of Statistics, Vol. 18, 241-269 (2009)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an unknown response function $f$ defined on $\Delta=[0,1]^d$,
$1\le d\le\infty$, taken at $n$ random uniform design points and observed with
Gaussian noise of known variance. Given a positive sequence $r_n\to 0$ as
$n\to\infty$ and a known function $f_0 \in L_2(\Delta)$, we propose, under
general conditions, a unified framework for the goodness-of-fit testing problem
for testing the null hypothesis $H_0: f=f_0$ against the alternative $H_1:
f\in\CF, \|f-f_0\|\ge r_n$, where $\CF$ is an ellipsoid in the Hilbert space $
L_2(\Delta)$ with respect to the tensor product Fourier basis and $\|\cdot\|$
is the norm in $ L_2(\Delta)$. We obtain both rate and sharp asymptotics for
the error probabilities in the minimax setup. The derived tests are inherently
non-adaptive.
  Several illustrative examples are presented. In particular, we consider
functions belonging to ellipsoids arising from the well-known multidimensional
Sobolev and tensor product Sobolev norms as well as from the less-known
Sloan-Wo$\rm\acute{z}$niakowski norm and a norm constructed from multivariable
analytic functions on the complex strip.
  Some extensions of the suggested minimax goodness-of-fit testing methodology,
covering the cases of general design schemes with a known product probability
density function, unknown variance, other basis functions and adaptivity of the
suggested tests, are also briefly discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.1022</identifier>
 <datestamp>2011-08-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.1022</id><created>2009-10-06</created><updated>2011-08-09</updated><authors><author><keyname>Blei</keyname><forenames>David M.</forenames></author><author><keyname>Frazier</keyname><forenames>Peter I.</forenames></author></authors><title>Distance Dependent Chinese Restaurant Processes</title><categories>stat.ML stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop the distance dependent Chinese restaurant process (CRP), a
flexible class of distributions over partitions that allows for
non-exchangeability. This class can be used to model many kinds of dependencies
between data in infinite clustering models, including dependencies across time
or space. We examine the properties of the distance dependent CRP, discuss its
connections to Bayesian nonparametric mixture models, and derive a Gibbs
sampler for both observed and mixture settings. We study its performance with
three text corpora. We show that relaxing the assumption of exchangeability
with distance dependent CRPs can provide a better fit to sequential data. We
also show its alternative formulation of the traditional CRP leads to a
faster-mixing Gibbs sampling algorithm than the one based on the original
formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.1452</identifier>
 <datestamp>2010-10-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.1452</id><created>2009-10-08</created><updated>2010-06-01</updated><authors><author><keyname>Marin</keyname><forenames>Jean-Michel</forenames></author><author><keyname>Robert</keyname><forenames>Christian</forenames></author></authors><title>On resolving the Savage-Dickey paradox</title><categories>math.ST math.PR stat.CO stat.TH</categories><comments>Accepted version of the paper to appear in the Electronic Journal of
  Statistics, 9 pages, one figure</comments><journal-ref>Electron. J. Statist. Volume 4 (2010), 643-654</journal-ref><doi>10.1214/10-EJS564</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Savage-Dickey ratio is known as a specialised representation of the Bayes
factor (O'Hagan and Forster, 2004) that allows for a functional plugging
approximation of this quantity. We demonstrate here that the Savage-Dickey
representation is in fact a generic representation of the Bayes factor that
relies on specific measure-theoretic versions of the densities involved in the
ratio, instead of a special identity imposing the above constraints on the
prior distributions. We completely clarify the measure-theoretic foundations of
the representation as well as the generalisation of Verdinelli and Wasserman
(1995) and propose a comparison of this new approximation with their version,
as well as with bridge sampling and Chib's approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.1610</identifier>
 <datestamp>2013-05-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.1610</id><created>2009-10-08</created><updated>2012-05-31</updated><authors><author><keyname>Petrovi</keyname><forenames>Sonja</forenames></author><author><keyname>Stokes</keyname><forenames>Erik</forenames></author></authors><title>Betti numbers of Stanley-Reisner rings determine hierarchical Markov
  degrees</title><categories>math.AC math.CO math.ST stat.TH</categories><comments>Section 6 outlines few open problems. (Final version, differs
  slightly then publication.) Version3 was a major revision: proved Conjecture
  from previous version for all simplicial complexes</comments><journal-ref>Journal of Algebraic Combinatorics, Volume 37, Issue 4 (2013),
  Page 667-682</journal-ref><doi>10.1007/s10801-012-0381-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are two seemingly unrelated ideals associated with a simplicial complex
\Delta. One is the Stanley-Reisner ideal I_\Delta, the monomial ideal generated
by minimal non-faces of \Delta, well-known in combinatorial commutative
algebra. The other is the toric ideal I_{M(\Delta)} of the facet subring of
\Delta, whose generators give a Markov basis for the hierarchical model defined
by \Delta, playing a prominent role in algebraic statistics.
  In this note we show that the complexity of the generators of I_{M(\Delta)}
is determined by the Betti numbers of I_\Delta. The unexpected connection
between the syzygies of the Stanley-Reisner ideal and degrees of minimal
generators of the toric ideal provide a framework for further exploration of
the connection between the model and its many relatives in algebra and
combinatorics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.1723</identifier>
 <datestamp>2010-04-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.1723</id><created>2009-10-09</created><updated>2009-12-09</updated><authors><author><keyname>Charbonnier</keyname><forenames>Camille</forenames></author><author><keyname>Chiquet</keyname><forenames>Julien</forenames></author><author><keyname>Ambroise</keyname><forenames>Christophe</forenames></author></authors><title>Weighted-Lasso for Structured Network Inference from Time Course Data</title><categories>stat.AP</categories><journal-ref>Statistical Applications in Genetics and Molecular Biology: Vol. 9
  : Iss. 1, Article 15, 2010.</journal-ref><doi>10.2202/1544-6115.1519</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a weighted-Lasso method to infer the parameters of a first-order
vector auto-regressive model that describes time course expression data
generated by directed gene-to-gene regulation networks. These networks are
assumed to own a prior internal structure of connectivity which drives the
inference method. This prior structure can be either derived from prior
biological knowledge or inferred by the method itself. We illustrate the
performance of this structure-based penalization both on synthetic data and on
two canonical regulatory networks, first yeast cell cycle regulation network by
analyzing Spellman et al's dataset and second E. coli S.O.S. DNA repair network
by analysing U. Alon's lab data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.2034</identifier>
 <datestamp>2010-11-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.2034</id><created>2009-10-11</created><updated>2010-11-10</updated><authors><author><keyname>Zanghi</keyname><forenames>Hugo</forenames></author><author><keyname>Picard</keyname><forenames>Franck</forenames></author><author><keyname>Miele</keyname><forenames>Vincent</forenames></author><author><keyname>Ambroise</keyname><forenames>Christophe</forenames></author></authors><title>Strategies for online inference of model-based clustering in large and
  growing networks</title><categories>stat.AP cs.LG</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOAS359 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS359</report-no><journal-ref>Annals of Applied Statistics 2010, Vol. 4, No. 2, 687-714</journal-ref><doi>10.1214/10-AOAS359</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we adapt online estimation strategies to perform model-based
clustering on large networks. Our work focuses on two algorithms, the first
based on the SAEM algorithm, and the second on variational methods. These two
strategies are compared with existing approaches on simulated and real data. We
use the method to decipher the connexion structure of the political websphere
during the US political campaign in 2008. We show that our online EM-based
algorithms offer a good trade-off between precision and speed, when estimating
parameters for mixture distributions in the context of random graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.2042</identifier>
 <datestamp>2010-09-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.2042</id><created>2009-10-11</created><authors><author><keyname>Raskutti</keyname><forenames>Garvesh</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author><author><keyname>Yu</keyname><forenames>Bin</forenames></author></authors><title>Minimax rates of estimation for high-dimensional linear regression over
  $\ell_q$-balls</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>Presented in part at the Allerton Conference on Control,
  Communication and Computer, Monticello, IL, October 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the standard linear regression model $\y = \Xmat \betastar + w$,
where $\y \in \real^\numobs$ is an observation vector, $\Xmat \in
\real^{\numobs \times \pdim}$ is a design matrix, $\betastar \in \real^\pdim$
is the unknown regression vector, and $w \sim \mathcal{N}(0, \sigma^2 I)$ is
additive Gaussian noise. This paper studies the minimax rates of convergence
for estimation of $\betastar$ for $\ell_\rpar$-losses and in the
$\ell_2$-prediction loss, assuming that $\betastar$ belongs to an
$\ell_{\qpar}$-ball $\Ballq(\myrad)$ for some $\qpar \in [0,1]$. We show that
under suitable regularity conditions on the design matrix $\Xmat$, the minimax
error in $\ell_2$-loss and $\ell_2$-prediction loss scales as $\Rq
\big(\frac{\log \pdim}{n}\big)^{1-\frac{\qpar}{2}}$. In addition, we provide
lower bounds on minimax risks in $\ell_{\rpar}$-norms, for all $\rpar \in [1,
+\infty], \rpar \neq \qpar$. Our proofs of the lower bounds are
information-theoretic in nature, based on Fano's inequality and results on the
metric entropy of the balls $\Ballq(\myrad)$, whereas our proofs of the upper
bounds are direct and constructive, involving direct analysis of least-squares
over $\ell_{\qpar}$-balls. For the special case $q = 0$, a comparison with
$\ell_2$-risks achieved by computationally efficient $\ell_1$-relaxations
reveals that although such methods can achieve the minimax rates up to constant
factors, they require slightly stronger assumptions on the design matrix
$\Xmat$ than algorithms involving least-squares over the $\ell_0$-ball.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.2145</identifier>
 <datestamp>2011-01-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.2145</id><created>2009-10-12</created><updated>2011-01-07</updated><authors><author><keyname>Meinshausen</keyname><forenames>Nicolai</forenames></author></authors><title>Node harvest</title><categories>stat.ML stat.AP stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOAS367 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS367</report-no><journal-ref>Annals of Applied Statistics 2010, Vol. 4, No. 4, 2049-2072</journal-ref><doi>10.1214/10-AOAS367</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When choosing a suitable technique for regression and classification with
multivariate predictor variables, one is often faced with a tradeoff between
interpretability and high predictive accuracy. To give a classical example,
classification and regression trees are easy to understand and interpret. Tree
ensembles like Random Forests provide usually more accurate predictions. Yet
tree ensembles are also more difficult to analyze than single trees and are
often criticized, perhaps unfairly, as `black box' predictors. Node harvest is
trying to reconcile the two aims of interpretability and predictive accuracy by
combining positive aspects of trees and tree ensembles. Results are very sparse
and interpretable and predictive accuracy is extremely competitive, especially
for low signal-to-noise data. The procedure is simple: an initial set of a few
thousand nodes is generated randomly. If a new observation falls into just a
single node, its prediction is the mean response of all training observation
within this node, identical to a tree-like prediction. A new observation falls
typically into several nodes and its prediction is then the weighted average of
the mean responses across all these nodes. The only role of node harvest is to
`pick' the right nodes from the initial large ensemble of nodes by choosing
node weights, which amounts in the proposed algorithm to a quadratic
programming problem with linear inequality constraints. The solution is sparse
in the sense that only very few nodes are selected with a nonzero weight. This
sparsity is not explicitly enforced. Maybe surprisingly, it is not necessary to
select a tuning parameter for optimal predictive accuracy. Node harvest can
handle mixed data and missing values and is shown to be simple to interpret and
competitive in predictive accuracy on a variety of data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.2497</identifier>
 <datestamp>2010-08-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.2497</id><created>2009-10-13</created><updated>2010-08-06</updated><authors><author><keyname>Barvinok</keyname><forenames>Alexander</forenames></author><author><keyname>Hartigan</keyname><forenames>J. A.</forenames></author></authors><title>Maximum entropy Edgeworth estimates of the number of integer points in
  polytopes</title><categories>stat.ME stat.CO</categories><comments>29 pages 3 tables Revision updates references,and sharpens statement
  and proof of theorem 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstract: The number of points $x=(x_1 ,x_2 ,...x_n)$ that lie in an integer
cube $C$ in $R^n$ and satisfy the constraints $\sum_j h_{ij}(x_j )=s_i ,1\le
i\le d$ is approximated by an Edgeworth-corrected Gaussian formula based on the
maximum entropy density $p$ on $x \in C$, that satisfies $E\sum_j h_{ij}(x_j
)=s_i ,1\le i\le d$. Under $p$, the variables $X_1 ,X_2 ,...X_n $ are
independent with densities of exponential form. Letting $S_i$ denote the random
variable $\sum_j h_{ij}(X_j )$, conditional on $S=s, X$ is uniformly
distributed over the integers in $C$ that satisfy $S=s$. The number of points
in $C$ satisfying $S=s$ is $p \{S=s\}\exp (I(p))$ where $I(p)$ is the entropy
of the density $p$. We estimate $p \{S=s\}$ by $p_Z(s)$, the density at $s$ of
the multivariate Gaussian $Z$ with the same first two moments as $S$; and when
$d$ is large we use in addition an Edgeworth factor that requires the first
four moments of $S$ under $p$. The asymptotic validity of the
Edgeworth-corrected estimate is proved and demonstrated for counting
contingency tables with given row and column sums as the number of rows and
columns approaches infinity, and demonstrated for counting the number of graphs
with a given degree sequence, as the number of vertices approaches infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.2585</identifier>
 <datestamp>2010-10-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.2585</id><created>2009-10-14</created><updated>2010-10-07</updated><authors><author><keyname>Murphy</keyname><forenames>Thomas Brendan</forenames></author><author><keyname>Dean</keyname><forenames>Nema</forenames></author><author><keyname>Raftery</keyname><forenames>Adrian E.</forenames></author></authors><title>Variable selection and updating in model-based discriminant analysis for
  high dimensional data with food authenticity applications</title><categories>stat.ME stat.AP</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOAS279 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS279</report-no><journal-ref>Annals of Applied Statistics 2010, Vol. 4, No. 1, 396-421</journal-ref><doi>10.1214/09-AOAS279</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Food authenticity studies are concerned with determining if food samples have
been correctly labeled or not. Discriminant analysis methods are an integral
part of the methodology for food authentication. Motivated by food authenticity
applications, a model-based discriminant analysis method that includes variable
selection is presented. The discriminant analysis model is fitted in a
semi-supervised manner using both labeled and unlabeled data. The method is
shown to give excellent classification performance on several high-dimensional
multiclass food authenticity data sets with more variables than observations.
The variables selected by the proposed method provide information about which
variables are meaningful for classification purposes. A headlong search
strategy for variable selection is shown to be efficient in terms of
computation and achieves excellent classification performance. In applications
to several food authenticity data sets, our proposed method outperformed
default implementations of Random Forests, AdaBoost, transductive SVMs and
Bayesian Multinomial Regression by substantial margins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.3070</identifier>
 <datestamp>2011-02-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.3070</id><created>2009-10-16</created><updated>2011-02-10</updated><authors><author><keyname>Crambes</keyname><forenames>Christophe</forenames><affiliation>I3M</affiliation></author><author><keyname>Mas</keyname><forenames>Andr</forenames><affiliation>I3M</affiliation></author></authors><title>Asymptotics of prediction in functional linear regression with
  functional outputs</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study prediction in the functional linear model with functional outputs :
$Y=SX+\epsilon $ where the covariates $X$ and $Y$ belong to some functional
space and $S$ is a linear operator. We provide the asymptotic mean square
prediction error with exact constants for our estimator which is based on
functional PCA of the input and has a classical form. As a consequence we
derive the optimal choice of the dimension $k_{n}$ of the projection space. The
rates we obtain are optimal in minimax sense and generalize those found when
the output is real. Our main results hold with no prior assumptions on the rate
of decay of the eigenvalues of the input. This allows to consider a wide class
of parameters and inputs $X(\cdot) $ that may be either very irregular or very
smooth. We also prove a central limit theorem for the predictor which improves
results by Cardot, Mas and Sarda (2007) in the simpler model with scalar
outputs. We show that, due to the underlying inverse problem, the bare estimate
cannot converge in distribution for the norm of the function space
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.3088</identifier>
 <datestamp>2010-06-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.3088</id><created>2009-10-16</created><updated>2010-05-31</updated><authors><author><keyname>Breton</keyname><forenames>Jean-Christophe</forenames><affiliation>MIA</affiliation></author><author><keyname>Coeurjolly</keyname><forenames>Jean-Franois</forenames><affiliation>LJK, GIPSA-lab</affiliation></author></authors><title>Confidence intervals for the Hurst parameter of a fractional Brownian
  motion based on finite sample size</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show how concentration inequalities for Gaussian quadratic
form can be used to propose exact confidence intervals of the Hurst index
parametrizing a fractional Brownian motion. Both cases where the scaling
parameter of the fractional Brownian motion is known or unknown are
investigated. These intervals are obtained by observing a single discretized
sample path of a fractional Brownian motion and without any assumption on the
parameter $H$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.3448</identifier>
 <datestamp>2011-02-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.3448</id><created>2009-10-18</created><updated>2011-02-10</updated><authors><author><keyname>Gordin</keyname><forenames>Mikhail</forenames></author><author><keyname>Peligrad</keyname><forenames>Magda</forenames></author></authors><title>On the functional central limit theorem via martingale approximation</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ276 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ276</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 1, 424-440</journal-ref><doi>10.3150/10-BEJ276</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop necessary and sufficient conditions for the
validity of a martingale approximation for the partial sums of a stationary
process in terms of the maximum of consecutive errors. Such an approximation is
useful for transferring the conditional functional central limit theorem from
the martingale to the original process. The condition found is simple and well
adapted to a variety of examples, leading to a better understanding of the
structure of several stochastic processes and their asymptotic behaviors. The
approximation brings together many disparate examples in probability theory. It
is valid for classes of variables defined by familiar projection conditions
such as the Maxwell--Woodroofe condition, various classes of mixing processes,
including the large class of strongly mixing processes, and for additive
functionals of Markov chains with normal or symmetric Markov operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.4337</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.4337</id><created>2009-10-22</created><authors><author><keyname>van Es</keyname><forenames>Bert</forenames></author><author><keyname>Spreij</keyname><forenames>Peter</forenames></author></authors><title>Multivariate Nonparametric Volatility Density Estimation</title><categories>math.ST stat.TH</categories><msc-class>62G07, 62M07, 62P20</msc-class><journal-ref>Journal of Multivariate Analysis 102, 683-697 (2011)</journal-ref><doi>10.1016/j.jmva.2010.12.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a continuous-time stochastic volatility model. The model contains
a stationary volatility process, the multivariate density of the finite
dimensional distributions of which we aim to estimate. We assume that we
observe the process at discrete instants in time. The sampling times will be
equidistant with vanishing distance.
  A multivariate Fourier-type deconvolution kernel density estimator based on
the logarithm of the squared processes is proposed to estimate the multivariate
volatility density. An expansion of the bias and a bound on the variance are
derived.
  Key words: stochastic volatility models, multivariate density estimation,
kernel estimator, deconvolution, mixing
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.4397</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.4397</id><created>2009-10-22</created><updated>2013-06-25</updated><authors><author><keyname>Nowak</keyname><forenames>Robert D.</forenames></author></authors><title>The Geometry of Generalized Binary Search</title><categories>stat.ML cs.IT math.IT math.ST stat.TH</categories><comments>corrected typo in Thm 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of determining a binary-valued function
through a sequence of strategically selected queries. The focus is an algorithm
called Generalized Binary Search (GBS). GBS is a well-known greedy algorithm
for determining a binary-valued function through a sequence of strategically
selected queries. At each step, a query is selected that most evenly splits the
hypotheses under consideration into two disjoint subsets, a natural
generalization of the idea underlying classic binary search. This paper
develops novel incoherence and geometric conditions under which GBS achieves
the information-theoretically optimal query complexity; i.e., given a
collection of N hypotheses, GBS terminates with the correct function after no
more than a constant times log N queries. Furthermore, a noise-tolerant version
of GBS is developed that also achieves the optimal query complexity. These
results are applied to learning halfspaces, a problem arising routinely in
image processing and machine learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.4472</identifier>
 <datestamp>2010-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.4472</id><created>2009-10-23</created><updated>2010-01-19</updated><authors><author><keyname>Toni</keyname><forenames>Tina</forenames></author><author><keyname>Stumpf</keyname><forenames>Michael P. H.</forenames></author></authors><title>Tutorial on ABC rejection and ABC SMC for parameter estimation and model
  selection</title><categories>stat.CO stat.ME</categories><comments>This tutorial forms a part of the supplementary material of the paper
  "T. Toni, M. P. H. Stumpf, Simulation-based model selection for dynamical
  systems in systems and population biology, Bioinformatics, 2009 (in press)"</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this tutorial we schematically illustrate four algorithms:
  (1) ABC rejection for parameter estimation
  (2) ABC SMC for parameter estimation
  (3) ABC rejection for model selection on the joint space
  (4) ABC SMC for model selection on the joint space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.4636</identifier>
 <datestamp>2011-05-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.4636</id><created>2009-10-24</created><updated>2011-05-10</updated><authors><author><keyname>Lember</keyname><forenames>J.</forenames></author></authors><title>On approximation of smoothing probabilities for hidden Markov models</title><categories>stat.ML math.ST stat.TH</categories><comments>submitted to Statistics and Probability Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the smoothing probabilities of hidden Markov model (HMM). We show
that under fairly general conditions for HMM, the exponential forgetting still
holds, and the smoothing probabilities can be well approximated with the ones
of double sided HMM. This makes it possible to use ergodic theorems. As an
applications we consider the pointwise maximum a posteriori segmentation, and
show that the corresponding risks converge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.5185</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.5185</id><created>2009-10-27</created><authors><author><keyname>van Es</keyname><forenames>Bert</forenames></author><author><keyname>Spreij</keyname><forenames>Peter</forenames></author><author><keyname>van Zanten</keyname><forenames>Harry</forenames></author></authors><title>Nonparametric methods for volatility density estimation</title><categories>stat.ME math.ST q-fin.ST stat.TH</categories><msc-class>62G07, 62G08, 62M07, 62P20, 91G70</msc-class><journal-ref>Advanced Mathematical Methods for Finance, Chapter 11, 293-312,
  Giulia di Nunno, Bernt {\O}ksendal Eds., Springer (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic volatility modelling of financial processes has become
increasingly popular. The proposed models usually contain a stationary
volatility process. We will motivate and review several nonparametric methods
for estimation of the density of the volatility process. Both models based on
discretely sampled continuous time processes and discrete time models will be
discussed.
  The key insight for the analysis is a transformation of the volatility
density estimation problem to a deconvolution model for which standard methods
exist. Three type of nonparametric density estimators are reviewed: the
Fourier-type deconvolution kernel density estimator, a wavelet deconvolution
density estimator and a penalized projection estimator. The performance of
these estimators will be compared. Key words: stochastic volatility models,
deconvolution, density estimation, kernel estimator, wavelets, minimum contrast
estimation, mixing
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.5193</identifier>
 <datestamp>2012-08-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.5193</id><created>2009-10-27</created><updated>2012-08-13</updated><authors><author><keyname>Vardar</keyname><forenames>Ceren</forenames></author></authors><title>Results on the Supremum of Fractional Brownian Motion</title><categories>math.PR math.ST stat.TH</categories><comments>14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the distribution of the square of the supremum of reflected
fractional Brownian motion up to time a, with Hurst parameter-H greater than
1/2, is related to the distribution of its hitting time to level $1,$ using the
self similarity property of fractional Brownian motion. It is also proven that
second moment of supremum of reflected fractional Brownian motion up to time
$a$ is bounded above by $a^{2H}.$ Similar relations are obtained for the
supremum of fractional Brownian motion with Hurst parameter greater than 1/2,
and its hitting time to level $1.$ What is more, we obtain an upper bound on
the complementary probability distribution of the supremum of fractional
Brownian motion and reflected fractional Brownian motion up to time a, using
Jensen's and Markov's inequalities. A sharper bound is observed on the
distribution of the supremum of fractional Brownian motion by the properties of
Gamma distribution. Finally, applications of the given results to the financial
markets are investigated and partial results are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0910.5454</identifier>
 <datestamp>2010-01-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0910.5454</id><created>2009-10-28</created><authors><author><keyname>McGuire</keyname><forenames>P. C.</forenames></author><author><keyname>Gross</keyname><forenames>C.</forenames></author><author><keyname>Wendt</keyname><forenames>L.</forenames></author><author><keyname>Bonnici</keyname><forenames>A.</forenames></author><author><keyname>Souza-Egipsy</keyname><forenames>V.</forenames></author><author><keyname>Ormo</keyname><forenames>J.</forenames></author><author><keyname>Diaz-Martinez</keyname><forenames>E.</forenames></author><author><keyname>Foing</keyname><forenames>B. H.</forenames></author><author><keyname>Bose</keyname><forenames>R.</forenames></author><author><keyname>Walter</keyname><forenames>S.</forenames></author><author><keyname>Oesker</keyname><forenames>M.</forenames></author><author><keyname>Ontrup</keyname><forenames>J.</forenames></author><author><keyname>Haschke</keyname><forenames>R.</forenames></author><author><keyname>Ritter</keyname><forenames>H.</forenames></author></authors><title>The Cyborg Astrobiologist: Testing a Novelty-Detection Algorithm on Two
  Mobile Exploration Systems at Rivas Vaciamadrid in Spain and at the Mars
  Desert Research Station in Utah</title><categories>cs.CV astro-ph.EP astro-ph.IM cs.LG stat.ML</categories><comments>28 pages, 12 figures, accepted for publication in the International
  Journal of Astrobiology</comments><journal-ref>International Journal of Astrobiology, Vol. 9, pp. 11-27 (2010).</journal-ref><doi>10.1017/S1473550409990358</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  (ABRIDGED) In previous work, two platforms have been developed for testing
computer-vision algorithms for robotic planetary exploration (McGuire et al.
2004b,2005; Bartolo et al. 2007). The wearable-computer platform has been
tested at geological and astrobiological field sites in Spain (Rivas
Vaciamadrid and Riba de Santiuste), and the phone-camera has been tested at a
geological field site in Malta. In this work, we (i) apply a Hopfield
neural-network algorithm for novelty detection based upon color, (ii) integrate
a field-capable digital microscope on the wearable computer platform, (iii)
test this novelty detection with the digital microscope at Rivas Vaciamadrid,
(iv) develop a Bluetooth communication mode for the phone-camera platform, in
order to allow access to a mobile processing computer at the field sites, and
(v) test the novelty detection on the Bluetooth-enabled phone-camera connected
to a netbook computer at the Mars Desert Research Station in Utah. This systems
engineering and field testing have together allowed us to develop a real-time
computer-vision system that is capable, for example, of identifying lichens as
novel within a series of images acquired in semi-arid desert environments. We
acquired sequences of images of geologic outcrops in Utah and Spain consisting
of various rock types and colors to test this algorithm. The algorithm robustly
recognized previously-observed units by their color, while requiring only a
single image or a few images to learn colors as familiar, demonstrating its
fast learning capability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.0108</identifier>
 <datestamp>2011-08-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.0108</id><created>2009-10-31</created><updated>2010-05-19</updated><authors><author><keyname>Yu</keyname><forenames>Yaming</forenames></author></authors><title>D-optimal designs via a cocktail algorithm</title><categories>stat.CO stat.ME</categories><comments>A number of changes after accounting for the referees' comments
  including new examples in Section 4 and more detailed explanations throughout</comments><journal-ref>Statistics and Computing 21 (2011) 475-481</journal-ref><doi>10.1007/s11222-010-9183-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fast new algorithm is proposed for numerical computation of (approximate)
D-optimal designs. This "cocktail algorithm" extends the well-known vertex
direction method (VDM; Fedorov 1972) and the multiplicative algorithm (Silvey,
Titterington and Torsney, 1978), and shares their simplicity and monotonic
convergence properties. Numerical examples show that the cocktail algorithm can
lead to dramatically improved speed, sometimes by orders of magnitude, relative
to either the multiplicative algorithm or the vertex exchange method (a variant
of VDM). Key to the improved speed is a new nearest neighbor exchange strategy,
which acts locally and complements the global effect of the multiplicative
algorithm. Possible extensions to related problems such as nonparametric
maximum likelihood estimation are mentioned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.0280</identifier>
 <datestamp>2012-07-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.0280</id><created>2009-11-02</created><authors><author><keyname>Peters</keyname><forenames>Jonas</forenames></author><author><keyname>Janzing</keyname><forenames>Dominik</forenames></author><author><keyname>Schlkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Causal Inference on Discrete Data using Additive Noise Models</title><categories>stat.ML</categories><journal-ref>IEEE TPAMI vol. 33 no. 12 (2011) 2436-2450</journal-ref><doi>10.1109/TPAMI.2011.71</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inferring the causal structure of a set of random variables from a finite
sample of the joint distribution is an important problem in science. Recently,
methods using additive noise models have been suggested to approach the case of
continuous variables. In many situations, however, the variables of interest
are discrete or even have only finitely many states. In this work we extend the
notion of additive noise models to these cases. We prove that whenever the
joint distribution $\prob^{(X,Y)}$ admits such a model in one direction, e.g.
$Y=f(X)+N, N \independent X$, it does not admit the reversed model
$X=g(Y)+\tilde N, \tilde N \independent Y$ as long as the model is chosen in a
generic way. Based on these deliberations we propose an efficient new algorithm
that is able to distinguish between cause and effect for a finite sample of
discrete variables. In an extensive experimental study we show that this
algorithm works both on synthetic and real data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.0522</identifier>
 <datestamp>2011-02-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.0522</id><created>2009-11-03</created><authors><author><keyname>Vihola</keyname><forenames>Matti</forenames></author></authors><title>Can the Adaptive Metropolis Algorithm Collapse Without the Covariance
  Lower Bound?</title><categories>math.PR math.ST stat.CO stat.TH</categories><comments>31 pages, 1 figure</comments><msc-class>65C40, 60J27, 93E15, 93E35</msc-class><journal-ref>Electronic Journal of Probability, Vol 16, pp. 45-75, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Adaptive Metropolis (AM) algorithm is based on the symmetric random-walk
Metropolis algorithm. The proposal distribution has the following
time-dependent covariance matrix at step $n+1$ \[
  S_n = Cov(X_1,...,X_n) + \epsilon I, \] that is, the sample covariance matrix
of the history of the chain plus a (small) constant $\epsilon&gt;0$ multiple of
the identity matrix $I$. The lower bound on the eigenvalues of $S_n$ induced by
the factor $\epsilon I$ is theoretically convenient, but practically
cumbersome, as a good value for the parameter $\epsilon$ may not always be easy
to choose. This article considers variants of the AM algorithm that do not
explicitly bound the eigenvalues of $S_n$ away from zero. The behaviour of
$S_n$ is studied in detail, indicating that the eigenvalues of $S_n$ do not
tend to collapse to zero in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.0930</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.0930</id><created>2009-11-04</created><updated>2010-04-23</updated><authors><author><keyname>Minin</keyname><forenames>Vladimir N.</forenames></author><author><keyname>O'Brien</keyname><forenames>John D.</forenames></author><author><keyname>Seregin</keyname><forenames>Arseni</forenames></author></authors><title>Imputation Estimators Partially Correct for Model Misspecification</title><categories>stat.ME</categories><comments>major rewrite, beta-binomial example removed, model based clustering
  is added to the mixture model example, Bayesian approach is now illustrated
  with the genetics example</comments><doi>10.2202/1544-6115.1650</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference problems with incomplete observations often aim at estimating
population properties of unobserved quantities. One simple way to accomplish
this estimation is to impute the unobserved quantities of interest at the
individual level and then take an empirical average of the imputed values. We
show that this simple imputation estimator can provide partial protection
against model misspecification. We illustrate imputation estimators' robustness
to model specification on three examples: mixture model-based clustering,
estimation of genotype frequencies in population genetics, and estimation of
Markovian evolutionary distances. In the final example, using a representative
model misspecification, we demonstrate that in non-degenerate cases, the
imputation estimator dominates the plug-in estimate asymptotically. We conclude
by outlining a Bayesian implementation of the imputation-based estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.1164</identifier>
 <datestamp>2011-05-17</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.1164</id><created>2009-11-05</created><updated>2011-05-16</updated><authors><author><keyname>Atchad</keyname><forenames>Yves F.</forenames></author></authors><title>Kernel estimators of asymptotic variance for adaptive Markov chain Monte
  Carlo</title><categories>math.PR math.ST stat.CO stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS828 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS828</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 2, 990-1011</journal-ref><doi>10.1214/10-AOS828</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the asymptotic behavior of kernel estimators of asymptotic variances
(or long-run variances) for a class of adaptive Markov chains. The convergence
is studied both in $L^p$ and almost surely. The results also apply to Markov
chains and improve on the existing literature by imposing weaker conditions. We
illustrate the results with applications to the $\operatorname {GARCH}(1,1)$
Markov model and to an adaptive MCMC algorithm for Bayesian logistic
regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.1189</identifier>
 <datestamp>2011-04-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.1189</id><created>2009-11-06</created><updated>2010-09-23</updated><authors><author><keyname>Marrel</keyname><forenames>Amandine</forenames><affiliation>IFP</affiliation></author><author><keyname>Iooss</keyname><forenames>Bertrand</forenames><affiliation>Mthodes d'Analyse Stochastique des Codes et Traitements Numriques</affiliation></author><author><keyname>Jullien</keyname><forenames>Michel</forenames><affiliation>IMT</affiliation></author><author><keyname>Laurent</keyname><forenames>Beatrice</forenames><affiliation>IMT</affiliation></author><author><keyname>Volkova</keyname><forenames>Elena</forenames></author></authors><title>Global sensitivity analysis for models with spatially dependent outputs</title><categories>stat.CO math.ST stat.AP stat.TH</categories><proxy>ccsd</proxy><journal-ref>Environmentrics 22 (2011) 383-397</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The global sensitivity analysis of a complex numerical model often calls for
the estimation of variance-based importance measures, named Sobol' indices.
Metamodel-based techniques have been developed in order to replace the cpu
time-expensive computer code with an inexpensive mathematical function, which
predicts the computer code output. The common metamodel-based sensitivity
analysis methods are well-suited for computer codes with scalar outputs.
However, in the environmental domain, as in many areas of application, the
numerical model outputs are often spatial maps, which may also vary with time.
In this paper, we introduce an innovative method to obtain a spatial map of
Sobol' indices with a minimal number of numerical model computations. It is
based upon the functional decomposition of the spatial output onto a wavelet
basis and the metamodeling of the wavelet coefficients by the Gaussian process.
An analytical example is presented to clarify the various steps of our
methodology. This technique is then applied to a real hydrogeological case: for
each model input variable, a spatial map of Sobol' indices is thus obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.1318</identifier>
 <datestamp>2012-07-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.1318</id><created>2009-11-06</created><authors><author><keyname>Egghe</keyname><forenames>Leo</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>The relation between Pearson's correlation coefficient r and Salton's
  cosine measure</title><categories>cs.IR stat.ME</categories><journal-ref>Journal of the American Society for Information Science and
  Technology 60(5) (2009) 1027-1036</journal-ref><doi>10.1016/j.eswa.2012.07.016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relation between Pearson's correlation coefficient and Salton's cosine
measure is revealed based on the different possible values of the division of
the L1-norm and the L2-norm of a vector. These different values yield a sheaf
of increasingly straight lines which form together a cloud of points, being the
investigated relation. The theoretical results are tested against the author
co-citation relations among 24 informetricians for whom two matrices can be
constructed, based on co-citations: the asymmetric occurrence matrix and the
symmetric co-citation matrix. Both examples completely confirm the theoretical
results. The results enable us to specify an algorithm which provides a
threshold value for the cosine above which none of the corresponding Pearson
correlations would be negative. Using this threshold value can be expected to
optimize the visualization of the vector space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.1443</identifier>
 <datestamp>2010-07-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.1443</id><created>2009-11-07</created><updated>2010-06-30</updated><authors><author><keyname>Achibi</keyname><forenames>Mohamed</forenames><affiliation>LSTA</affiliation></author><author><keyname>Broniatowski</keyname><forenames>Michel</forenames><affiliation>LSTA</affiliation></author></authors><title>Bivariate Cox model and copulas</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new class of Cox models for dependent bivariate data.
The impact of the covariate on the dependence of the variables is captured
through the modification of their copula. Various classes of well known copulas
are stable under the model (archimedean type and extreme value copulas),
meaning that the role of the covariate acts in a simple and explicit way on the
copula in the class; specific parametric classes are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.1497</identifier>
 <datestamp>2011-12-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.1497</id><created>2009-11-08</created><updated>2011-12-13</updated><authors><author><keyname>Lerasle</keyname><forenames>Matthieu</forenames></author></authors><title>Optimal model selection for density estimation of stationary data under
  various mixing conditions</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/11-AOS888 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS888</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 4, 1852-1877</journal-ref><doi>10.1214/11-AOS888</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a block-resampling penalization method for marginal density
estimation with nonnecessary independent observations. When the data are
$\beta$ or $\tau$-mixing, the selected estimator satisfies oracle inequalities
with leading constant asymptotically equal to 1. We also prove in this setting
the slope heuristic, which is a data-driven method to optimize the leading
constant in the penalty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.1697</identifier>
 <datestamp>2011-08-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.1697</id><created>2009-11-09</created><updated>2010-04-18</updated><authors><author><keyname>Rudoy</keyname><forenames>Daniel</forenames></author><author><keyname>Quatieri</keyname><forenames>Thomas F.</forenames></author><author><keyname>Wolfe</keyname><forenames>Patrick J.</forenames></author></authors><title>Time-Varying Autoregressions in Speech: Detection Theory and
  Applications</title><categories>stat.AP</categories><comments>12 pages, 12 figures; revised version</comments><journal-ref>IEEE Transactions on Audio, Speech, and Language Processing, vol.
  19, pp. 977-989, 2011</journal-ref><doi>10.1109/TASL.2010.2073704</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article develops a general detection theory for speech analysis based on
time-varying autoregressive models, which themselves generalize the classical
linear predictive speech analysis framework. This theory leads to a
computationally efficient decision-theoretic procedure that may be applied to
detect the presence of vocal tract variation in speech waveform data. A
corresponding generalized likelihood ratio test is derived and studied both
empirically for short data records, using formant-like synthetic examples, and
asymptotically, leading to constant false alarm rate hypothesis tests for
changes in vocal tract configuration. Two in-depth case studies then serve to
illustrate the practical efficacy of this procedure across different time
scales of speech dynamics: first, the detection of formant changes on the scale
of tens of milliseconds of data, and second, the identification of glottal
opening and closing instants on time scales below ten milliseconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.1705</identifier>
 <datestamp>2010-01-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.1705</id><created>2009-11-09</created><updated>2010-01-20</updated><authors><author><keyname>Toni</keyname><forenames>Tina</forenames></author><author><keyname>Stumpf</keyname><forenames>Michael P. H.</forenames></author></authors><title>Simulation-based model selection for dynamical systems in systems and
  population biology</title><categories>q-bio.QM stat.CO</categories><comments>This article is in press in Bioinformatics, 2009. Advance Access is
  available on Bioinformatics webpage</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer simulations have become an important tool across the biomedical
sciences and beyond. For many important problems several different models or
hypotheses exist and choosing which one best describes reality or observed data
is not straightforward. We therefore require suitable statistical tools that
allow us to choose rationally between different mechanistic models of e.g.
signal transduction or gene regulation networks. This is particularly
challenging in systems biology where only a small number of molecular species
can be assayed at any given time and all measurements are subject to
measurement uncertainty. Here we develop such a model selection framework based
on approximate Bayesian computation and employing sequential Monte Carlo
sampling. We show that our approach can be applied across a wide range of
biological scenarios, and we illustrate its use on real data describing
influenza dynamics and the JAK-STAT signalling pathway. Bayesian model
selection strikes a balance between the complexity of the simulation models and
their ability to describe observed data. The present approach enables us to
employ the whole formal apparatus to any system that can be (efficiently)
simulated, even when exact likelihoods are computationally intractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.1768</identifier>
 <datestamp>2010-10-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.1768</id><created>2009-11-09</created><updated>2010-10-25</updated><authors><author><keyname>Scott</keyname><forenames>James G.</forenames></author></authors><title>Benchmarking Historical Corporate Performance</title><categories>stat.ME stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper uses Bayesian tree models for statistical benchmarking in data
sets with awkward marginals and complicated dependence structures. The method
is applied to a very large database on corporate performance over the last four
decades. The results of this study provide a formal basis for making
cross-peer-group comparisons among companies in very different industries and
operating environments. This is done by using models for Bayesian multiple
hypothesis testing to determine which firms, if any, have systematically
outperformed their peer groups over time. We conclude that systematic
outperformance, while it seems to exist, is quite rare worldwide.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.1777</identifier>
 <datestamp>2010-06-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.1777</id><created>2009-11-09</created><updated>2010-06-22</updated><authors><author><keyname>Weinberg</keyname><forenames>Martin D.</forenames></author></authors><title>Computing the Bayesian Factor from a Markov chain Monte Carlo Simulation
  of the Posterior Distribution</title><categories>astro-ph.IM astro-ph.CO math.ST stat.TH</categories><comments>34 pages, 10 figures, submitted to Bayesian Analysis, revised</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computation of the marginal likelihood from a simulated posterior
distribution is central to Bayesian model selection but is computationally
difficult. I argue that the marginal likelihood can be reliably computed from a
posterior sample by careful attention to the numerics of the probability
integral. Posing the expression for the marginal likelihood as a Lebesgue
integral, we may convert the harmonic mean approximation from a sample
statistic to a quadrature rule. As a quadrature, the harmonic mean
approximation suffers from enormous truncation error as consequence . In
addition, I demonstrate that the integral expression for the harmonic-mean
approximation converges slowly at best for high-dimensional problems with
uninformative prior distributions. These observations lead to two
computationally-modest families of quadrature algorithms that use the full
generality sample posterior but without the instability. The first algorithm
automatically eliminates the part of the sample that contributes large
truncation error. The second algorithm uses the posterior sample to assign
probability to a partition of the sample space and performs the marginal
likelihood integral directly. This eliminates convergence issues. The first
algorithm is analogous to standard quadrature but can only be applied for
convergent problems. The second is a hybrid of cubature: it uses the posterior
to discover and tessellate the subset of that sample space was explored and
uses quantiles to compute a representive field value. Neither algorithm makes
strong assumptions about the shape of the posterior distribution and neither is
sensitive outliers. [abridged]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.2375</identifier>
 <datestamp>2010-01-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.2375</id><created>2009-11-12</created><updated>2009-11-17</updated><authors><author><keyname>Rtimann</keyname><forenames>Philipp</forenames></author><author><keyname>Bhlmann</keyname><forenames>Peter</forenames></author></authors><title>High dimensional sparse covariance estimation via directed acyclic
  graphs</title><categories>stat.ME</categories><journal-ref>Electronic Journal of Statistics, 3, (2009), 1133-1160
  (electronic)</journal-ref><doi>10.1214/09-EJS534</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a graph-based technique for estimating sparse covariance matrices
and their inverses from high-dimensional data. The method is based on learning
a directed acyclic graph (DAG) and estimating parameters of a multivariate
Gaussian distribution based on a DAG. For inferring the underlying DAG we use
the PC-algorithm and for estimating the DAG-based covariance matrix and its
inverse, we use a Cholesky decomposition approach which provides a positive
(semi-)definite sparse estimate. We present a consistency result in the
high-dimensional framework and we compare our method with the Glasso for
simulated and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.2551</identifier>
 <datestamp>2010-06-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.2551</id><created>2009-11-13</created><updated>2010-05-31</updated><authors><author><keyname>Unnikrishnan</keyname><forenames>Jayakrishnan</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author><author><keyname>Meyn</keyname><forenames>Sean</forenames></author></authors><title>Minimax Robust Quickest Change Detection</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>Submitted to IEEE Transactions on Information Theory, Nov. 2009.
  Revised May 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The popular criteria of optimality for quickest change detection procedures
are the Lorden criterion, the Shiryaev-Roberts-Pollak criterion, and the
Bayesian criterion. In this paper a robust version of these quickest change
detection problems is considered when the pre-change and post-change
distributions are not known exactly but belong to known uncertainty classes of
distributions. For uncertainty classes that satisfy a specific condition, it is
shown that one can identify least favorable distributions (LFDs) from the
uncertainty classes, such that the detection rule designed for the LFDs is
optimal for the robust problem in a minimax sense. The condition is similar to
that required for the identification of LFDs for the robust hypothesis testing
problem originally studied by Huber. An upper bound on the delay incurred by
the robust test is also obtained in the asymptotic setting under the Lorden
criterion of optimality. This bound quantifies the delay penalty incurred to
guarantee robustness. When the LFDs can be identified, the proposed test is
easier to implement than the CUSUM test based on the Generalized Likelihood
Ratio (GLR) statistic which is a popular approach for such robust change
detection problems. The proposed test is also shown to give better performance
than the GLR test in simulations for some parameter values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.2746</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.2746</id><created>2009-11-15</created><updated>2010-04-29</updated><authors><author><keyname>Bajwa</keyname><forenames>Waheed U.</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author><author><keyname>Jafarpour</keyname><forenames>Sina</forenames></author></authors><title>Model Selection: Two Fundamental Measures of Coherence and Their
  Algorithmic Significance</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>5 pages; Accepted for Proc. 2010 IEEE International Symposium on
  Information Theory (ISIT 2010)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of model selection arises in a number of contexts, such as
compressed sensing, subset selection in linear regression, estimation of
structures in graphical models, and signal denoising. This paper generalizes
the notion of \emph{incoherence} in the existing literature on model selection
and introduces two fundamental measures of coherence---termed as the worst-case
coherence and the average coherence---among the columns of a design matrix. In
particular, it utilizes these two measures of coherence to provide an in-depth
analysis of a simple one-step thresholding (OST) algorithm for model selection.
One of the key insights offered by the ensuing analysis is that OST is feasible
for model selection as long as the design matrix obeys an easily verifiable
property. In addition, the paper also characterizes the model-selection
performance of OST in terms of the worst-case coherence, \mu, and establishes
that OST performs near-optimally in the low signal-to-noise ratio regime for N
x C design matrices with \mu = O(N^{-1/2}). Finally, in contrast to some of the
existing literature on model selection, the analysis in the paper is
nonasymptotic in nature, it does not require knowledge of the true model order,
it is applicable to generic (random or deterministic) design matrices, and it
neither requires submatrices of the design matrix to have full rank, nor does
it assume a statistical prior on the values of the nonzero entries of the data
vector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.2784</identifier>
 <datestamp>2011-10-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.2784</id><created>2009-11-14</created><updated>2011-10-05</updated><authors><author><keyname>Stummer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Vajda</keyname><forenames>Igor</forenames></author></authors><title>On Bregman Distances and Divergences of Probability Measures</title><categories>cs.IT math.IT math.PR math.ST stat.TH</categories><comments>12 two-column pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces scaled Bregman distances of probability distributions
which admit non-uniform contributions of observed events. They are introduced
in a general form covering not only the distances of discrete and continuous
stochastic observations, but also the distances of random processes and
signals. It is shown that the scaled Bregman distances extend not only the
classical ones studied in the previous literature, but also the information
divergence and the related wider class of convex divergences of probability
measures. An information processing theorem is established too, but only in the
sense of invariance w.r.t. statistically sufficient transformations and not in
the sense of universal monotonicity. Pathological situations where coding can
increase the classical Bregman distance are illustrated by a concrete example.
In addition to the classical areas of application of the Bregman distances and
convex divergences such as recognition, classification, learning and evaluation
of proximity of various features and signals, the paper mentions a new
application in 3D-exploratory data analysis. Explicit expressions for the
scaled Bregman distances are obtained in general exponential families, with
concrete applications in the binomial, Poisson and Rayleigh families, and in
the families of exponential processes such as the Poisson and diffusion
processes including the classical examples of the Wiener process and geometric
Brownian motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.2803</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.2803</id><created>2009-11-14</created><updated>2010-02-02</updated><authors><author><keyname>Hangelbroek</keyname><forenames>Thomas</forenames></author><author><keyname>Ron</keyname><forenames>Amos</forenames></author></authors><title>Nonlinear Approximation Using Gaussian Kernels</title><categories>math.CA math.ST stat.TH</categories><comments>15 Pages; corrected typos; to appear in J. Funct. Anal</comments><msc-class>42C40, 46B70, 26B35, 42B25</msc-class><journal-ref>Journal of Functional Analysis, vol. 259 (2010), Pages 203-219</journal-ref><doi>10.1016/j.jfa.2010.02.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that non-linear approximation has an advantage over linear
schemes in the sense that it provides comparable approximation rates to those
of the linear schemes, but to a larger class of approximands. This was
established for spline approximations and for wavelet approximations, and more
recently by DeVore and Ron for homogeneous radial basis function (surface
spline) approximations. However, no such results are known for the Gaussian
function, the preferred kernel in machine learning and several engineering
problems. We introduce and analyze in this paper a new algorithm for
approximating functions using translates of Gaussian functions with varying
tension parameters. At heart it employs the strategy for nonlinear
approximation of DeVore and Ron, but it selects kernels by a method that is not
straightforward. The crux of the difficulty lies in the necessity to vary the
tension parameter in the Gaussian function spatially according to local
information about the approximand: error analysis of Gaussian approximation
schemes with varying tension are, by and large, an elusive target for
approximators. We show that our algorithm is suitably optimal in the sense that
it provides approximation rates similar to other established nonlinear
methodologies like spline and wavelet approximations. As expected and desired,
the approximation rates can be as high as needed and are essentially saturated
only by the smoothness of the approximand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.2919</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.2919</id><created>2009-11-16</created><updated>2012-06-05</updated><authors><author><keyname>Rigollet</keyname><forenames>Philippe</forenames></author></authors><title>Kullback-Leibler aggregation and misspecified generalized linear models</title><categories>stat.ML math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/11-AOS961 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS961</report-no><journal-ref>Annals of Statistics 2012, Vol. 40, No. 2, 639-665</journal-ref><doi>10.1214/11-AOS961</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a regression setup with deterministic design, we study the pure
aggregation problem and introduce a natural extension from the Gaussian
distribution to distributions in the exponential family. While this extension
bears strong connections with generalized linear models, it does not require
identifiability of the parameter or even that the model on the systematic
component is true. It is shown that this problem can be solved by constrained
and/or penalized likelihood maximization and we derive sharp oracle
inequalities that hold both in expectation and with high probability. Finally
all the bounds are proved to be optimal in a minimax sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.3083</identifier>
 <datestamp>2011-07-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.3083</id><created>2009-11-16</created><updated>2011-07-27</updated><authors><author><keyname>Sharipov</keyname><forenames>Olimjon Sh.</forenames></author><author><keyname>Wendler</keyname><forenames>Martin</forenames></author></authors><title>Bootstrap for the Sample Mean and for U-Statistics of Mixing and Near
  Epoch Dependent Processes</title><categories>math.ST math.PR stat.TH</categories><comments>29 pages</comments><msc-class>62G09, 60G10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The validity of various bootstrapping methods has been proved for the sample
mean of strongly mixing data. But in many applications, there appear nonlinear
statistics of processes that are not strongly mixing. We investigate the
nonoverlapping block bootstrap sequences which are near epoch dependent on
strong mixing or absolutely regular processes. This includes ARMA and
GARCH-processes as well as data from chaotic dynamical systems. We establish
the strong consistency of the bootstrap distribution estimator not only for the
sample mean, but also for U-statistics, which include examples as Gini's mean
difference or the chi^2-test statistic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.3270</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.3270</id><created>2009-11-17</created><updated>2012-05-10</updated><authors><author><keyname>Guillotte</keyname><forenames>Simon</forenames></author><author><keyname>Perron</keyname><forenames>Francois</forenames></author><author><keyname>Segers</keyname><forenames>Johan</forenames></author></authors><title>Nonparametric Bayesian Inference on Bivariate Extremes</title><categories>math.ST stat.TH</categories><comments>The paper has been withdrawn by the author due to a major revision</comments><msc-class>62G32, 62F30, 65C60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The tail of a bivariate distribution function in the domain of attraction of
a bivariate extreme-value distribution may be approximated by the one of its
extreme-value attractor. The extreme-value attractor has margins that belong to
a three-parameter family and a dependence structure which is characterised by a
spectral measure, that is a probability measure on the unit interval with mean
equal to one half. As an alternative to parametric modelling of the spectral
measure, we propose an infinite-dimensional model which is at the same time
manageable and still dense within the class of spectral measures. Inference is
done in a Bayesian framework, using the censored-likelihood approach. In
particular, we construct a prior distribution on the class of spectral measures
and develop a trans-dimensional Markov chain Monte Carlo algorithm for
numerical computations. The method provides a bivariate predictive density
which can be used for predicting the extreme outcomes of the bivariate
distribution. In a practical perspective, this is useful for computing rare
event probabilities and extreme conditional quantiles. The methodology is
validated by simulations and applied to a data-set of Danish fire insurance
claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.3462</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.3462</id><created>2009-11-17</created><updated>2011-02-18</updated><authors><author><keyname>Touboul</keyname><forenames>Jonathan</forenames></author><author><keyname>Faugeras</keyname><forenames>Olivier</forenames></author></authors><title>A Markovian event-based framework for stochastic spiking neural networks</title><categories>stat.AP math.ST q-bio.NC stat.TH</categories><doi>10.1007/s10827-011-0327-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In spiking neural networks, the information is conveyed by the spike times,
that depend on the intrinsic dynamics of each neuron, the input they receive
and on the connections between neurons. In this article we study the Markovian
nature of the sequence of spike times in stochastic neural networks, and in
particular the ability to deduce from a spike train the next spike time, and
therefore produce a description of the network activity only based on the spike
times regardless of the membrane potential process.
  To study this question in a rigorous manner, we introduce and study an
event-based description of networks of noisy integrate-and-fire neurons, i.e.
that is based on the computation of the spike times. We show that the firing
times of the neurons in the networks constitute a Markov chain, whose
transition probability is related to the probability distribution of the
interspike interval of the neurons in the network. In the cases where the
Markovian model can be developed, the transition probability is explicitly
derived in such classical cases of neural networks as the linear
integrate-and-fire neuron models with excitatory and inhibitory interactions,
for different types of synapses, possibly featuring noisy synaptic integration,
transmission delays and absolute and relative refractory period. This covers
most of the cases that have been investigated in the event-based description of
spiking deterministic neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.3633</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.3633</id><created>2009-11-18</created><authors><author><keyname>Rubinstein</keyname><forenames>Benjamin I. P.</forenames></author><author><keyname>Rubinstein</keyname><forenames>J. Hyam</forenames></author></authors><title>A Geometric Approach to Sample Compression</title><categories>cs.LG math.CO math.GT stat.ML</categories><comments>37 pages, 18 figures, submitted to the Journal of Machine Learning
  Research</comments><acm-class>I.2.6; I.5.2; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Sample Compression Conjecture of Littlestone &amp; Warmuth has remained
unsolved for over two decades. This paper presents a systematic geometric
investigation of the compression of finite maximum concept classes. Simple
arrangements of hyperplanes in Hyperbolic space, and Piecewise-Linear
hyperplane arrangements, are shown to represent maximum classes, generalizing
the corresponding Euclidean result. A main result is that PL arrangements can
be swept by a moving hyperplane to unlabeled d-compress any finite maximum
class, forming a peeling scheme as conjectured by Kuzmin &amp; Warmuth. A corollary
is that some d-maximal classes cannot be embedded into any maximum class of VC
dimension d+k, for any constant k. The construction of the PL sweeping involves
Pachner moves on the one-inclusion graph, corresponding to moves of a
hyperplane across the intersection of d other hyperplanes. This extends the
well known Pachner moves for triangulations to cubical complexes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.3944</identifier>
 <datestamp>2011-08-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.3944</id><created>2009-11-19</created><authors><author><keyname>White</keyname><forenames>Christopher M.</forenames></author><author><keyname>Khudanpur</keyname><forenames>Sanjeev P.</forenames></author><author><keyname>Wolfe</keyname><forenames>Patrick J.</forenames></author></authors><title>Likelihood-based semi-supervised model selection with applications to
  speech processing</title><categories>stat.ML cs.CL cs.LG stat.AP</categories><comments>11 pages, 2 figures; submitted for publication</comments><journal-ref>IEEE Journal of Selected Topics in Signal Processing, vol. 4, pp.
  1016-1026, 2010</journal-ref><doi>10.1109/JSTSP.2010.2076050</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In conventional supervised pattern recognition tasks, model selection is
typically accomplished by minimizing the classification error rate on a set of
so-called development data, subject to ground-truth labeling by human experts
or some other means. In the context of speech processing systems and other
large-scale practical applications, however, such labeled development data are
typically costly and difficult to obtain. This article proposes an alternative
semi-supervised framework for likelihood-based model selection that leverages
unlabeled data by using trained classifiers representing each model to
automatically generate putative labels. The errors that result from this
automatic labeling are shown to be amenable to results from robust statistics,
which in turn provide for minimax-optimal censored likelihood ratio tests that
recover the nonparametric sign test as a limiting case. This approach is then
validated experimentally using a state-of-the-art automatic speech recognition
system to select between candidate word pronunciations using unlabeled speech
data that only potentially contain instances of the words under test. Results
provide supporting evidence for the utility of this approach, and suggest that
it may also find use in other applications of machine learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.4046</identifier>
 <datestamp>2011-06-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.4046</id><created>2009-11-20</created><updated>2011-01-02</updated><authors><author><keyname>Tomioka</keyname><forenames>Ryota</forenames></author><author><keyname>Suzuki</keyname><forenames>Taiji</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Super-Linear Convergence of Dual Augmented-Lagrangian Algorithm for
  Sparsity Regularized Estimation</title><categories>stat.ML cs.LG stat.ME</categories><comments>51 pages, 9 figures</comments><journal-ref>Journal of Machine Learning Research, 12(May):1537-1586, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the convergence behaviour of a recently proposed algorithm for
regularized estimation called Dual Augmented Lagrangian (DAL). Our analysis is
based on a new interpretation of DAL as a proximal minimization algorithm. We
theoretically show under some conditions that DAL converges super-linearly in a
non-asymptotic and global sense. Due to a special modelling of sparse
estimation problems in the context of machine learning, the assumptions we make
are milder and more natural than those made in conventional analysis of
augmented Lagrangian algorithms. In addition, the new interpretation enables us
to generalize DAL to wide varieties of sparse estimation problems. We
experimentally confirm our analysis in a large scale $\ell_1$-regularized
logistic regression problem and extensively compare the efficiency of DAL
algorithm to previously proposed algorithms on both synthetic and benchmark
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.4076</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.4076</id><created>2009-11-20</created><updated>2014-07-09</updated><authors><author><keyname>Hall</keyname><forenames>Peter</forenames></author><author><keyname>Jin</keyname><forenames>Jiashun</forenames></author><author><keyname>Miller</keyname><forenames>Hugh</forenames></author></authors><title>Feature selection when there are many influential features</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/13-BEJ536 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ536</report-no><journal-ref>Bernoulli 2014, Vol. 20, No. 3, 1647-1671</journal-ref><doi>10.3150/13-BEJ536</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent discussion of the success of feature selection methods has argued that
focusing on a relatively small number of features has been counterproductive.
Instead, it is suggested, the number of significant features can be in the
thousands or tens of thousands, rather than (as is commonly supposed at
present) approximately in the range from five to fifty. This change, in orders
of magnitude, in the number of influential features, necessitates alterations
to the way in which we choose features and to the manner in which the success
of feature selection is assessed. In this paper, we suggest a general approach
that is suited to cases where the number of relevant features is very large,
and we consider particular versions of the approach in detail. We propose ways
of measuring performance, and we study both theoretical and numerical
properties of the proposed methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.4139</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.4139</id><created>2009-11-21</created><updated>2011-07-14</updated><authors><author><keyname>Kabluchko</keyname><forenames>Zakhar</forenames></author></authors><title>Functional limit theorems for sums of independent geometric L\'{e}vy
  processes</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ299 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ299</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 3, 942-968</journal-ref><doi>10.3150/10-BEJ299</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\xi_i$, $i\in \mathbb {N}$, be independent copies of a L\'{e}vy process
$\{\xi(t),t\geq0\}$. Motivated by the results obtained previously in the
context of the random energy model, we prove functional limit theorems for the
process \[Z_N(t)=\sum_{i=1}^N\mathrm{e}^{\xi_i(s_N+t)}\] as $N\to\infty$, where
$s_N$ is a non-negative sequence converging to $+\infty$. The limiting process
depends heavily on the growth rate of the sequence $s_N$. If $s_N$ grows slowly
in the sense that $\liminf_{N\to\infty}\log N/s_N&gt;\lambda_2$ for some critical
value $\lambda_2&gt;0$, then the limit is an Ornstein--Uhlenbeck process. However,
if $\lambda:=\lim_{N\to\infty}\log N/s_N\in(0,\lambda_2)$, then the limit is a
certain completely asymmetric $\alpha$-stable process $\mathbb {Y}_{\alpha
;\xi}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.4151</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.4151</id><created>2009-11-20</created><updated>2012-11-14</updated><authors><author><keyname>Seregin</keyname><forenames>Arseni</forenames></author><author><keyname>Wellner</keyname><forenames>Jon A.</forenames></author></authors><title>Nonparametric estimation of multivariate convex-transformed densities</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS840 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS840</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 6, 3751-3781</journal-ref><doi>10.1214/10-AOS840</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study estimation of multivariate densities $p$ of the form $p(x)=h(g(x))$
for $x\in \mathbb {R}^d$ and for a fixed monotone function $h$ and an unknown
convex function $g$. The canonical example is $h(y)=e^{-y}$ for $y\in \mathbb
{R}$; in this case, the resulting class of densities [\mathcal
{P}(e^{-y})={p=\exp(-g):g is convex}] is well known as the class of log-concave
densities. Other functions $h$ allow for classes of densities with heavier
tails than the log-concave class. We first investigate when the maximum
likelihood estimator $\hat{p}$ exists for the class $\mathcal {P}(h)$ for
various choices of monotone transformations $h$, including decreasing and
increasing functions $h$. The resulting models for increasing transformations
$h$ extend the classes of log-convex densities studied previously in the
econometrics literature, corresponding to $h(y)=\exp(y)$. We then establish
consistency of the maximum likelihood estimator for fairly general functions
$h$, including the log-concave class $\mathcal {P}(e^{-y})$ and many others. In
a final section, we provide asymptotic minimax lower bounds for the estimation
of $p$ and its vector of derivatives at a fixed point $x_0$ under natural
smoothness hypotheses on $h$ and $g$. The proofs rely heavily on results from
convex analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.4207</identifier>
 <datestamp>2011-10-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.4207</id><created>2009-11-21</created><authors><author><keyname>Calsaverini</keyname><forenames>Rafael S.</forenames></author><author><keyname>Vicente</keyname><forenames>Renato</forenames></author></authors><title>An information theoretic approach to statistical dependence: copula
  information</title><categories>q-fin.ST cs.IT math.IT physics.data-an stat.AP</categories><comments>to appear in Europhysics Letters</comments><journal-ref>Europ. Phys. Lett. 88 68003 (2009)</journal-ref><doi>10.1209/0295-5075/88/68003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the connection between information and copula theories by showing
that a copula can be employed to decompose the information content of a
multivariate distribution into marginal and dependence components, with the
latter quantified by the mutual information. We define the information excess
as a measure of deviation from a maximum entropy distribution. The idea of
marginal invariant dependence measures is also discussed and used to show that
empirical linear correlation underestimates the amplitude of the actual
correlation in the case of non-Gaussian marginals. The mutual information is
shown to provide an upper bound for the asymptotic empirical log-likelihood of
a copula. An analytical expression for the information excess of T-copulas is
provided, allowing for simple model identification within this family. We
illustrate the framework in a financial data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.4546</identifier>
 <datestamp>2012-02-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.4546</id><created>2009-11-24</created><updated>2012-02-03</updated><authors><author><keyname>Hobert</keyname><forenames>James P.</forenames></author><author><keyname>Roy</keyname><forenames>Vivekananda</forenames></author><author><keyname>Robert</keyname><forenames>Christian P.</forenames></author></authors><title>Improving the Convergence Properties of the Data Augmentation Algorithm
  with an Application to Bayesian Mixture Modeling</title><categories>stat.ME stat.CO</categories><comments>Published in at http://dx.doi.org/10.1214/11-STS365 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS365</report-no><journal-ref>Statistical Science 2011, Vol. 26, No. 3, 332-351</journal-ref><doi>10.1214/11-STS365</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reversible Markov chains that drive the data augmentation (DA) and
sandwich algorithms define self-adjoint operators whose spectra encode the
convergence properties of the algorithms. When the target distribution has
uncountable support, as is nearly always the case in practice, it is generally
quite difficult to get a handle on these spectra. We show that, if the
augmentation space is finite, then (under regularity conditions) the operators
defined by the DA and sandwich chains are compact, and the spectra are finite
subsets of $[0,1)$. Moreover, we prove that the spectrum of the sandwich
operator dominates the spectrum of the DA operator in the sense that the
ordered elements of the former are all less than or equal to the corresponding
elements of the latter. As a concrete example, we study a widely used DA
algorithm for the exploration of posterior densities associated with Bayesian
mixture models [J. Roy. Statist. Soc. Ser. B 56 (1994) 363--375]. In
particular, we compare this mixture DA algorithm with an alternative algorithm
proposed by Fr\"{u}hwirth-Schnatter [J. Amer. Statist. Assoc. 96 (2001)
194--209] that is based on random label switching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.4727</identifier>
 <datestamp>2010-12-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.4727</id><created>2009-11-24</created><updated>2010-12-09</updated><authors><author><keyname>de Cooman</keyname><forenames>Gert</forenames></author><author><keyname>Quaeghebeur</keyname><forenames>Erik</forenames></author></authors><title>Exchangeability and sets of desirable gambles</title><categories>math.PR cs.AI math.ST stat.TH</categories><comments>40 pages</comments><doi>10.1016/j.ijar.2010.12.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sets of desirable gambles constitute a quite general type of uncertainty
model with an interesting geometrical interpretation. We give a general
discussion of such models and their rationality criteria. We study
exchangeability assessments for them, and prove counterparts of de Finetti's
finite and infinite representation theorems. We show that the finite
representation in terms of count vectors has a very nice geometrical
interpretation, and that the representation in terms of frequency vectors is
tied up with multivariate Bernstein (basis) polynomials. We also lay bare the
relationships between the representations of updated exchangeable models, and
discuss conservative inference (natural extension) under exchangeability and
the extension of exchangeable sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.4917</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.4917</id><created>2009-11-25</created><updated>2012-11-29</updated><authors><author><keyname>Houdr</keyname><forenames>Christian</forenames></author><author><keyname>Talata</keyname><forenames>Zsolt</forenames></author></authors><title>On the rate of approximation in finite-alphabet longest increasing
  subsequence problems</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/12-AAP853 the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP853</report-no><journal-ref>Annals of Applied Probability 2012, Vol. 22, No. 6, 2539-2559</journal-ref><doi>10.1214/12-AAP853</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rate of convergence of the distribution of the length of the longest
increasing subsequence, toward the maximal eigenvalue of certain matrix
ensembles, is investigated. For finite-alphabet uniform and nonuniform i.i.d.
sources, a rate of $\log n/\sqrt{n}$ is obtained. The uniform binary case is
further explored, and an improved $1/\sqrt{n}$ rate obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.5357</identifier>
 <datestamp>2011-07-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.5357</id><created>2009-11-27</created><updated>2011-07-06</updated><authors><author><keyname>Ribatet</keyname><forenames>Mathieu</forenames></author><author><keyname>Cooley</keyname><forenames>Daniel</forenames></author><author><keyname>Davison</keyname><forenames>Anthony C.</forenames></author></authors><title>Bayesian Inference from Composite Likelihoods, with an Application to
  Spatial Extremes</title><categories>stat.ME math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Composite likelihoods are increasingly used in applications where the full
likelihood is analytically unknown or computationally prohibitive. Although the
maximum composite likelihood estimator has frequentist properties akin to those
of the usual maximum likelihood estimator, Bayesian inference based on
composite likelihoods has yet to be explored. In this paper we investigate the
use of the Metropolis--Hastings algorithm to compute a pseudo-posterior
distribution based on the composite likelihood. Two methodologies for adjusting
the algorithm are presented and their performance on approximating the true
posterior distribution is investigated using simulated data sets and real data
on spatial extremes of rainfall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.5460</identifier>
 <datestamp>2011-11-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.5460</id><created>2009-11-29</created><updated>2011-11-10</updated><authors><author><keyname>She</keyname><forenames>Yiyuan</forenames></author></authors><title>An Iterative Algorithm for Fitting Nonconvex Penalized Generalized
  Linear Models with Grouped Predictors</title><categories>stat.ML stat.ME</categories><comments>Computational Statistics and Data Analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-dimensional data pose challenges in statistical learning and modeling.
Sometimes the predictors can be naturally grouped where pursuing the
between-group sparsity is desired. Collinearity may occur in real-world
high-dimensional applications where the popular $l_1$ technique suffers from
both selection inconsistency and prediction inaccuracy. Moreover, the problems
of interest often go beyond Gaussian models. To meet these challenges,
nonconvex penalized generalized linear models with grouped predictors are
investigated and a simple-to-implement algorithm is proposed for computation. A
rigorous theoretical result guarantees its convergence and provides tight
preliminary scaling. This framework allows for grouped predictors and nonconvex
penalties, including the discrete $l_0$ and the `$l_0+l_2$' type penalties.
Penalty design and parameter tuning for nonconvex penalties are examined.
Applications of super-resolution spectrum estimation in signal processing and
cancer classification with joint gene selection in bioinformatics show the
performance improvement by nonconvex penalized estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.5482</identifier>
 <datestamp>2010-01-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.5482</id><created>2009-11-30</created><updated>2010-01-08</updated><authors><author><keyname>Bochkina</keyname><forenames>Natalia</forenames></author><author><keyname>Ritov</keyname><forenames>Ya'acov</forenames></author></authors><title>Sparse Empirical Bayes Analysis (SEBA)</title><categories>stat.ML math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a joint processing of $n$ independent sparse regression problems.
Each is based on a sample $(y_{i1},x_{i1})...,(y_{im},x_{im})$ of $m$ \iid
observations from $y_{i1}=x_{i1}\t\beta_i+\eps_{i1}$, $y_{i1}\in \R$, $x_{i
1}\in\R^p$, $i=1,...,n$, and $\eps_{i1}\dist N(0,\sig^2)$, say. $p$ is large
enough so that the empirical risk minimizer is not consistent. We consider
three possible extensions of the lasso estimator to deal with this problem, the
lassoes, the group lasso and the RING lasso, each utilizing a different
assumption how these problems are related. For each estimator we give a
Bayesian interpretation, and we present both persistency analysis and
non-asymptotic error bounds based on restricted eigenvalue - type assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0911.5628</identifier>
 <datestamp>2010-03-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0911.5628</id><created>2009-11-30</created><authors><author><keyname>Patriota</keyname><forenames>Alexandre G.</forenames></author><author><keyname>Sato</keyname><forenames>Joao R.</forenames></author><author><keyname>Blas</keyname><forenames>Betsabe G.</forenames></author></authors><title>Vector Autoregressive Models With Measurement Errors for Testing Ganger
  Causality</title><categories>stat.ME</categories><comments>manuscript submitted for possible publication</comments><doi>10.1016/j.stamet.2010.02.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a method for estimating parameters of a vector
autoregression (VAR) observed in white noise. The estimation method assumes the
noise variance matrix is known and does not require any iterative process. This
study provides consistent estimators and shows the asymptotic distribution of
the parameters required for conducting tests of Granger causality. Methods in
the existing statistical literature cannot be used for testing Granger
causality, since under the null hypothesis the model becomes unidentifiable.
Measurement error effects on parameter estimates were evaluated by using
computational simulations. The results show that the proposed approach produces
empirical false positive rates close to the adopted nominal level (even for
small samples) and has a good performance around the null hypothesis. The
applicability and usefulness of the proposed approach are illustrated using a
functional magnetic resonance imaging dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.0284</identifier>
 <datestamp>2012-01-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.0284</id><created>2009-12-01</created><updated>2011-11-24</updated><authors><author><keyname>Bartholdi</keyname><forenames>Laurent</forenames><affiliation>Georg-August-Universitt Gttingen</affiliation></author><author><keyname>Schick</keyname><forenames>Thomas</forenames><affiliation>Georg-August-Universitt Gttingen</affiliation></author><author><keyname>Smale</keyname><forenames>Nat</forenames><affiliation>University of Utah</affiliation></author><author><keyname>Smale</keyname><forenames>Steve</forenames><affiliation>City University of Hong Kong</affiliation></author><author><keyname>Baker</keyname><forenames>Anthony W.</forenames><affiliation>The Boing Company</affiliation></author></authors><title>Hodge Theory on Metric Spaces</title><categories>math.KT cs.CG math.GT math.NA stat.ML</categories><comments>appendix by Anthony W. Baker, 48 pages, AMS-LaTeX. v2: final version,
  to appear in Foundations of Computational Mathematics. Minor changes and
  additions</comments><msc-class>58A14, 54E05, 55P55, 57M50</msc-class><journal-ref>Foundations of Computational Mathematics 12:1 (2012), 1-48</journal-ref><doi>10.1007/s10208-011-9107-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hodge theory is a beautiful synthesis of geometry, topology, and analysis,
which has been developed in the setting of Riemannian manifolds. On the other
hand, spaces of images, which are important in the mathematical foundations of
vision and pattern recognition, do not fit this framework. This motivates us to
develop a version of Hodge theory on metric spaces with a probability measure.
We believe that this constitutes a step towards understanding the geometry of
vision.
  The appendix by Anthony Baker provides a separable, compact metric space with
infinite dimensional \alpha-scale homology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.0577</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.0577</id><created>2009-12-03</created><updated>2010-01-22</updated><authors><author><keyname>Kuriki</keyname><forenames>Satoshi</forenames></author><author><keyname>Numata</keyname><forenames>Yasuhide</forenames></author></authors><title>Graph presentations for moments of noncentral Wishart distributions and
  their applications</title><categories>math.ST stat.TH</categories><journal-ref>Ann. Inst. Statist. Math. 62 (2010), no. 4, 645-672</journal-ref><doi>10.1007/s10463-010-0279-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide formulas for the moments of the real and complex noncentral
Wishart distributions of general degrees. The obtained formulas for the real
and complex cases are described in terms of the undirected and directed graphs,
respectively. By considering degenerate cases, we give explicit formulas for
the moments of bivariate chi-square distributions and $2\times 2$ Wishart
distributions by enumerating the graphs. Noting that the Laguerre polynomials
can be considered to be moments of a noncentral chi-square distributions
formally, we demonstrate a combinatorial interpretation of the coefficients of
the Laguerre polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.0786</identifier>
 <datestamp>2010-06-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.0786</id><created>2009-12-04</created><updated>2010-05-31</updated><authors><author><keyname>Autin</keyname><forenames>Florent</forenames><affiliation>LATP</affiliation></author><author><keyname>Pouet</keyname><forenames>Christophe</forenames><affiliation>LATP</affiliation></author></authors><title>Tests on components of density mixtures</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with statistical tests on the components of mixture
densities. We propose to test whether the densities of two independent samples
of independent random variables $Y_1, ..., Y_n$ and $Z_1, ..., Z_n$ result from
the same mixture of $M$ components or not. We provide a test procedure which is
proved to be asymptotically optimal according to the minimax setting. We
extensively discuss the connection between the mixing weights and the
performance of the testing procedure and illustrate it with numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.0874</identifier>
 <datestamp>2011-11-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.0874</id><created>2009-12-04</created><updated>2011-11-03</updated><authors><author><keyname>Hable</keyname><forenames>Robert</forenames><affiliation>University of Bayreuth</affiliation></author><author><keyname>Christmann</keyname><forenames>Andreas</forenames><affiliation>University of Bayreuth</affiliation></author></authors><title>Qualitative Robustness of Support Vector Machines</title><categories>stat.ML math.ST stat.TH</categories><msc-class>62G08, 62G35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support vector machines have attracted much attention in theoretical and in
applied statistics. Main topics of recent interest are consistency, learning
rates and robustness. In this article, it is shown that support vector machines
are qualitatively robust. Since support vector machines can be represented by a
functional on the set of all probability measures, qualitative robustness is
proven by showing that this functional is continuous with respect to the
topology generated by weak convergence of probability measures. Combined with
the existence and uniqueness of support vector machines, our results show that
support vector machines are the solutions of a well-posed mathematical problem
in Hadamard's sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.0902</identifier>
 <datestamp>2010-03-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.0902</id><created>2009-12-04</created><updated>2010-03-07</updated><authors><author><keyname>Gneiting</keyname><forenames>Tilmann</forenames></author></authors><title>Making and Evaluating Point Forecasts</title><categories>math.ST math.PR stat.ME stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Typically, point forecasting methods are compared and assessed by means of an
error measure or scoring function, such as the absolute error or the squared
error. The individual scores are then averaged over forecast cases, to result
in a summary measure of the predictive performance, such as the mean absolute
error or the (root) mean squared error. I demonstrate that this common practice
can lead to grossly misguided inferences, unless the scoring function and the
forecasting task are carefully matched.
  Effective point forecasting requires that the scoring function be specified
ex ante, or that the forecaster receives a directive in the form of a
statistical functional, such as the mean or a quantile of the predictive
distribution. If the scoring function is specified ex ante, the forecaster can
issue the optimal point forecast, namely, the Bayes rule. If the forecaster
receives a directive in the form of a functional, it is critical that the
scoring function be consistent for it, in the sense that the expected score is
minimized when following the directive.
  A functional is elicitable if there exists a scoring function that is
strictly consistent for it. Expectations, ratios of expectations and quantiles
are elicitable. For example, a scoring function is consistent for the mean
functional if and only if it is a Bregman function. It is consistent for a
quantile if and only if it is generalized piecewise linear. Similar
characterizations apply to ratios of expectations and to expectiles. Weighted
scoring functions are consistent for functionals that adapt to the weighting in
peculiar ways. Not all functionals are elicitable; for instance, conditional
value-at-risk is not, despite its popularity in quantitative finance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.1072</identifier>
 <datestamp>2012-02-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.1072</id><created>2009-12-05</created><updated>2011-12-19</updated><authors><author><keyname>Freer</keyname><forenames>Cameron E.</forenames></author><author><keyname>Roy</keyname><forenames>Daniel M.</forenames></author></authors><title>Computable de Finetti measures</title><categories>math.LO cs.LO cs.PL math.PR math.ST stat.ML stat.TH</categories><comments>32 pages. Final journal version; expanded somewhat, with minor
  corrections. To appear in Annals of Pure and Applied Logic. Extended abstract
  appeared in Proceedings of CiE '09, LNCS 5635, pp. 218-231</comments><msc-class>03D78, 60G09, 68Q10, 03F60, 68N18</msc-class><journal-ref>Annals of Pure and Applied Logic 163 (2012) pp. 530-546</journal-ref><doi>10.1016/j.apal.2011.06.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a computable version of de Finetti's theorem on exchangeable
sequences of real random variables. As a consequence, exchangeable stochastic
processes expressed in probabilistic functional programming languages can be
automatically rewritten as procedures that do not modify non-local state. Along
the way, we prove that a distribution on the unit interval is computable if and
only if its moments are uniformly computable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.1207</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.1207</id><created>2009-12-07</created><updated>2013-12-10</updated><authors><author><keyname>Johannes</keyname><forenames>Jan</forenames></author><author><keyname>Schwarz</keyname><forenames>Maik</forenames></author></authors><title>Adaptive circular deconvolution by model selection under unknown error
  distribution</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/12-BEJ422 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ422</report-no><journal-ref>Bernoulli 2013, Vol. 19, No. 5A, 1576-1611</journal-ref><doi>10.3150/12-BEJ422</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a circular deconvolution problem, in which the density $f$ of a
circular random variable $X$ must be estimated nonparametrically based on an
i.i.d. sample from a noisy observation $Y$ of $X$. The additive measurement
error is supposed to be independent of $X$. The objective of this work was to
construct a fully data-driven estimation procedure when the error density
$\varphi$ is unknown. We assume that in addition to the i.i.d. sample from $Y$,
we have at our disposal an additional i.i.d. sample drawn independently from
the error distribution. We first develop a minimax theory in terms of both
sample sizes. We propose an orthogonal series estimator attaining the minimax
rates but requiring optimal choice of a dimension parameter depending on
certain characteristics of $f$ and $\varphi$, which are not known in practice.
The main issue addressed in this work is the adaptive choice of this dimension
parameter using a model selection approach. In a first step, we develop a
penalized minimum contrast estimator assuming that the error density is known.
We show that this partially adaptive estimator can attain the lower risk bound
up to a constant in both sample sizes $n$ and $m$. Finally, by randomizing the
penalty and the collection of models, we modify the estimator such that it no
longer requires any previous knowledge of the error distribution. Even when
dispensing with any hypotheses on $\varphi$, this fully data-driven estimator
still preserves minimax optimality in almost the same cases as the partially
adaptive estimator. We illustrate our results by computing minimal rates under
classical smoothness assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.1586</identifier>
 <datestamp>2010-11-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.1586</id><created>2009-12-08</created><updated>2010-11-21</updated><authors><author><keyname>Taddy</keyname><forenames>Matthew A.</forenames></author><author><keyname>Gramacy</keyname><forenames>Robert B.</forenames></author><author><keyname>Polson</keyname><forenames>Nicholas G.</forenames></author></authors><title>Dynamic Trees for Learning and Design</title><categories>stat.ME stat.CO</categories><comments>37 pages, 8 figures, 3 tables; accepted at JASA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic regression trees are an attractive option for automatic regression
and classification with complicated response surfaces in on-line application
settings. We create a sequential tree model whose state changes in time with
the accumulation of new data, and provide particle learning algorithms that
allow for the efficient on-line posterior filtering of tree-states. A major
advantage of tree regression is that it allows for the use of very simple
models within each partition. The model also facilitates a natural division of
labor in our sequential particle-based inference: tree dynamics are defined
through a few potential changes that are local to each newly arrived
observation, while global uncertainty is captured by the ensemble of particles.
We consider both constant and linear mean functions at the tree leaves, along
with multinomial leaves for classification problems, and propose default prior
specifications that allow for prediction to be integrated over all model
parameters conditional on a given tree. Inference is illustrated in some
standard nonparametric regression examples, as well as in the setting of
sequential experiment design, including both active learning and optimization
applications, and in on-line classification. We detail implementation
guidelines and problem specific methodology for each of these motivating
applications. Throughout, it is demonstrated that our practical approach is
able to provide better results compared to commonly used methods at a fraction
of the cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.1628</identifier>
 <datestamp>2010-03-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.1628</id><created>2009-12-08</created><updated>2010-03-23</updated><authors><author><keyname>Vaswani</keyname><forenames>Namrata</forenames></author></authors><title>KF-CS: Compressive Sensing on Kalman Filtered Residual</title><categories>cs.IT math.IT stat.ME</categories><comments>7 pages, 2 figures, submitted to the IEEE for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recursively reconstructing time sequences of
sparse signals (with unknown and time-varying sparsity patterns) from a limited
number of linear incoherent measurements with additive noise. The idea of our
proposed solution, KF CS-residual (KF-CS) is to replace compressed sensing (CS)
on the observation by CS on the Kalman filtered (KF) observation residual
computed using the previous estimate of the support. KF-CS error stability over
time is studied. Simulation comparisons with CS and LS-CS are shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.1672</identifier>
 <datestamp>2011-08-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.1672</id><created>2009-12-09</created><updated>2011-08-05</updated><authors><author><keyname>Vogelstein</keyname><forenames>Joshua T.</forenames></author><author><keyname>Vogelstein</keyname><forenames>R. Jacob</forenames></author><author><keyname>Priebe</keyname><forenames>Carey E.</forenames></author></authors><title>Are mental properties supervenient on brain properties?</title><categories>q-bio.OT stat.AP</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The "mind-brain supervenience" conjecture suggests that all mental properties
are derived from the physical properties of the brain. To address the question
of whether the mind supervenes on the brain, we frame a supervenience
hypothesis in rigorous statistical terms. Specifically, we propose a modified
version of supervenience (called epsilon-supervenience) that is amenable to
experimental investigation and statistical analysis. To illustrate this
approach, we perform a thought experiment that illustrates how the
probabilistic theory of pattern recognition can be used to make a one-sided
determination of epsilon-supervenience. The physical property of the brain
employed in this analysis is the graph describing brain connectivity (i.e., the
brain-graph or connectome). epsilon-supervenience allows us to determine
whether a particular mental property can be inferred from one's connectome to
within any given positive misclassification rate, regardless of the
relationship between the two. This may provide motivation for
cross-disciplinary research between neuroscientists and statisticians.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.2026</identifier>
 <datestamp>2011-06-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.2026</id><created>2009-12-10</created><updated>2011-06-06</updated><authors><author><keyname>Bigot</keyname><forenames>Jrmie</forenames><affiliation>IMT</affiliation></author><author><keyname>Lirio</keyname><forenames>Rolando Biscay</forenames><affiliation>IMT</affiliation></author><author><keyname>Loubes</keyname><forenames>Jean-Michel</forenames><affiliation>IMT</affiliation></author><author><keyname>Alvarez</keyname><forenames>Lilian Muniz</forenames></author></authors><title>Adaptive estimation of spectral densities via wavelet thresholding and
  information projection</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of adaptive estimation of the spectral
density of a stationary Gaussian process. For this purpose, we consider a
wavelet-based method which combines the ideas of wavelet approximation and
estimation by information projection in order to warrants that the solution is
a nonnegative function. The spectral density of the process is estimated by
projecting the wavelet thresholding expansion of the periodogram onto a family
of exponential functions. This ensures that the spectral density estimator is a
strictly positive function. Then, by Bochner's theorem, the corresponding
estimator of the covariance function is semidefinite positive. The theoretical
behavior of the estimator is established in terms of rate of convergence of the
Kullback-Leibler discrepancy over Besov classes. We also show the excellent
practical performance of the estimator in some numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.2380</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.2380</id><created>2009-12-11</created><updated>2010-07-26</updated><authors><author><keyname>Brewer</keyname><forenames>Brendon J.</forenames></author><author><keyname>Prtay</keyname><forenames>Livia B.</forenames></author><author><keyname>Csnyi</keyname><forenames>Gbor</forenames></author></authors><title>Diffusive Nested Sampling</title><categories>stat.CO astro-ph.IM physics.data-an</categories><comments>Accepted for publication in Statistics and Computing. C++ code
  available at http://lindor.physics.ucsb.edu/DNest</comments><journal-ref>Statistics and Computing, 2011, 21, 4, 649-656</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We introduce a general Monte Carlo method based on Nested Sampling (NS), for
sampling complex probability distributions and estimating the normalising
constant. The method uses one or more particles, which explore a mixture of
nested probability distributions, each successive distribution occupying ~e^-1
times the enclosed prior mass of the previous distribution. While NS
technically requires independent generation of particles, Markov Chain Monte
Carlo (MCMC) exploration fits naturally into this technique. We illustrate the
new method on a test problem and find that it can achieve four times the
accuracy of classic MCMC-based Nested Sampling, for the same computational
effort; equivalent to a factor of 16 speedup. An additional benefit is that
more samples and a more accurate evidence value can be obtained simply by
continuing the run for longer, as in standard MCMC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.2412</identifier>
 <datestamp>2010-08-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.2412</id><created>2009-12-12</created><authors><author><keyname>Haufe</keyname><forenames>Stefan</forenames></author><author><keyname>Tomioka</keyname><forenames>Ryota</forenames></author><author><keyname>Nolte</keyname><forenames>Guido</forenames></author><author><keyname>Mueller</keyname><forenames>Klaus-Robert</forenames></author><author><keyname>Kawanabe</keyname><forenames>Motoaki</forenames></author></authors><title>Modeling sparse connectivity between underlying brain sources for
  EEG/MEG</title><categories>stat.ME stat.AP stat.ML</categories><comments>9 pages, 6 figures</comments><journal-ref>IEEE Trans. Biomed. Eng. 57(8) (2010) 1954 - 1963;</journal-ref><doi>10.1109/TBME.2010.2046325</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel technique to assess functional brain connectivity in
EEG/MEG signals. Our method, called Sparsely-Connected Sources Analysis (SCSA),
can overcome the problem of volume conduction by modeling neural data
innovatively with the following ingredients: (a) the EEG is assumed to be a
linear mixture of correlated sources following a multivariate autoregressive
(MVAR) model, (b) the demixing is estimated jointly with the source MVAR
parameters, (c) overfitting is avoided by using the Group Lasso penalty. This
approach allows to extract the appropriate level cross-talk between the
extracted sources and in this manner we obtain a sparse data-driven model of
functional connectivity. We demonstrate the usefulness of SCSA with simulated
data, and compare to a number of existing algorithms with excellent results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.2423</identifier>
 <datestamp>2010-02-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.2423</id><created>2009-12-12</created><updated>2010-02-09</updated><authors><author><keyname>Laurent</keyname><forenames>Batrice</forenames><affiliation>IMT</affiliation></author><author><keyname>Loubs</keyname><forenames>Jean-Michel</forenames><affiliation>IMT</affiliation></author><author><keyname>Marteau</keyname><forenames>Clment</forenames><affiliation>IMT</affiliation></author></authors><title>Non asymptotic minimax rates of testing in signal detection with
  heterogeneous variances</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00440825</proxy><msc-class>62G05, 62G20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to establish non-asymptotic minimax rates of testing
for goodness-of-fit hypotheses in a heteroscedastic setting. More precisely, we
deal with sequences $(Y_j)_{j\in J}$ of independent Gaussian random variables,
having mean $(\theta_j)_{j\in J}$ and variance $(\sigma_j)_{j\in J}$. The set
$J$ will be either finite or countable. In particular, such a model covers the
inverse problem setting where few results in test theory have been obtained.
The rates of testing are obtained with respect to $l_2$ and $l_{\infty}$ norms,
without assumption on $(\sigma_j)_{j\in J}$ and on several functions spaces.
Our point of view is completely non-asymptotic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.2577</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.2577</id><created>2009-12-14</created><authors><author><keyname>Andoni</keyname><forenames>Alexandr</forenames></author><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Hassidim</keyname><forenames>Avinatan</forenames></author><author><keyname>Roch</keyname><forenames>Sebastien</forenames></author></authors><title>Global Alignment of Molecular Sequences via Ancestral State
  Reconstruction</title><categories>math.PR cs.DS math.ST q-bio.PE q-bio.QM stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Molecular phylogenetic techniques do not generally account for such common
evolutionary events as site insertions and deletions (known as indels). Instead
tree building algorithms and ancestral state inference procedures typically
rely on substitution-only models of sequence evolution. In practice these
methods are extended beyond this simplified setting with the use of heuristics
that produce global alignments of the input sequences--an important problem
which has no rigorous model-based solution. In this paper we consider a new
version of the multiple sequence alignment in the context of stochastic indel
models. More precisely, we introduce the following {\em trace reconstruction
problem on a tree} (TRPT): a binary sequence is broadcast through a tree
channel where we allow substitutions, deletions, and insertions; we seek to
reconstruct the original sequence from the sequences received at the leaves of
the tree. We give a recursive procedure for this problem with strong
reconstruction guarantees at low mutation rates, providing also an alignment of
the sequences at the leaves of the tree. The TRPT problem without indels has
been studied in previous work (Mossel 2004, Daskalakis et al. 2006) as a
bootstrapping step towards obtaining optimal phylogenetic reconstruction
methods. The present work sets up a framework for extending these works to
evolutionary models with indels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.2695</identifier>
 <datestamp>2011-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.2695</id><created>2009-12-14</created><updated>2011-01-18</updated><authors><author><keyname>Fan</keyname><forenames>Jianqing</forenames></author><author><keyname>Feng</keyname><forenames>Yang</forenames></author><author><keyname>Song</keyname><forenames>Rui</forenames></author></authors><title>Nonparametric Independence Screening in Sparse Ultra-High Dimensional
  Additive Models</title><categories>stat.ME math.ST stat.CO stat.ML stat.TH</categories><comments>48 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variable screening procedure via correlation learning was proposed Fan and
Lv (2008) to reduce dimensionality in sparse ultra-high dimensional models.
Even when the true model is linear, the marginal regression can be highly
nonlinear. To address this issue, we further extend the correlation learning to
marginal nonparametric learning. Our nonparametric independence screening is
called NIS, a specific member of the sure independence screening. Several
closely related variable screening procedures are proposed. Under the
nonparametric additive models, it is shown that under some mild technical
conditions, the proposed independence screening methods enjoy a sure screening
property. The extent to which the dimensionality can be reduced by independence
screening is also explicitly quantified. As a methodological extension, an
iterative nonparametric independence screening (INIS) is also proposed to
enhance the finite sample performance for fitting sparse additive models. The
simulation results and a real data analysis demonstrate that the proposed
procedure works well with moderate sample size and large dimension and performs
better than competing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.2873</identifier>
 <datestamp>2010-07-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.2873</id><created>2009-12-15</created><updated>2010-07-24</updated><authors><author><keyname>Latouche</keyname><forenames>Pierre</forenames></author><author><keyname>Birmele</keyname><forenames>Etienne</forenames></author><author><keyname>Ambroise</keyname><forenames>Christophe</forenames></author></authors><title>Variational Bayesian Inference and Complexity Control for Stochastic
  Block Models</title><categories>stat.AP stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is now widely accepted that knowledge can be acquired from networks by
clustering their vertices according to connection profiles. Many methods have
been proposed and in this paper we concentrate on the Stochastic Block Model
(SBM). The clustering of vertices and the estimation of SBM model parameters
have been subject to previous work and numerous inference strategies such as
variational Expectation Maximization (EM) and classification EM have been
proposed. However, SBM still suffers from a lack of criteria to estimate the
number of components in the mixture. To our knowledge, only one model based
criterion, ICL, has been derived for SBM in the literature. It relies on an
asymptotic approximation of the Integrated Complete-data Likelihood and recent
studies have shown that it tends to be too conservative in the case of small
networks. To tackle this issue, we propose a new criterion that we call ILvb,
based on a non asymptotic approximation of the marginal likelihood. We describe
how the criterion can be computed through a variational Bayes EM algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.3091</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.3091</id><created>2009-12-16</created><updated>2011-07-14</updated><authors><author><keyname>Barndorff-Nielsen</keyname><forenames>Ole E.</forenames></author><author><keyname>Basse-O'Connor</keyname><forenames>Andreas</forenames></author></authors><title>Quasi Ornstein-Uhlenbeck processes</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ311 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ311</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 3, 916-941</journal-ref><doi>10.3150/10-BEJ311</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question of existence and properties of stationary solutions to Langevin
equations driven by noise processes with stationary increments is discussed,
with particular focus on noise processes of pseudo-moving-average type. On
account of the Wold-Karhunen decomposition theorem, such solutions are, in
principle, representable as a moving average (plus a drift-like term) but the
kernel in the moving average is generally not available in explicit form. A
class of cases is determined where an explicit expression of the kernel can be
given, and this is used to obtain information on the asymptotic behavior of the
associated autocorrelation functions, both for small and large lags.
Applications to Gaussian- and L\'{e}vy-driven fractional Ornstein-Uhlenbeck
processes are presented. A Fubini theorem for L\'{e}vy bases is established as
an element in the derivations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.3148</identifier>
 <datestamp>2010-09-17</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.3148</id><created>2009-12-16</created><authors><author><keyname>Chronopoulou</keyname><forenames>Alexandra</forenames><affiliation>LPP</affiliation></author><author><keyname>Tudor</keyname><forenames>Ciprian</forenames><affiliation>LPP</affiliation></author><author><keyname>Viens</keyname><forenames>Frederi</forenames></author></authors><title>Variations and Hurst index estimation for a Rosenblatt process using
  longer filters</title><categories>math.PR math.ST stat.TH</categories><comments>To appear in Electronic Journal of Statistics</comments><proxy>ccsd hal-00440149</proxy><journal-ref>Electronic Journal of Statistics 3 (2009) 1393-1435</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Rosenblatt process is a self-similar non-Gaussian process which lives in
second Wiener chaos, and occurs as the limit of correlated random sequences in
so-called \textquotedblleft non-central limit theorems\textquotedblright. It
shares the same covariance as fractional Brownian motion. We study the
asymptotic distribution of the quadratic variations of the Rosenblatt process
based on long filters, including filters based on high-order finite-difference
and wavelet-based schemes. We find exact formulas for the limiting
distributions, which we then use to devise strongly consistent estimators of
the self-similarity parameter $H$. Unlike the case of fractional Brownian
motion, no matter now high the filter orders are, the estimators are never
asymptotically normal, converging instead in the mean square to the observed
value of the Rosenblatt process at time 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.3295</identifier>
 <datestamp>2010-10-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.3295</id><created>2009-12-16</created><updated>2010-10-06</updated><authors><author><keyname>Bickel</keyname><forenames>Peter J.</forenames></author><author><keyname>Xu</keyname><forenames>Ying</forenames></author></authors><title>Discussion of: Brownian distance covariance</title><categories>math.ST stat.AP stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOAS312A the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS312A</report-no><journal-ref>Annals of Applied Statistics 2009, Vol. 3, No. 4, 1266-1269</journal-ref><doi>10.1214/09-AOAS312A</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discussion on "Brownian distance covariance" by G\'{a}bor J. Sz\'{e}kely and
Maria L. Rizzo [arXiv:1010.0297]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.3330</identifier>
 <datestamp>2012-10-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.3330</id><created>2009-12-17</created><updated>2010-02-23</updated><authors><author><keyname>Kenah</keyname><forenames>Eben</forenames></author></authors><title>Contact intervals, survival analysis of epidemic data, and estimation of
  R_0</title><categories>stat.AP</categories><comments>30 pages, 4 figures; submitted to Biostatistics</comments><journal-ref>Biostatistics 12(3): 548-566 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We argue that the time from the onset of infectiousness to infectious
contact, which we call the contact interval, is a better basis for inference in
epidemic data than the generation or serial interval. Since contact intervals
can be right-censored, survival analysis is the natural approach to estimation.
Estimates of the contact interval distribution can be used to estimate R_0 in
both mass-action and network-based models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.3389</identifier>
 <datestamp>2012-02-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.3389</id><created>2009-12-17</created><authors><author><keyname>Leonenko</keyname><forenames>Nikolai</forenames></author><author><keyname>Sakhno</keyname><forenames>Ludmila</forenames></author></authors><title>On spectral representations of tensor random fields on the sphere</title><categories>math.PR astro-ph.CO math.ST stat.TH</categories><comments>21 pages</comments><msc-class>60G60, 60B15, 62M15</msc-class><journal-ref>Stochastic Analysis and Applications, Vol. 30 (2012), 44-66</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the representations of tensor random fields on the sphere basing on
the theory of representations of the rotation group. Introducing specific
components of a tensor field and imposing the conditions of weak isotropy and
mean square continuity, we derive their spectral decompositions in terms of
generalized spherical functions. The properties of random coefficients of the
decompositions are characterized, including such an important question as
conditions of Gaussianity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.3604</identifier>
 <datestamp>2010-10-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.3604</id><created>2009-12-18</created><updated>2010-10-03</updated><authors><author><keyname>Mannor</keyname><forenames>Shie</forenames><affiliation>EE-Technion</affiliation></author><author><keyname>Stoltz</keyname><forenames>Gilles</forenames><affiliation>DMA, GREGH</affiliation></author></authors><title>A Geometric Proof of Calibration</title><categories>stat.ML</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide yet another proof of the existence of calibrated forecasters; it
has two merits. First, it is valid for an arbitrary finite number of outcomes.
Second, it is short and simple and it follows from a direct application of
Blackwell's approachability theorem to carefully chosen vector-valued payoff
function and convex target set. Our proof captures the essence of existing
proofs based on approachability (e.g., the proof by Foster, 1999 in case of
binary outcomes) and highlights the intrinsic connection between
approachability and calibration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.3878</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.3878</id><created>2009-12-19</created><updated>2014-02-11</updated><authors><author><keyname>Wood</keyname><forenames>Michael</forenames></author></authors><title>P values, confidence intervals, or confidence levels for hypotheses?</title><categories>stat.ME</categories><comments>The essential argument is unchanged from previous versions, but the
  paper has been largely rewritten, the argument extended, and more examples
  and background context included. 21 pages, 3 diagrams, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Null hypothesis significance tests and p values are widely used despite very
strong arguments against their use in many contexts. Confidence intervals are
often recommended as an alternative, but these do not achieve the objective of
assessing the credibility of a hypothesis, and the distinction between
confidence and probability is an unnecessary confusion. This paper proposes a
more straightforward (probabilistic) definition of confidence, and suggests how
the idea can be applied to whatever hypotheses are of interest to researchers.
The relative merits of the different approaches are discussed using a series of
illustrative examples: usually confidence based approaches seem more
transparent and useful, but there are some contexts in which p values may be
appropriate. I also suggest some methods for converting results from one format
to another. (The attractiveness of the idea of confidence is demonstrated by
the widespread persistence of the completely incorrect idea that p=5% is
equivalent to 95% confidence in the alternative hypothesis. In this paper I
show how p values can be used to derive meaningful confidence statements, and
the assumptions underlying the derivation.) Key words: Confidence interval,
Confidence level, Hypothesis testing, Null hypothesis significance tests, P
value, User friendliness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.3880</identifier>
 <datestamp>2012-07-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.3880</id><created>2009-12-19</created><updated>2012-07-06</updated><authors><author><keyname>Wood</keyname><forenames>Michael</forenames></author></authors><title>Bootstrapping Confidence Levels for Hypotheses about Quadratic
  (U-Shaped) Regression Models</title><categories>stat.ME stat.CO</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bootstrapping can produce confidence levels for hypotheses about quadratic
regression models - such as whether the U-shape is inverted, and the location
of optima. The method has several advantages over conventional methods: it
provides more, and clearer, information, and is flexible - it could easily be
applied to a wide variety of different types of models. The utility of the
method can be enhanced by formulating models with interpretable coefficients,
such as the location and value of the optimum. Keywords: Bootstrap resampling;
Confidence level; Quadratic model; Regression, U-shape.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.3891</identifier>
 <datestamp>2011-11-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.3891</id><created>2009-12-19</created><updated>2010-09-29</updated><authors><author><keyname>Cardot</keyname><forenames>Herv</forenames></author><author><keyname>Josserand</keyname><forenames>Etienne</forenames></author></authors><title>Horvitz-Thompson estimators for functional data: asymptotic confidence
  bands and optimal allocation for stratified sampling</title><categories>stat.ME stat.AP</categories><comments>Accepted for publication in Biometrika</comments><journal-ref>Biometrika, 2011, vol. 98, pages 107-118</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When dealing with very large datasets of functional data, survey sampling
approaches are useful in order to obtain estimators of simple functional
quantities, without being obliged to store all the data. We propose here a
Horvitz--Thompson estimator of the mean trajectory. In the context of a
superpopulation framework, we prove under mild regularity conditions that we
obtain uniformly consistent estimators of the mean function and of its variance
function. With additional assumptions on the sampling design we state a
functional Central Limit Theorem and deduce asymptotic confidence bands.
Stratified sampling is studied in detail, and we also obtain a functional
version of the usual optimal allocation rule considering a mean variance
criterion. These techniques are illustrated by means of a test population of
N=18902 electricity meters for which we have individual electricity consumption
measures every 30 minutes over one week. We show that stratification can
substantially improve both the accuracy of the estimators and reduce the width
of the global confidence bands compared to simple random sampling without
replacement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.4269</identifier>
 <datestamp>2011-06-17</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.4269</id><created>2009-12-21</created><updated>2011-06-16</updated><authors><author><keyname>Shafer</keyname><forenames>Glenn</forenames></author><author><keyname>Shen</keyname><forenames>Alexander</forenames></author><author><keyname>Vereshchagin</keyname><forenames>Nikolai</forenames></author><author><keyname>Vovk</keyname><forenames>Vladimir</forenames></author></authors><title>Test Martingales, Bayes Factors and $p$-Values</title><categories>math.ST stat.ME stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-STS347 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS347</report-no><journal-ref>Statistical Science 2011, Vol. 26, No. 1, 84-101</journal-ref><doi>10.1214/10-STS347</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A nonnegative martingale with initial value equal to one measures evidence
against a probabilistic hypothesis. The inverse of its value at some stopping
time can be interpreted as a Bayes factor. If we exaggerate the evidence by
considering the largest value attained so far by such a martingale, the
exaggeration will be limited, and there are systematic ways to eliminate it.
The inverse of the exaggerated value at some stopping time can be interpreted
as a $p$-value. We give a simple characterization of all increasing functions
that eliminate the exaggeration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.4387</identifier>
 <datestamp>2010-09-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.4387</id><created>2009-12-22</created><updated>2010-09-13</updated><authors><author><keyname>Abramovich</keyname><forenames>Felix</forenames></author><author><keyname>Grinshtein</keyname><forenames>Vadim</forenames></author></authors><title>MAP model selection in Gaussian regression</title><categories>math.ST stat.TH</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a Bayesian approach to model selection in Gaussian linear
regression, where the number of predictors might be much larger than the number
of observations. From a frequentist view, the proposed procedure results in the
penalized least squares estimation with a complexity penalty associated with a
prior on the model size. We investigate the optimality properties of the
resulting estimator. We establish the oracle inequality and specify conditions
on the prior that imply its asymptotic minimaxity within a wide range of sparse
and dense settings for "nearly-orthogonal" and "multicollinear" designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.4434</identifier>
 <datestamp>2010-05-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.4434</id><created>2009-12-22</created><updated>2010-05-12</updated><authors><author><keyname>Chiquet</keyname><forenames>Julien</forenames></author><author><keyname>Grandvalet</keyname><forenames>Yves</forenames></author><author><keyname>Ambroise</keyname><forenames>Christophe</forenames></author></authors><title>Inferring Multiple Graphical Structures</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian Graphical Models provide a convenient framework for representing
dependencies between variables. Recently, this tool has received a high
interest for the discovery of biological networks. The literature focuses on
the case where a single network is inferred from a set of measurements, but, as
wetlab data is typically scarce, several assays, where the experimental
conditions affect interactions, are usually merged to infer a single network.
In this paper, we propose two approaches for estimating multiple related
graphs, by rendering the closeness assumption into an empirical prior or group
penalties. We provide quantitative results demonstrating the benefits of the
proposed approaches. The methods presented in this paper are embeded in the R
package 'simone' from version 1.0-0 and later.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.4480</identifier>
 <datestamp>2011-03-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.4480</id><created>2009-12-22</created><updated>2011-03-09</updated><authors><author><keyname>Douc</keyname><forenames>Randal</forenames></author><author><keyname>Moulines</keyname><forenames>Eric</forenames></author><author><keyname>Olsson</keyname><forenames>Jimmy</forenames></author><author><keyname>van Handel</keyname><forenames>Ramon</forenames></author></authors><title>Consistency of the maximum likelihood estimator for general hidden
  Markov models</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS834 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS834</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 1, 474-513</journal-ref><doi>10.1214/10-AOS834</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a parametrized family of general hidden Markov models, where both
the observed and unobserved components take values in a complete separable
metric space. We prove that the maximum likelihood estimator (MLE) of the
parameter is strongly consistent under a rather minimal set of assumptions. As
special cases of our main result, we obtain consistency in a large class of
nonlinear state space models, as well as general results on linear Gaussian
state space models and finite state models. A novel aspect of our approach is
an information-theoretic technique for proving identifiability, which does not
require an explicit representation for the relative entropy rate. Our method of
proof could therefore form a foundation for the investigation of MLE
consistency in more general dependent and non-Markovian time series. Also of
independent interest is a general concentration inequality for $V$-uniformly
ergodic Markov chains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.4489</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.4489</id><created>2009-12-22</created><updated>2012-08-14</updated><authors><author><keyname>Serdyukova</keyname><forenames>Nora</forenames></author></authors><title>Spatial adaptation in heteroscedastic regression: Propagation approach</title><categories>math.ST stat.TH</categories><comments>47 pages. This is the final version of the paper published in at
  http://dx.doi.org/10.1214/08-EJS180 the Electronic Journal of Statistics
  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics
  (http://www.imstat.org)</comments><msc-class>62G05, 62G08</msc-class><journal-ref>Electron. J. Stat., Vol. 6 (2012), 861-907</journal-ref><doi>10.1214/12-EJS693</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper concerns the problem of pointwise adaptive estimation in regression
when the noise is heteroscedastic and incorrectly known. The use of the local
approximation method, which includes the local polynomial smoothing as a
particular case, leads to a finite family of estimators corresponding to
different degrees of smoothing. Data-driven choice of localization degree in
this case can be understood as the problem of selection from this family. This
task can be performed by a suggested in Katkovnik and Spokoiny (2008) FLL
technique based on Lepski's method. An important issue with this type of
procedures - the choice of certain tuning parameters - was addressed in
Spokoiny and Vial (2009). The authors called their approach to the parameter
calibration "propagation". In the present paper the propagation approach is
developed and justified for the heteroscedastic case in presence of the noise
misspecification. Our analysis shows that the adaptive procedure allows a
misspecification of the covariance matrix with a relative error of order
1/log(n), where n is the sample size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.4566</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.4566</id><created>2009-12-23</created><updated>2011-09-06</updated><authors><author><keyname>Shea</keyname><forenames>Brian P.</forenames></author><author><keyname>Jones</keyname><forenames>Galin L.</forenames></author></authors><title>Evaluating Default Priors with a Generalization of Eaton's Markov Chain</title><categories>math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider evaluating improper priors in a formal Bayes setting according to
the consequences of their use. Let $\Phi$ be a class of functions on the
parameter space and consider estimating elements of $\Phi$ under quadratic
loss. If the formal Bayes estimator of every function in $\Phi$ is admissible,
then the prior is strongly admissible with respect to $\Phi$. Eaton's method
for establishing strong admissibility is based on studying the stability
properties of a particular Markov chain associated with the inferential
setting. In previous work, this was handled differently depending upon whether
$\phi \in \Phi$ was bounded or unbounded. We introduce and study a new Markov
chain which allows us to unify and generalize existing approaches while
simultaneously broadening the scope of their potential applicability. To
illustrate the method, we establish strong admissibility conditions when the
model is a $p$-dimensional multivariate normal distribution with unknown mean
vector $\theta$ and the prior is of the form $\nu(\|\theta\|^{2})d\theta$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.4688</identifier>
 <datestamp>2010-12-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.4688</id><created>2009-12-23</created><updated>2010-12-03</updated><authors><author><keyname>Lvy-Leduc</keyname><forenames>Cline</forenames><affiliation>LTCI</affiliation></author><author><keyname>Boistard</keyname><forenames>Hlne</forenames><affiliation>GREMAQ</affiliation></author><author><keyname>Moulines</keyname><forenames>Eric</forenames><affiliation>LTCI</affiliation></author><author><keyname>Taqqu</keyname><forenames>Murad S.</forenames></author><author><keyname>Reisen</keyname><forenames>Valderio A.</forenames></author></authors><title>Asymptotic properties of U-processes under long-range dependence</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $(X_i)_{i\geq 1}$ be a stationary mean-zero Gaussian process with
covariances $\rho(k)=\PE(X_{1}X_{k+1})$ satisfying: $\rho(0)=1$ and
$\rho(k)=k^{-D} L(k)$ where $D$ is in $(0,1)$ and $L$ is slowly varying at
infinity. Consider the $U$-process $\{U_n(r),\; r\in I\}$ defined as $$
U_n(r)=\frac{1}{n(n-1)}\sum_{1\leq i\neq j\leq n}\1_{\{G(X_i,X_j)\leq r\}}\; ,
$$ where $I$ is an interval included in $\rset$ and $G$ is a symmetric
function. In this paper, we provide central and non-central limit theorems for
$U_n$. They are used to derive the asymptotic behavior of the Hodges-Lehmann
estimator, the Wilcoxon-signed rank statistic, the sample correlation integral
and an associated scale estimator. The limiting distributions are expressed
through multiple Wiener-It\^o integrals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.4743</identifier>
 <datestamp>2012-02-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.4743</id><created>2009-12-23</created><updated>2012-02-17</updated><authors><author><keyname>Kuznetsov</keyname><forenames>A.</forenames></author><author><keyname>Kyprianou</keyname><forenames>A. E.</forenames></author><author><keyname>Pardo</keyname><forenames>J. C.</forenames></author><author><keyname>van Schaik</keyname><forenames>K.</forenames></author></authors><title>A Wiener--Hopf Monte Carlo simulation technique for L\'{e}vy processes</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AAP746 the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP746</report-no><journal-ref>Annals of Applied Probability 2011, Vol. 21, No. 6, 2171-2190</journal-ref><doi>10.1214/10-AAP746</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a completely new and straightforward method for simulating the
joint law of the position and running maximum at a fixed time of a general
L\'{e}vy process with a view to application in insurance and financial
mathematics. Although different, our method takes lessons from Carr's so-called
"Canadization" technique as well as Doney's method of stochastic bounds for
L\'{e}vy processes; see Carr [Rev. Fin. Studies 11 (1998) 597--626] and Doney
[Ann. Probab. 32 (2004) 1545-1552]. We rely fundamentally on the Wiener-Hopf
decomposition for L\'{e}vy processes as well as taking advantage of recent
developments in factorization techniques of the latter theory due to Vigon
[Simplifiez vos L\'{e}vy en titillant la factorization de Wiener-Hopf (2002)
Laboratoire de Math\'{e}matiques de L'INSA de Rouen] and Kuznetsov [Ann. Appl.
Probab. 20 (2010) 1801--1830]. We illustrate our Wiener--Hopf Monte Carlo
method on a number of different processes, including a new family of L\'{e}vy
processes called hypergeometric L\'{e}vy processes. Moreover, we illustrate the
robustness of working with a Wiener--Hopf decomposition with two extensions.
The first extension shows that if one can successfully simulate for a given
L\'{e}vy processes then one can successfully simulate for any independent sum
of the latter process and a compound Poisson process. The second extension
illustrates how one may produce a straightforward approximation for simulating
the two-sided exit problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.4883</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.4883</id><created>2009-12-24</created><authors><author><keyname>Ryabko</keyname><forenames>Daniil</forenames><affiliation>INRIA Futurs, Lifl</affiliation></author></authors><title>On Finding Predictors for Arbitrary Families of Processes</title><categories>cs.LG cs.AI cs.IT math.IT math.ST stat.TH</categories><proxy>ccsd inria-00442881</proxy><journal-ref>Journal of Machine Learning Research 11 (2010) 581-602</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem is sequence prediction in the following setting. A sequence
$x_1,...,x_n,...$ of discrete-valued observations is generated according to
some unknown probabilistic law (measure) $\mu$. After observing each outcome,
it is required to give the conditional probabilities of the next observation.
The measure $\mu$ belongs to an arbitrary but known class $C$ of stochastic
process measures. We are interested in predictors $\rho$ whose conditional
probabilities converge (in some sense) to the "true" $\mu$-conditional
probabilities if any $\mu\in C$ is chosen to generate the sequence. The
contribution of this work is in characterizing the families $C$ for which such
predictors exist, and in providing a specific and simple form in which to look
for a solution. We show that if any predictor works, then there exists a
Bayesian predictor, whose prior is discrete, and which works too. We also find
several sufficient and necessary conditions for the existence of a predictor,
in terms of topological characterizations of the family $C$, as well as in
terms of local behaviour of the measures in $C$, which in some cases lead to
procedures for constructing such predictors. It should be emphasized that the
framework is completely general: the stochastic processes considered are not
required to be i.i.d., stationary, or to belong to any parametric or countable
family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.5013</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.5013</id><created>2009-12-26</created><authors><author><keyname>Chernozhukov</keyname><forenames>Victor</forenames></author><author><keyname>Fernandez-Val</keyname><forenames>Ivan</forenames></author></authors><title>Inference for Extremal Conditional Quantile Models, with an Application
  to Market and Birthweight Risks</title><categories>stat.ME math.ST q-fin.RM stat.TH</categories><comments>41 pages, 9 figures</comments><journal-ref>Review of Economic Studies (2011) 78 (2): 559-589</journal-ref><doi>10.1093/restud/rdq020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantile regression is an increasingly important empirical tool in economics
and other sciences for analyzing the impact of a set of regressors on the
conditional distribution of an outcome. Extremal quantile regression, or
quantile regression applied to the tails, is of interest in many economic and
financial applications, such as conditional value-at-risk, production
efficiency, and adjustment bands in (S,s) models. In this paper we provide
feasible inference tools for extremal conditional quantile models that rely
upon extreme value approximations to the distribution of self-normalized
quantile regression statistics. The methods are simple to implement and can be
of independent interest even in the non-regression case. We illustrate the
results with two empirical examples analyzing extreme fluctuations of a stock
return and extremely low percentiles of live infants' birthweights in the range
between 250 and 1500 grams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.5193</identifier>
 <datestamp>2013-08-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.5193</id><created>2009-12-28</created><updated>2013-08-29</updated><authors><author><keyname>Silva</keyname><forenames>Ricardo</forenames></author><author><keyname>Heller</keyname><forenames>Katherine</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author><author><keyname>Airoldi</keyname><forenames>Edoardo M.</forenames></author></authors><title>Ranking relations using analogies in biological and information networks</title><categories>stat.ME cs.LG physics.soc-ph q-bio.QM stat.AP</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOAS321 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS321</report-no><journal-ref>Annals of Applied Statistics 2010, Vol. 4, No. 2, 615-644</journal-ref><doi>10.1214/09-AOAS321</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analogical reasoning depends fundamentally on the ability to learn and
generalize about relations between objects. We develop an approach to
relational learning which, given a set of pairs of objects
$\mathbf{S}=\{A^{(1)}:B^{(1)},A^{(2)}:B^{(2)},\ldots,A^{(N)}:B ^{(N)}\}$,
measures how well other pairs A:B fit in with the set $\mathbf{S}$. Our work
addresses the following question: is the relation between objects A and B
analogous to those relations found in $\mathbf{S}$? Such questions are
particularly relevant in information retrieval, where an investigator might
want to search for analogous pairs of objects that match the query set of
interest. There are many ways in which objects can be related, making the task
of measuring analogies very challenging. Our approach combines a similarity
measure on function spaces with Bayesian analysis to produce a ranking. It
requires data containing features of the objects of interest and a link matrix
specifying which relationships exist; no further attributes of such
relationships are necessary. We illustrate the potential of our method on text
analysis and information networks. An application on discovering functional
interactions between pairs of proteins is discussed in detail, where we show
that our approach can work in practice even if a small set of protein pairs is
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.5200</identifier>
 <datestamp>2011-07-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.5200</id><created>2009-12-28</created><updated>2010-06-30</updated><authors><author><keyname>Bradic</keyname><forenames>Jelena</forenames></author><author><keyname>Fan</keyname><forenames>Jianqing</forenames></author><author><keyname>Wang</keyname><forenames>Weiwei</forenames></author></authors><title>Penalized Composite Quasi-Likelihood for Ultrahigh-Dimensional Variable
  Selection</title><categories>stat.ME math.ST stat.TH</categories><journal-ref>Journal of Royal Statistical Society: Series B (2011), 73(3), p.
  325-349</journal-ref><doi>10.1111/j.1467-9868.2010.00764.x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In high-dimensional model selection problems, penalized simple least-square
approaches have been extensively used. This paper addresses the question of
both robustness and efficiency of penalized model selection methods, and
proposes a data-driven weighted linear combination of convex loss functions,
together with weighted $L_1$-penalty. It is completely data-adaptive and does
not require prior knowledge of the error distribution. The weighted
$L_1$-penalty is used both to ensure the convexity of the penalty term and to
ameliorate the bias caused by the $L_1$-penalty. In the setting with
dimensionality much larger than the sample size, we establish a strong oracle
property of the proposed method that possesses both the model selection
consistency and estimation efficiency for the true non-zero coefficients. As
specific examples, we introduce a robust method of composite L1-L2, and optimal
composite quantile method and evaluate their performance in both simulated and
real data examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.5303</identifier>
 <datestamp>2010-04-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.5303</id><created>2009-12-29</created><updated>2010-04-05</updated><authors><author><keyname>Azzalini</keyname><forenames>Adelchi</forenames></author></authors><title>Selection models under generalized symmetry settings</title><categories>stat.ME math.PR math.ST stat.TH</categories><comments>13 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An active stream of literature has followed up the idea of skew-elliptical
densities initiated by Azzalini and Capitanio (1999). Their original
formulation was based on a general lemma which is however of broader
applicability than usually perceived. This note examines new directions of its
use, and illustrates them with the construction of some probability
distributions falling outside the family of the so-called skew-symmetric
densities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.5337</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.5337</id><created>2009-12-29</created><updated>2012-11-23</updated><authors><author><keyname>Balkema</keyname><forenames>Guus</forenames></author><author><keyname>Embrechts</keyname><forenames>Paul</forenames></author><author><keyname>Nolde</keyname><forenames>Natalia</forenames></author></authors><title>Sensitivity of the limit shape of sample clouds from meta densities</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/11-BEJ370 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ370</report-no><journal-ref>Bernoulli 2012, Vol. 18, No. 4, 1386-1404</journal-ref><doi>10.3150/11-BEJ370</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper focuses on a class of light-tailed multivariate probability
distributions. These are obtained via a transformation of the margins from a
heavy-tailed original distribution. This class was introduced in Balkema et al.
(J. Multivariate Anal. 101 (2010) 1738-1754). As shown there, for the
light-tailed meta distribution the sample clouds, properly scaled, converge
onto a deterministic set. The shape of the limit set gives a good description
of the relation between extreme observations in different directions. This
paper investigates how sensitive the limit shape is to changes in the
underlying heavy-tailed distribution. Copulas fit in well with multivariate
extremes. By Galambos's theorem, existence of directional derivatives in the
upper endpoint of the copula is necessary and sufficient for convergence of the
multivariate extremes provided the marginal maxima converge. The copula of the
max-stable limit distribution does not depend on the margins. So margins seem
to play a subsidiary role in multivariate extremes. The theory and examples
presented in this paper cast a different light on the significance of margins.
For light-tailed meta distributions, the asymptotic behaviour is very sensitive
to perturbations of the underlying heavy-tailed original distribution, it may
change drastically even when the asymptotic behaviour of the heavy-tailed
density is not affected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.5338</identifier>
 <datestamp>2011-05-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.5338</id><created>2009-12-29</created><updated>2011-05-13</updated><authors><author><keyname>Rohde</keyname><forenames>Angelika</forenames></author><author><keyname>Tsybakov</keyname><forenames>Alexandre B.</forenames></author></authors><title>Estimation of high-dimensional low-rank matrices</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS860 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS860</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 2, 887-930</journal-ref><doi>10.1214/10-AOS860</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that we observe entries or, more generally, linear combinations of
entries of an unknown $m\times T$-matrix $A$ corrupted by noise. We are
particularly interested in the high-dimensional setting where the number $mT$
of unknown entries can be much larger than the sample size $N$. Motivated by
several applications, we consider estimation of matrix $A$ under the assumption
that it has small rank. This can be viewed as dimension reduction or sparsity
assumption. In order to shrink toward a low-rank representation, we investigate
penalized least squares estimators with a Schatten-$p$ quasi-norm penalty term,
$p\leq1$. We study these estimators under two possible assumptions---a modified
version of the restricted isometry condition and a uniform bound on the ratio
"empirical norm induced by the sampling operator/Frobenius norm." The main
results are stated as nonasymptotic upper bounds on the prediction risk and on
the Schatten-$q$ risk of the estimators, where $q\in[p,2]$. The rates that we
obtain for the prediction risk are of the form $rm/N$ (for $m=T$), up to
logarithmic factors, where $r$ is the rank of $A$. The particular examples of
multi-task learning and matrix completion are worked out in detail. The proofs
are based on tools from the theory of empirical processes. As a by-product, we
derive bounds for the $k$th entropy numbers of the quasi-convex Schatten class
embeddings $S_p^M\hookrightarrow S_2^M$, $p&lt;1$, which are of independent
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.5467</identifier>
 <datestamp>2010-11-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.5467</id><created>2009-12-30</created><updated>2010-11-25</updated><authors><author><keyname>Sagnol</keyname><forenames>Guillaume</forenames></author></authors><title>Computing Optimal Designs of multiresponse Experiments reduces to
  Second-Order Cone Programming</title><categories>stat.ME math.OC math.ST stat.TH</categories><msc-class>62K05, 90C25, 90C46</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Elfving's Theorem is a major result in the theory of optimal experimental
design, which gives a geometrical characterization of $c-$optimality. In this
paper, we extend this theorem to the case of multiresponse experiments, and we
show that when the number of experiments is finite, $c-,A-,T-$ and $D-$optimal
design of multiresponse experiments can be computed by Second-Order Cone
Programming (SOCP). Moreover, our SOCP approach can deal with design problems
in which the variable is subject to several linear constraints.
  We give two proofs of this generalization of Elfving's theorem. One is based
on Lagrangian dualization techniques and relies on the fact that the
semidefinite programming (SDP) formulation of the multiresponse $c-$optimal
design always has a solution which is a matrix of rank $1$. Therefore, the
complexity of this problem fades.
  We also investigate a \emph{model robust} generalization of $c-$optimality,
for which an Elfving-type theorem was established by Dette (1993). We show with
the same Lagrangian approach that these model robust designs can be computed
efficiently by minimizing a geometric mean under some norm constraints.
Moreover, we show that the optimality conditions of this geometric programming
problem yield an extension of Dette's theorem to the case of multiresponse
experiments.
  When the number of unknown parameters is small, or when the number of linear
functions of the parameters to be estimated is small, we show by numerical
examples that our approach can be between 10 and 1000 times faster than the
classic, state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.5489</identifier>
 <datestamp>2011-05-31</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.5489</id><created>2009-12-29</created><updated>2011-05-30</updated><authors><author><keyname>Belloni</keyname><forenames>Alexandre</forenames></author><author><keyname>Winkler</keyname><forenames>Robert L.</forenames></author></authors><title>On multivariate quantiles under partial orders</title><categories>math.ST math.PR stat.ME stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS863 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS863</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 2, 1125-1179</journal-ref><doi>10.1214/10-AOS863</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on generalizing quantiles from the ordering point of view.
We propose the concept of partial quantiles, which are based on a given partial
order. We establish that partial quantiles are equivariant under
order-preserving transformations of the data, robust to outliers, characterize
the probability distribution if the partial order is sufficiently rich,
generalize the concept of efficient frontier, and can measure dispersion from
the partial order perspective. We also study several statistical aspects of
partial quantiles. We provide estimators, associated rates of convergence, and
asymptotic distributions that hold uniformly over a continuum of quantile
indices. Furthermore, we provide procedures that can restore monotonicity
properties that might have been disturbed by estimation error, establish
computational complexity bounds, and point out a concentration of measure
phenomenon (the latter under independence and the componentwise natural order).
Finally, we illustrate the concepts by discussing several theoretical examples
and simulations. Empirical applications to compare intake nutrients within
diets, to evaluate the performance of investment funds, and to study the impact
of policies on tobacco awareness are also presented to illustrate the concepts
and their use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:0912.5507</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>0912.5507</id><created>2009-12-30</created><authors><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Ahmed</keyname><forenames>Amr</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>MedLDA: A General Framework of Maximum Margin Supervised Topic Models</title><categories>stat.ML stat.ME</categories><comments>27 Pages</comments><journal-ref>Journal of Machine Learning Research, 13(Aug): 2237--2278, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervised topic models utilize document's side information for discovering
predictive low dimensional representations of documents. Existing models apply
the likelihood-based estimation. In this paper, we present a general framework
of max-margin supervised topic models for both continuous and categorical
response variables. Our approach, the maximum entropy discrimination latent
Dirichlet allocation (MedLDA), utilizes the max-margin principle to train
supervised topic models and estimate predictive topic representations that are
arguably more suitable for prediction tasks. The general principle of MedLDA
can be applied to perform joint max-margin learning and maximum likelihood
estimation for arbitrary topic models, directed or undirected, and supervised
or unsupervised, when the supervised side information is available. We develop
efficient variational methods for posterior inference and parameter estimation,
and demonstrate qualitatively and quantitatively the advantages of MedLDA over
likelihood-based topic models on movie review and 20 Newsgroups data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.0036</identifier>
 <datestamp>2010-04-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.0036</id><created>2009-12-30</created><authors><author><keyname>Haslinger</keyname><forenames>Robert</forenames></author><author><keyname>Klinkner</keyname><forenames>Kristina Lisa</forenames></author><author><keyname>Shalizi</keyname><forenames>Cosma Rohilla</forenames></author></authors><title>The Computational Structure of Spike Trains</title><categories>q-bio.NC cs.IT math.IT nlin.AO physics.data-an stat.ML</categories><comments>Somewhat different format from journal version but same content</comments><journal-ref>Neural Computation, vol. 22 (2010), pp. 121--157</journal-ref><doi>10.1162/neco.2009.12-07-678</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neurons perform computations, and convey the results of those computations
through the statistical structure of their output spike trains. Here we present
a practical method, grounded in the information-theoretic analysis of
prediction, for inferring a minimal representation of that structure and for
characterizing its complexity. Starting from spike trains, our approach finds
their causal state models (CSMs), the minimal hidden Markov models or
stochastic automata capable of generating statistically identical time series.
We then use these CSMs to objectively quantify both the generalizable structure
and the idiosyncratic randomness of the spike train. Specifically, we show that
the expected algorithmic information content (the information needed to
describe the spike train exactly) can be split into three parts describing (1)
the time-invariant structure (complexity) of the minimal spike-generating
process, which describes the spike train statistically; (2) the randomness
(internal entropy rate) of the minimal spike-generating process; and (3) a
residual pure noise term not described by the minimal spike-generating process.
We use CSMs to approximate each of these quantities. The CSMs are inferred
nonparametrically from the data, making only mild regularity assumptions, via
the causal state splitting reconstruction algorithm. The methods presented here
complement more traditional spike train analyses by describing not only spiking
probability and spike train entropy, but also the complexity of a spike train's
structure. We demonstrate our approach using both simulated spike trains and
experimental data recorded in rat barrel cortex during vibrissa stimulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.0160</identifier>
 <datestamp>2010-08-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.0160</id><created>2009-12-31</created><updated>2010-08-19</updated><authors><author><keyname>Adams</keyname><forenames>Ryan Prescott</forenames></author><author><keyname>Wallach</keyname><forenames>Hanna M.</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>Learning the Structure of Deep Sparse Graphical Models</title><categories>stat.ML</categories><comments>20 pages, 6 figures, AISTATS 2010, Revised</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep belief networks are a powerful way to model complex probability
distributions. However, learning the structure of a belief network,
particularly one with hidden units, is difficult. The Indian buffet process has
been used as a nonparametric Bayesian prior on the directed structure of a
belief network with a single infinitely wide hidden layer. In this paper, we
introduce the cascading Indian buffet process (CIBP), which provides a
nonparametric prior on the structure of a layered, directed belief network that
is unbounded in both depth and width, yet allows tractable inference. We use
the CIBP prior with the nonlinear Gaussian belief network so each unit can
additionally vary its behavior between discrete and continuous representations.
We provide Markov chain Monte Carlo algorithms for inference in these belief
networks and explore the structures learned on several image data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.0175</identifier>
 <datestamp>2010-03-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.0175</id><created>2009-12-31</created><updated>2010-03-19</updated><authors><author><keyname>Murray</keyname><forenames>Iain</forenames></author><author><keyname>Adams</keyname><forenames>Ryan Prescott</forenames></author><author><keyname>MacKay</keyname><forenames>David J. C.</forenames></author></authors><title>Elliptical slice sampling</title><categories>stat.CO stat.ML</categories><comments>8 pages, 6 figures, appearing in AISTATS 2010 (JMLR: W&amp;CP volume 6).
  Differences from first submission: some minor edits in response to feedback.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many probabilistic models introduce strong dependencies between variables
using a latent multivariate Gaussian distribution or a Gaussian process. We
present a new Markov chain Monte Carlo algorithm for performing inference in
models with multivariate Gaussian priors. Its key properties are: 1) it has
simple, generic code applicable to many models, 2) it has no free parameters,
3) it works well for a variety of Gaussian process based models. These
properties make our method ideal for use while model building, removing the
need to spend time deriving and tuning updates for more complex algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.0188</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.0188</id><created>2009-12-31</created><updated>2013-03-20</updated><authors><author><keyname>Belloni</keyname><forenames>Alexandre</forenames></author><author><keyname>Chernozhukov</keyname><forenames>Victor</forenames></author></authors><title>Least squares after model selection in high-dimensional sparse models</title><categories>math.ST math.PR stat.ME stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/11-BEJ410 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ410</report-no><journal-ref>Bernoulli 2013, Vol. 19, No. 2, 521-547</journal-ref><doi>10.3150/11-BEJ410</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we study post-model selection estimators that apply ordinary
least squares (OLS) to the model selected by first-step penalized estimators,
typically Lasso. It is well known that Lasso can estimate the nonparametric
regression function at nearly the oracle rate, and is thus hard to improve
upon. We show that the OLS post-Lasso estimator performs at least as well as
Lasso in terms of the rate of convergence, and has the advantage of a smaller
bias. Remarkably, this performance occurs even if the Lasso-based model
selection "fails" in the sense of missing some components of the "true"
regression model. By the "true" model, we mean the best s-dimensional
approximation to the nonparametric regression function chosen by the oracle.
Furthermore, OLS post-Lasso estimator can perform strictly better than Lasso,
in the sense of a strictly faster rate of convergence, if the Lasso-based model
selection correctly includes all components of the "true" model as a subset and
also achieves sufficient sparsity. In the extreme case, when Lasso perfectly
selects the "true" model, the OLS post-Lasso estimator becomes the oracle
estimator. An important ingredient in our analysis is a new sparsity bound on
the dimension of the model selected by Lasso, which guarantees that this
dimension is at most of the same order as the dimension of the "true" model.
Our rate results are nonasymptotic and hold in both parametric and
nonparametric models. Moreover, our analysis is not limited to the Lasso
estimator acting as a selector in the first step, but also applies to any other
estimator, for example, various forms of thresholded Lasso, with good rates and
good sparsity properties. Our analysis covers both traditional thresholding and
a new practical, data-driven thresholding scheme that induces additional
sparsity subject to maintaining a certain goodness of fit. The latter scheme
has theoretical guarantees similar to those of Lasso or OLS post-Lasso, but it
dominates those procedures as well as traditional thresholding in a wide
variety of experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.0200</identifier>
 <datestamp>2010-01-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.0200</id><created>2009-12-31</created><authors><author><keyname>Gao</keyname><forenames>Fuchang</forenames><affiliation>University of Idaho</affiliation></author><author><keyname>Li</keyname><forenames>Wenbo V.</forenames><affiliation>University of Delaware</affiliation></author><author><keyname>Wellner</keyname><forenames>Jon A.</forenames><affiliation>University of Washington</affiliation></author></authors><title>How many Laplace transforms of probability measures are there?</title><categories>math.ST stat.TH</categories><comments>15 pages</comments><msc-class>62G05, 60G15 (primary), 41.44, 46B50 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A bracketing metric entropy bound for the class of Laplace transforms of
probability measures on [0,\infty) is obtained through its connection with the
small deviation probability of a smooth Gaussian process. Our results for the
particular smooth Gaussian process seem to be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.0245</identifier>
 <datestamp>2010-01-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.0245</id><created>2010-01-01</created><authors><author><keyname>Cerquetti</keyname><forenames>Annalisa</forenames></author></authors><title>Bayesian nonparametric analysis for a species sampling model with
  finitely many types</title><categories>math.PR math.ST stat.TH</categories><comments>10 pages</comments><msc-class>60G58; 60G09</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive explicit Bayesian nonparametric analysis for a species sampling
model with finitely many types of Gibbs form of type $\alpha= -1$ recently
introduced in Gnedin (2009). Our results complement existing analysis under
Gibbs priors of type $\alpha \in [0, 1)$ proposed in Lijoi et al. (2008).
Calculations rely on a groups sequential construction of Gibbs partitions
introduced in Cerquetti (2008).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.0279</identifier>
 <datestamp>2010-01-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.0279</id><created>2010-01-01</created><authors><author><keyname>Keshavan</keyname><forenames>Raghunandan H.</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>Regularization for Matrix Completion</title><categories>stat.ML stat.AP</categories><comments>5 pages, 3 figures, Conference Version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of reconstructing a low rank matrix from noisy
observations of a subset of its entries. This task has applications in
statistical learning, computer vision, and signal processing. In these
contexts, "noise" generically refers to any contribution to the data that is
not captured by the low-rank model. In most applications, the noise level is
large compared to the underlying signal and it is important to avoid
overfitting. In order to tackle this problem, we define a regularized cost
function well suited for spectral reconstruction methods. Within a random noise
model, and in the large system limit, we prove that the resulting accuracy
undergoes a phase transition depending on the noise level and on the fraction
of observed entries. The cost function can be minimized using OPTSPACE (a
manifold gradient descent algorithm). Numerical simulations show that this
approach is competitive with state-of-the-art alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.0341</identifier>
 <datestamp>2010-12-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.0341</id><created>2010-01-02</created><updated>2010-10-15</updated><authors><author><keyname>Zhou</keyname><forenames>Qing</forenames></author></authors><title>On Weight Matrix and Free Energy Models for Sequence Motif Detection</title><categories>q-bio.GN stat.AP</categories><comments>23 pages, 1 figure and 4 tables</comments><journal-ref>Journal of Computational Biology, 17 (2010): 1621-1638</journal-ref><doi>10.1089/cmb.2009.0142</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of motif detection can be formulated as the construction of a
discriminant function to separate sequences of a specific pattern from
background. In computational biology, motif detection is used to predict DNA
binding sites of a transcription factor (TF), mostly based on the weight matrix
(WM) model or the Gibbs free energy (FE) model. However, despite the wide
applications, theoretical analysis of these two models and their predictions is
still lacking. We derive asymptotic error rates of prediction procedures based
on these models under different data generation assumptions. This allows a
theoretical comparison between the WM-based and the FE-based predictions in
terms of asymptotic efficiency. Applications of the theoretical results are
demonstrated with empirical studies on ChIP-seq data and protein binding
microarray data. We find that, irrespective of underlying data generation
mechanisms, the FE approach shows higher or comparable predictive power
relative to the WM approach when the number of observed binding sites used for
constructing a discriminant decision is not too small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.0492</identifier>
 <datestamp>2010-01-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.0492</id><created>2010-01-04</created><authors><author><keyname>Karoui</keyname><forenames>Noureddine El</forenames></author></authors><title>The spectrum of kernel random matrices</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/08-AOS648 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS648</report-no><msc-class>62H10 (Primary) 60F99 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 1, 1-50</journal-ref><doi>10.1214/08-AOS648</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We place ourselves in the setting of high-dimensional statistical inference
where the number of variables $p$ in a dataset of interest is of the same order
of magnitude as the number of observations $n$. We consider the spectrum of
certain kernel random matrices, in particular $n\times n$ matrices whose
$(i,j)$th entry is $f(X_i'X_j/p)$ or $f(\Vert X_i-X_j\Vert^2/p)$ where $p$ is
the dimension of the data, and $X_i$ are independent data vectors. Here $f$ is
assumed to be a locally smooth function. The study is motivated by questions
arising in statistics and computer science where these matrices are used to
perform, among other things, nonlinear versions of principal component
analysis. Surprisingly, we show that in high-dimensions, and for the models we
analyze, the problem becomes essentially linear--which is at odds with
heuristics sometimes used to justify the usage of these methods. The analysis
also highlights certain peculiarities of models widely studied in random matrix
theory and raises some questions about their relevance as tools to model
high-dimensional data encountered in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.0597</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.0597</id><created>2010-01-04</created><updated>2011-01-21</updated><authors><author><keyname>Nguyen</keyname><forenames>XuanLong</forenames></author></authors><title>Inference of global clusters from locally distributed data</title><categories>stat.ME cs.LG stat.ML</categories><comments>27 pages, 12 figures</comments><report-no>Technical report 504, Department of Statistics, University of
  Michigan</report-no><journal-ref>Published in Bayesian Analysis, 5(4), 817--846, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of analyzing the heterogeneity of clustering
distributions for multiple groups of observed data, each of which is indexed by
a covariate value, and inferring global clusters arising from observations
aggregated over the covariate domain. We propose a novel Bayesian nonparametric
method reposing on the formalism of spatial modeling and a nested hierarchy of
Dirichlet processes. We provide an analysis of the model properties, relating
and contrasting the notions of local and global clusters. We also provide an
efficient inference algorithm, and demonstrate the utility of our method in
several data examples, including the problem of object tracking and a global
clustering analysis of functional data where the functional identity
information is not available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.0612</identifier>
 <datestamp>2011-02-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.0612</id><created>2010-01-04</created><updated>2011-02-21</updated><authors><author><keyname>Goldstein</keyname><forenames>Larry</forenames></author><author><keyname>Zhang</keyname><forenames>Haimeng</forenames></author></authors><title>A Berry Esseen Theorem for the Lightbulb Process</title><categories>math.PR math.ST stat.TH</categories><comments>38 pages - Version 3 provides a much shorter and completely
  transparent proof of Lemma 3.2. An error in the odd case coupling has been
  corrected, resulting in a somewhat larger constant, and a simpler overall
  expression for the bound. The constant for the even case has been slightly
  improved</comments><msc-class>62E17 (Primary), 60C05, 60B15, 62P10 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the so called lightbulb process, on days $r=1,..., n$, out of $n$
lightbulbs, all initially off, exactly $r$ bulbs, selected uniformly and
independent of the past, have their status changed from off to on, or vice
versa. With $X$ the number of bulbs on at the terminal time $n$, an even
integer, and $\mu=n/2, \sigma^2=Var(X)$, we have $$ \sup_{z \in \mathbb{R}}
|P(\frac{X-\mu}{\sigma} \le z)-P(Z \le z)| \le \frac{n}{2\sigma^2}
\bar{\Delta}_0 + 1.64 \frac{n}{\sigma^3}+ \frac{2}{\sigma} $$ where $Z$ is a
standard normal random variable, and $$ \bar{\Delta}_0 = 1/2\sqrt{n}} +
\frac{1}{2n} + 1/3 e^{-n/2} \qmq {for $n \ge 6$,} $$ yielding a bound of order
$O(n^{-1/2})$ as $n \to \infty$. A similar, though slightly larger bound holds
for $n$ odd. The results are shown using a version of Stein's method for
bounded, monotone size bias couplings. The argument for even $n$ depends on the
construction of a variable $X^s$ on the same space as $X$ that has the $X$-size
bias distribution, that is, that satisfies \beas E [X g(X)] =\mu E[g(X^s)]
\quad for all bounded continuous $g$, \enas and for which there exists a $B \ge
0$, in this case B=2, such that $X \le X^s \le X+B$ almost surely. The argument
for $n$ odd is similar to that for $n$ even, but one first couples $X$ closely
to $V$, a symmetrized version of $X$, for which a size bias coupling of $V$ to
$V^s$ can proceed as in the even case. In both the even and odd cases, the
crucial calculation of the variance of a conditional expectation requires
detailed information on the spectral decomposition of the lightbulb chain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.0736</identifier>
 <datestamp>2010-01-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.0736</id><created>2010-01-05</created><authors><author><keyname>Friedman</keyname><forenames>J.</forenames></author><author><keyname>Hastie</keyname><forenames>T.</forenames></author><author><keyname>Tibshirani</keyname><forenames>R.</forenames></author></authors><title>A note on the group lasso and a sparse group lasso</title><categories>math.ST stat.TH</categories><comments>8 pages, 3 figs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the group lasso penalty for the linear model. We note that the
standard algorithm for solving the problem assumes that the model matrices in
each group are orthonormal. Here we consider a more general penalty that blends
the lasso (L1) with the group lasso ("two-norm"). This penalty yields solutions
that are sparse at both the group and individual feature levels. We derive an
efficient algorithm for the resulting convex problem based on coordinate
descent. This algorithm can also be used to solve the general form of the group
lasso, with non-orthonormal model matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.0863</identifier>
 <datestamp>2010-01-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.0863</id><created>2010-01-06</created><authors><author><keyname>Hosseini</keyname><forenames>Shahram</forenames></author><author><keyname>Deville</keyname><forenames>Yannick</forenames></author></authors><title>Correction to: "Blind maximum likelihood separation of a
  linear-quadratic mixture"</title><categories>math.ST stat.ME stat.TH</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An error occurred in the computation of a gradient in our paper entitled
"Blind maximum likelihood separation of a linear-quadratic mixture", presented
in ICA'2004. The equations (20) in Appendix and (17) in the text were not
correct. The current paper presents the correct version of these equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.0951</identifier>
 <datestamp>2012-02-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.0951</id><created>2010-01-06</created><updated>2010-01-11</updated><authors><author><keyname>Aydin</keyname><forenames>Burcu</forenames></author><author><keyname>Pataki</keyname><forenames>Gabor</forenames></author><author><keyname>Wang</keyname><forenames>Haonan</forenames></author><author><keyname>Ladha</keyname><forenames>Alim</forenames></author><author><keyname>Bullitt</keyname><forenames>Elizabeth</forenames></author><author><keyname>Marron</keyname><forenames>J. S.</forenames></author></authors><title>Visualizing the Structure of Large Trees</title><categories>stat.AP</categories><comments>17 pages, 8 figures</comments><journal-ref>Electronic Journal of Statistics 2011, Vol. 5, 405-420</journal-ref><doi>10.1214/11-EJS612</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study introduces a new method of visualizing complex tree structured
objects. The usefulness of this method is illustrated in the context of
detecting unexpected features in a data set of very large trees. The major
contribution is a novel two-dimensional graphical representation of each tree,
with a covariate coded by color. The motivating data set contains three
dimensional representations of brain artery systems of 105 subjects. Due to
inaccuracies inherent in the medical imaging techniques, issues with the
reconstruction algo- rithms and inconsistencies introduced by manual
adjustment, various discrepancies are present in the data. The proposed
representation enables quick visual detection of the most common discrepancies.
For our driving example, this tool led to the modification of 10% of the artery
trees and deletion of 6.7%. The benefits of our cleaning method are
demonstrated through a statistical hypothesis test on the effects of aging on
vessel structure. The data cleaning resulted in improved significance levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1014</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1014</id><created>2010-01-06</created><updated>2012-11-30</updated><authors><author><keyname>Gervini</keyname><forenames>Daniel</forenames></author></authors><title>Outlier detection and trimmed estimation for general functional data</title><categories>stat.ME</categories><journal-ref>Statistica Sinica, 2012, volume 22, pages 1639-1660</journal-ref><doi>10.5705/ss.2010.282</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces trimmed estimators for the mean and covariance
function of general functional data. The estimators are based on a new measure
of outlyingness or data depth that is well defined on any metric space,
although this paper focuses on Euclidean spaces. We compute the breakdown point
of the estimators and show that the optimal breakdown point is attainable for
the appropriate choice of tuning parameters. The small-sample behavior of the
estimators is studied by simulation, and we show that they have better
outlier-resistance properties than alternative estimators. This is confirmed by
two real-data applications, that also show that the outlyingness measure can be
used as a graphical outlier-detection tool in functional spaces where visual
screening of the data is difficult.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1049</identifier>
 <datestamp>2011-04-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1049</id><created>2010-01-07</created><updated>2010-09-23</updated><authors><author><keyname>Iooss</keyname><forenames>Bertrand</forenames><affiliation>Mthodes d'Analyse Stochastique des Codes et Traitements Numriques</affiliation></author><author><keyname>Boussouf</keyname><forenames>Loc</forenames><affiliation>IFP</affiliation></author><author><keyname>Feuillard</keyname><forenames>Vincent</forenames><affiliation>IFP</affiliation></author><author><keyname>Marrel</keyname><forenames>Amandine</forenames><affiliation>IFP</affiliation></author></authors><title>Numerical studies of the metamodel fitting and validation processes</title><categories>math.NA math.ST stat.AP stat.CO stat.TH</categories><proxy>ccsd</proxy><journal-ref>International Journal of Advances in Systems and Measurements 3
  (2010) 11-21</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex computer codes, for instance simulating physical phenomena, are often
too time expensive to be directly used to perform uncertainty, sensitivity,
optimization and robustness analyses. A widely accepted method to circumvent
this problem consists in replacing cpu time expensive computer models by cpu
inexpensive mathematical functions, called metamodels. In this paper, we focus
on the Gaussian process metamodel and two essential steps of its definition
phase. First, the initial design of the computer code input variables (which
allows to fit the metamodel) has to honor adequate space filling properties. We
adopt a numerical approach to compare the performance of different types of
space filling designs, in the class of the optimal Latin hypercube samples, in
terms of the predictivity of the subsequent fitted metamodel. We conclude that
such samples with minimal wrap-around discrepancy are particularly well-suited
for the Gaussian process metamodel fitting. Second, the metamodel validation
process consists in evaluating the metamodel predictivity with respect to the
initial computer code. We propose and test an algorithm which optimizes the
distance between the validation points and the metamodel learning points in
order to estimate the true metamodel predictivity with a minimum number of
validation points. Comparisons with classical validation algorithms and
application to a nuclear safety computer code show the relevance of this new
sequential validation design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1051</identifier>
 <datestamp>2010-06-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1051</id><created>2010-01-07</created><updated>2010-06-15</updated><authors><author><keyname>Nekrutkin</keyname><forenames>Vladimir</forenames></author></authors><title>Perturbation expansions of signal subspaces for long signals</title><categories>math.NA math.ST stat.TH</categories><comments>61</comments><msc-class>65G99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Singular Spectrum Analysis and many other subspace-based methods of signal
processing are implicitly relying on the assumption of close proximity of
unperturbed and perturbed signal subspaces extracted by the Singular Value
Decomposition of special "signal" and "perturbed signal" matrices. In this
paper, the analysis of the main principal angle between these subspaces is
performed in terms of the perturbation expansions of the corresponding
orthogonal projectors. Applicable upper bounds are derived. The main attention
is paid to the asymptotical case when the length of the time series tends to
infinity. Results concerning conditions for convergence, rate of convergence,
and the main terms of proximity are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1297</identifier>
 <datestamp>2010-01-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1297</id><created>2010-01-08</created><authors><author><keyname>Klouda</keyname><forenames>Karel</forenames></author></authors><title>BSA - exact algorithm computing LTS estimate</title><categories>stat.CO math.ST stat.TH</categories><comments>18 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main result of this paper is a new exact algorithm computing the estimate
given by the Least Trimmed Squares (LTS). The algorithm works under very weak
assumptions. To prove that, we study the respective objective function using
basic techniques of analysis and linear algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1323</identifier>
 <datestamp>2011-11-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1323</id><created>2010-01-08</created><updated>2011-11-28</updated><authors><author><keyname>Arias-Castro</keyname><forenames>Ery</forenames></author><author><keyname>Chen</keyname><forenames>Guangliang</forenames></author><author><keyname>Lerman</keyname><forenames>Gilad</forenames></author></authors><title>Spectral clustering based on local linear approximations</title><categories>stat.ML math.ST stat.TH</categories><msc-class>62H30, 62G20, 68T10</msc-class><journal-ref>Electronic Journal of Statistics, Vol. 5 (2011), pages 1537-1587</journal-ref><doi>10.1214/11-EJS651</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of clustering, we assume a generative model where each cluster
is the result of sampling points in the neighborhood of an embedded smooth
surface; the sample may be contaminated with outliers, which are modeled as
points sampled in space away from the clusters. We consider a prototype for a
higher-order spectral clustering method based on the residual from a local
linear approximation. We obtain theoretical guarantees for this algorithm and
show that, in terms of both separation and robustness to outliers, it
outperforms the standard spectral clustering algorithm (based on pairwise
distances) of Ng, Jordan and Weiss (NIPS '01). The optimal choice for some of
the tuning parameters depends on the dimension and thickness of the clusters.
We provide estimators that come close enough for our theoretical purposes. We
also discuss the cases of clusters of mixed dimensions and of clusters that are
generated from smoother surfaces. In our experiments, this algorithm is shown
to outperform pairwise spectral clustering on both simulated and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1345</identifier>
 <datestamp>2012-10-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1345</id><created>2010-01-08</created><updated>2012-10-11</updated><authors><author><keyname>Basrak</keyname><forenames>Bojan</forenames></author><author><keyname>Krizmani</keyname><forenames>Danijel</forenames></author><author><keyname>Segers</keyname><forenames>Johan</forenames></author></authors><title>A functional limit theorem for dependent sequences with infinite
  variance stable limits</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/11-AOP669 the Annals of
  Probability (http://www.imstat.org/aop/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOP-AOP669</report-no><journal-ref>Annals of Probability 2012, Vol. 40, No. 5, 2008-2033</journal-ref><doi>10.1214/11-AOP669</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under an appropriate regular variation condition, the affinely normalized
partial sums of a sequence of independent and identically distributed random
variables converges weakly to a non-Gaussian stable random variable. A
functional version of this is known to be true as well, the limit process being
a stable L\'{e}vy process. The main result in the paper is that for a
stationary, regularly varying sequence for which clusters of high-threshold
excesses can be broken down into asymptotically independent blocks, the
properly centered partial sum process still converges to a stable L\'{e}vy
process. Due to clustering, the L\'{e}vy triple of the limit process can be
different from the one in the independent case. The convergence takes place in
the space of c\`{a}dl\`{a}g functions endowed with Skorohod's $M_1$ topology,
the more usual $J_1$ topology being inappropriate as the partial sum processes
may exhibit rapid successions of jumps within temporal clusters of large
values, collapsing in the limit to a single jump. The result rests on a new
limit theorem for point processes which is of independent interest. The theory
is applied to moving average processes, squared GARCH(1,1) processes and
stochastic volatility models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1557</identifier>
 <datestamp>2010-10-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1557</id><created>2010-01-10</created><updated>2010-10-20</updated><authors><author><keyname>Liu</keyname><forenames>Han</forenames></author><author><keyname>Xu</keyname><forenames>Min</forenames></author><author><keyname>Gu</keyname><forenames>Haijie</forenames></author><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Lafferty</keyname><forenames>John</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Forest Density Estimation</title><categories>stat.ML</categories><comments>Extended version of earlier paper titled "Tree density estimation"</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study graph estimation and density estimation in high dimensions, using a
family of density estimators based on forest structured undirected graphical
models. For density estimation, we do not assume the true distribution
corresponds to a forest; rather, we form kernel density estimates of the
bivariate and univariate marginals, and apply Kruskal's algorithm to estimate
the optimal forest on held out data. We prove an oracle inequality on the
excess risk of the resulting estimator relative to the risk of the best forest.
For graph estimation, we consider the problem of estimating forests with
restricted tree sizes. We prove that finding a maximum weight spanning forest
with restricted tree size is NP-hard, and develop an approximation algorithm
for this problem. Viewing the tree size as a complexity parameter, we then
select a forest using data splitting, and prove bounds on excess risk and
structure selection consistency of the procedure. Experiments with simulated
data and microarray data indicate that the methods are a practical alternative
to Gaussian graphical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1609</identifier>
 <datestamp>2010-01-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1609</id><created>2010-01-11</created><authors><author><keyname>Cai</keyname><forenames>T. Tony</forenames></author><author><keyname>Jin</keyname><forenames>Jiashun</forenames></author></authors><title>Optimal rates of convergence for estimating the null density and
  proportion of nonnull effects in large-scale multiple testing</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS696 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS696</report-no><msc-class>62G05, 62G10 (Primary), 62G20 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 1, 100-145</journal-ref><doi>10.1214/09-AOS696</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important estimation problem that is closely related to large-scale
multiple testing is that of estimating the null density and the proportion of
nonnull effects. A few estimators have been introduced in the literature;
however, several important problems, including the evaluation of the minimax
rate of convergence and the construction of rate-optimal estimators, remain
open. In this paper, we consider optimal estimation of the null density and the
proportion of nonnull effects. Both minimax lower and upper bounds are derived.
The lower bound is established by a two-point testing argument, where at the
core is the novel construction of two least favorable marginal densities $f_1$
and $f_2$. The density $f_1$ is heavy tailed both in the spatial and frequency
domains and $f_2$ is a perturbation of $f_1$ such that the characteristic
functions associated with $f_1$ and $f_2$ match each other in low frequencies.
The minimax upper bound is obtained by constructing estimators which rely on
the empirical characteristic function and Fourier analysis. The estimator is
shown to be minimax rate optimal. Compared to existing methods in the
literature, the proposed procedure not only provides more precise estimates of
the null density and the proportion of the nonnull effects, but also yields
more accurate results when used inside some multiple testing procedures which
aim at controlling the False Discovery Rate (FDR). The procedure is easy to
implement and numerical results are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1615</identifier>
 <datestamp>2010-01-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1615</id><created>2010-01-11</created><authors><author><keyname>Rousseau</keyname><forenames>Judith</forenames></author></authors><title>Rates of convergence for the posterior distributions of mixtures of
  Betas and adaptive nonparametric estimation of the density</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS703 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS703</report-no><msc-class>62G07, 62G20 (Primary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 1, 146-180</journal-ref><doi>10.1214/09-AOS703</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the asymptotic properties of nonparametric
Bayesian mixtures of Betas for estimating a smooth density on $[0,1]$. We
consider a parametrization of Beta distributions in terms of mean and scale
parameters and construct a mixture of these Betas in the mean parameter, while
putting a prior on this scaling parameter. We prove that such Bayesian
nonparametric models have good frequentist asymptotic properties. We determine
the posterior rate of concentration around the true density and prove that it
is the minimax rate of concentration when the true density belongs to a
H\"{o}lder class with regularity $\beta$, for all positive $\beta$, leading to
a minimax adaptive estimating procedure of the density. We also believe that
the approximating results obtained on these mixtures of Beta densities can be
of interest in a frequentist framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1623</identifier>
 <datestamp>2010-01-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1623</id><created>2010-01-11</created><authors><author><keyname>Bolla</keyname><forenames>Marianna</forenames></author><author><keyname>Koi</keyname><forenames>Tamas</forenames></author><author><keyname>Kramli</keyname><forenames>Andras</forenames></author></authors><title>Testability of minimum balanced multiway cut densities</title><categories>math.PR math.ST stat.TH</categories><comments>24 pages, short version was a contributed paper of the conference:
  Fete of Combinatorics and Computer Science, Keszthely, Hungary, August 11-15,
  2008</comments><msc-class>05C35; 62H30; 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Testable weighted graph parameters and equivalent notions of testability are
investigated based on papers of Laszlo Lovasz and coauthors. We prove that
certain balanced minimum multiway cut densities are testable. Using this fact,
quadratic programming techniques are applied to approximate some of these
quantities. The problem is related to cluster analysis and statistical physics.
Convergence of special noisy graph sequences is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1653</identifier>
 <datestamp>2010-01-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1653</id><created>2010-01-11</created><authors><author><keyname>Shafer</keyname><forenames>Glenn</forenames></author></authors><title>A betting interpretation for probabilities and Dempster-Shafer degrees
  of belief</title><categories>math.ST cs.AI stat.TH</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are at least two ways to interpret numerical degrees of belief in terms
of betting: (1) you can offer to bet at the odds defined by the degrees of
belief, or (2) you can judge that a strategy for taking advantage of such
betting offers will not multiply the capital it risks by a large factor. Both
interpretations can be applied to ordinary additive probabilities and used to
justify updating by conditioning. Only the second can be applied to
Dempster-Shafer degrees of belief and used to justify Dempster's rule of
combination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1667</identifier>
 <datestamp>2010-01-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1667</id><created>2010-01-11</created><authors><author><keyname>Chen</keyname><forenames>Song Xi</forenames></author><author><keyname>Van Keilegom</keyname><forenames>Ingrid</forenames></author></authors><title>A goodness-of-fit test for parametric and semi-parametric models in
  multiresponse regression</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ208 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ208</report-no><journal-ref>Bernoulli 2009, Vol. 15, No. 4, 955-976</journal-ref><doi>10.3150/09-BEJ208</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an empirical likelihood test that is able to test the goodness of
fit of a class of parametric and semi-parametric multiresponse regression
models. The class includes as special cases fully parametric models;
semi-parametric models, like the multiindex and the partially linear models;
and models with shape constraints. Another feature of the test is that it
allows both the response variable and the covariate be multivariate, which
means that multiple regression curves can be tested simultaneously. The test
also allows the presence of infinite-dimensional nuisance functions in the
model to be tested. It is shown that the empirical likelihood test statistic is
asymptotically normally distributed under certain mild conditions and permits a
wild bootstrap calibration. Despite the large size of the class of models to be
considered, the empirical likelihood test enjoys good power properties against
departures from a hypothesized model within the class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1693</identifier>
 <datestamp>2010-01-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1693</id><created>2010-01-11</created><authors><author><keyname>Davies</keyname><forenames>E B</forenames></author></authors><title>Embeddable Markov Matrices</title><categories>math.PR math.SP math.ST stat.TH</categories><comments>15 pages</comments><msc-class>60J27; 60J22; 60J10; 65C40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an account of some results, both old and new, about any $n\times n$
Markov matrix that is embeddable in a one-parameter Markov semigroup. These
include the fact that its eigenvalues must lie in a certain region in the unit
ball. We prove that a well-known procedure for approximating a non-embeddable
Markov matrix by an embeddable one is optimal in a certain sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1782</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1782</id><created>2010-01-11</created><authors><author><keyname>Seregin</keyname><forenames>Arseni</forenames></author></authors><title>Uniqueness of the maximum likelihood estimator for k-monotone densities</title><categories>math.ST stat.TH</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove uniqueness of the maximum likelihood estimator for the class of
k-monotone densities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1810</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1810</id><created>2010-01-12</created><authors><author><keyname>Liao</keyname><forenames>Yuan</forenames></author><author><keyname>Jiang</keyname><forenames>Wenxin</forenames></author></authors><title>Bayesian analysis in moment inequality models</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS714 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS714</report-no><msc-class>62F15, 62N01 (Primary) 62F99 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 1, 275-316</journal-ref><doi>10.1214/09-AOS714</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a study of the large-sample behavior of the posterior
distribution of a structural parameter which is partially identified by moment
inequalities. The posterior density is derived based on the limited information
likelihood. The posterior distribution converges to zero exponentially fast on
any $\delta$-contraction outside the identified region. Inside, it is bounded
below by a positive constant if the identified region is assumed to have a
nonempty interior. Our simulation evidence indicates that the Bayesian approach
has advantages over frequentist methods, in the sense that, with a proper
choice of the prior, the posterior provides more information about the true
parameter inside the identified region. We also address the problem of moment
and model selection. Our optimality criterion is the maximum posterior
procedure and we show that, asymptotically, it selects the true moment/model
combination with the most moment inequalities and the simplest model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1817</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1817</id><created>2010-01-12</created><updated>2012-05-14</updated><authors><author><keyname>Dette</keyname><forenames>Holger</forenames></author><author><keyname>Leonenko</keyname><forenames>Nikolai</forenames></author><author><keyname>Pepelyshev</keyname><forenames>Andrey</forenames></author><author><keyname>Zhigljavsky</keyname><forenames>Anatoly</forenames></author></authors><title>Asymptotic optimal designs under long-range dependence error structure</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ185 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ185</report-no><journal-ref>Bernoulli 2009, Vol. 15, No. 4, 1036-1056</journal-ref><doi>10.3150/09-BEJ185</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the optimal design problem in regression models with long-range
dependence error structure. Asymptotic optimal designs are derived and it is
demonstrated that these designs depend only indirectly on the correlation
function. Several examples are investigated to illustrate the theory. Finally,
the optimal designs are compared with asymptotic optimal designs which were
derived by Bickel and Herzberg [Ann. Statist. 7 (1979) 77--95] for regression
models with short-range dependent error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1820</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1820</id><created>2010-01-12</created><authors><author><keyname>Belomestny</keyname><forenames>Denis</forenames></author></authors><title>Spectral estimation of the fractional order of a L\'{e}vy process</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS715 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS715</report-no><msc-class>62F10 (Primary) 62J12, 62F25, 62H12 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 1, 317-351</journal-ref><doi>10.1214/09-AOS715</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the fractional order of a L\'{e}vy
process from low frequency historical and options data. An estimation
methodology is developed which allows us to treat both estimation and
calibration problems in a unified way. The corresponding procedure consists of
two steps: the estimation of a conditional characteristic function and the
weighted least squares estimation of the fractional order in spectral domain.
While the second step is identical for both calibration and estimation, the
first one depends on the problem at hand. Minimax rates of convergence for the
fractional order estimate are derived, the asymptotic normality is proved and a
data-driven algorithm based on aggregation is proposed. The performance of the
estimator in both estimation and calibration setups is illustrated by a
simulation study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1821</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1821</id><created>2010-01-12</created><authors><author><keyname>Davis</keyname><forenames>Richard A.</forenames></author><author><keyname>Mikosch</keyname><forenames>Thomas</forenames></author></authors><title>The extremogram: A correlogram for extreme events</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ213 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ213</report-no><journal-ref>Bernoulli 2009, Vol. 15, No. 4, 977-1009</journal-ref><doi>10.3150/09-BEJ213</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a strictly stationary sequence of random vectors whose
finite-dimensional distributions are jointly regularly varying with some
positive index. This class of processes includes, among others, ARMA processes
with regularly varying noise, GARCH processes with normally or
Student-distributed noise and stochastic volatility models with regularly
varying multiplicative noise. We define an analog of the autocorrelation
function, the extremogram, which depends only on the extreme values in the
sequence. We also propose a natural estimator for the extremogram and study its
asymptotic properties under $\alpha$-mixing. We show asymptotic normality,
calculate the extremogram for various examples and consider spectral analysis
related to the extremogram.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1825</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1825</id><created>2010-01-12</created><authors><author><keyname>Beran</keyname><forenames>Jan</forenames></author><author><keyname>Schtzner</keyname><forenames>Martin</forenames></author></authors><title>On approximate pseudo-maximum likelihood estimation for LARCH-processes</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ189 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ189</report-no><journal-ref>Bernoulli 2009, Vol. 15, No. 4, 1057-1081</journal-ref><doi>10.3150/09-BEJ189</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear ARCH (LARCH) processes were introduced by Robinson [J. Econometrics 47
(1991) 67--84] to model long-range dependence in volatility and leverage. Basic
theoretical properties of LARCH processes have been investigated in the recent
literature. However, there is a lack of estimation methods and corresponding
asymptotic theory. In this paper, we consider estimation of the dependence
parameters for LARCH processes with non-summable hyperbolically decaying
coefficients. Asymptotic limit theorems are derived. A central limit theorem
with $\sqrt{n}$-rate of convergence holds for an approximate conditional
pseudo-maximum likelihood estimator. To obtain a computable version that
includes observed values only, a further approximation is required. The
computable estimator is again asymptotically normal, however with a rate of
convergence that is slower than $\sqrt{n}.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1828</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1828</id><created>2010-01-12</created><authors><author><keyname>Steland</keyname><forenames>Ansgar</forenames></author></authors><title>Random walks - a sequential approach</title><categories>math.ST math.PR stat.TH</categories><msc-class>60F17; 60G50; 62G08; 62P20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper sequential monitoring schemes to detect nonparametric drifts
are studied for the random walk case. The procedure is based on a kernel
smoother. As a by-product we obtain the asymptotics of the Nadaraya-Watson
estimator and its as- sociated sequential partial sum process under
non-standard sampling. The asymptotic behavior differs substantially from the
stationary situation, if there is a unit root (random walk component). To
obtain meaningful asymptotic results we consider local nonpara- metric
alternatives for the drift component. It turns out that the rate of convergence
at which the drift vanishes determines whether the asymptotic properties of the
monitoring procedure are determined by a deterministic or random function.
Further, we provide a theoretical result about the optimal kernel for a given
alternative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1829</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1829</id><created>2010-01-12</created><authors><author><keyname>Groeneboom</keyname><forenames>Piet</forenames></author><author><keyname>Jongbloed</keyname><forenames>Geurt</forenames></author><author><keyname>Witte</keyname><forenames>Birgit I.</forenames></author></authors><title>Maximum smoothed likelihood estimation and smoothed maximum likelihood
  estimation in the current status model</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS721 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS721</report-no><msc-class>62G05, 62N01 (Primary) 62G20 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 1, 352-387</journal-ref><doi>10.1214/09-AOS721</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the distribution function, the density
and the hazard rate of the (unobservable) event time in the current status
model. A well studied and natural nonparametric estimator for the distribution
function in this model is the nonparametric maximum likelihood estimator (MLE).
We study two alternative methods for the estimation of the distribution
function, assuming some smoothness of the event time distribution. The first
estimator is based on a maximum smoothed likelihood approach. The second method
is based on smoothing the (discrete) MLE of the distribution function. These
estimators can be used to estimate the density and hazard rate of the event
time distribution based on the plug-in principle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1830</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1830</id><created>2010-01-12</created><authors><author><keyname>Steland</keyname><forenames>Ansgar</forenames></author></authors><title>Optimal Sequential Kernel Detection for Dependent Processes</title><categories>math.ST math.OC stat.TH</categories><msc-class>62L15; 62G20</msc-class><journal-ref>Journal of Statistical Planning and Inference 2005, 132, 131-147</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications one is interested to detect certain (known) patterns in
the mean of a process with smallest delay. Using an asymptotic framework which
allows to capture that feature, we study a class of appropriate sequential
nonparametric kernel procedures under local nonparametric alternatives. We
prove a new theorem on the con- vergence of the normed delay of the associated
sequential detection procedure which holds for dependent time series under a
weak mixing condition. The result suggests a simple procedure to select a
kernel from a finite set of candidate kernels, and therefore may also be of
interest from a practical point of view. Further, we provide two new theorems
about the existence and an explicit representation of optimal kernels
minimizing the asymptotic normed delay. The results are illustrated by some
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1831</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1831</id><created>2010-01-12</created><authors><author><keyname>Steland</keyname><forenames>Ansgar</forenames></author></authors><title>Monitoring Procedures to Detect Unit Roots and Stationarity</title><categories>math.ST stat.AP stat.TH</categories><journal-ref>Econometric Theory 2007, 23 (6), 1108-1135</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When analysing time series an important issue is to decide whether the time
series is stationary or a random walk. Relaxing these notions, we consider the
problem to decide in favor of the I(0)- or I(1)-property. Fixed-sample
statistical tests for that problem are well studied in the literature. In this
paper we provide first results for the problem to monitor sequentially a time
series. Our stopping times are based on a sequential version of a
kernel-weighted variance-ratio statistic. The asymptotic distributions are
established for I(1) processes, a rich class of stationary processes, possibly
affected by local nonpara- metric alternatives, and the local-to-unity model.
Further, we consider the two interesting change-point models where the time
series changes its behaviour after a certain fraction of the observations and
derive the associated limiting laws. Our Monte-Carlo studies show that the
proposed detection procedures have high power when interpreted as a hypothesis
test, and that the decision can often be made very early.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1833</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1833</id><created>2010-01-12</created><authors><author><keyname>Steland</keyname><forenames>Ansgar</forenames></author></authors><title>Weighted Dickey-Fuller Processes for Detecting Stationarity</title><categories>math.PR math.ST stat.ME stat.TH</categories><msc-class>60F17; 62L10; 62E20; 62P20</msc-class><journal-ref>Journal of Statistical Planning and Inference 2007, 137 (12),
  4011-4030</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aiming at monitoring a time series to detect stationarity as soon as
possible, we introduce monitoring procedures based on kernel-weighted
sequential Dickey-Fuller (DF) processes, and related stopping times, which may
be called weighted Dickey-Fuller control charts. Under rather weak assumptions,
(functional) central limit theorems are established under the unit root null
hypothesis and local-to-unity alternatives. For gen- eral dependent and
heterogeneous innovation sequences the limit processes depend on a nuisance
parameter. In this case of practical interest, one can use estimated control
limits obtained from the estimated asymptotic law. Another easy-to-use approach
is to transform the DF processes to obtain limit laws which are invariant with
respect to the nuisance pa- rameter. We provide asymptotic theory for both
approaches and compare their statistical behavior in finite samples by
simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1841</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1841</id><created>2010-01-12</created><authors><author><keyname>Steland</keyname><forenames>Ansgar</forenames></author><author><keyname>Rafalowicz</keyname><forenames>Ewaryst</forenames></author></authors><title>A Binary Control Chart to Detect Small Jumps</title><categories>stat.ME stat.AP stat.ML</categories><journal-ref>Statistics 2009, 43 (3), 295-311</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classic N p chart gives a signal if the number of successes in a sequence
of inde- pendent binary variables exceeds a control limit. Motivated by
engineering applications in industrial image processing and, to some extent,
financial statistics, we study a simple modification of this chart, which uses
only the most recent observations. Our aim is to construct a control chart for
detecting a shift of an unknown size, allowing for an unknown distribution of
the error terms. Simulation studies indicate that the proposed chart is su-
perior in terms of out-of-control average run length, when one is interest in
the detection of very small shifts. We provide a (functional) central limit
theorem under a change-point model with local alternatives which explains that
unexpected and interesting behavior. Since real observations are often not
independent, the question arises whether these re- sults still hold true for
the dependent case. Indeed, our asymptotic results work under the fairly
general condition that the observations form a martingale difference array.
This enlarges the applicability of our results considerably, firstly, to a
large class time series models, and, secondly, to locally dependent image data,
as we demonstrate by an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1845</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1845</id><created>2010-01-12</created><authors><author><keyname>Steland</keyname><forenames>Ansgar</forenames></author></authors><title>Sequentially Updated Residuals and Detection of Stationary Errors in
  Polynomial Regression Models</title><categories>math.PR math.ST stat.ME stat.TH</categories><msc-class>60G40; 60G50; 62L12; 62M10; 62E20.</msc-class><journal-ref>Sequential Analysis 2008, 27 (3), 304-329</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question whether a time series behaves as a random walk or as a station-
ary process is an important and delicate problem, particularly arising in
financial statistics, econometrics, and engineering. This paper studies the
problem to detect sequentially that the error terms in a polynomial regression
model no longer behave as a random walk but as a stationary process. We provide
the asymptotic distribution theory for a monitoring procedure given by a
control chart, i.e., a stopping time, which is related to a well known unit
root test statistic calculated from sequentially updated residuals. We provide
a functional central limit theorem for the corresponding stochastic process
which implies a central limit theorem for the control chart. The finite sample
properties are investigated by a simulation study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1853</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1853</id><created>2010-01-12</created><updated>2012-09-25</updated><authors><author><keyname>Ingster</keyname><forenames>Yuri I.</forenames></author><author><keyname>Sapatinas</keyname><forenames>Theofanis</forenames></author><author><keyname>Suslina</keyname><forenames>Irina A.</forenames></author></authors><title>Minimax signal detection in ill-posed inverse problems</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOS1011 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><report-no>IMS-AOS-AOS1011</report-no><msc-class>62G10, 62G20 (Primary) 62C20 (Secondary)</msc-class><journal-ref>Annals of Statistics 2012, Vol. 40, No. 3, 1524-1549</journal-ref><doi>10.1214/12-AOS1011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ill-posed inverse problems arise in various scientific fields. We consider
the signal detection problem for mildly, severely and extremely ill-posed
inverse problems with $l^q$-ellipsoids (bodies), $q\in(0,2]$, for Sobolev,
analytic and generalized analytic classes of functions under the Gaussian white
noise model. We study both rate and sharp asymptotics for the error
probabilities in the minimax setup. By construction, the derived tests are,
often, nonadaptive. Minimax rate-optimal adaptive tests of rather simple
structure are also constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1858</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1858</id><created>2010-01-12</created><authors><author><keyname>Lahiri</keyname><forenames>S. N.</forenames></author></authors><title>Edgeworth expansions for studentized statistics under weak dependence</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS722 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS722</report-no><msc-class>60F05 (Primary) 62E20, 62M10 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 1, 388-434</journal-ref><doi>10.1214/09-AOS722</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive valid Edgeworth expansions for studentized versions
of a large class of statistics when the data are generated by a strongly mixing
process. Under dependence, the asymptotic variance of such a statistic is given
by an infinite series of lag-covariances, and therefore, studentizing factors
(i.e., estimators of the asymptotic standard error) typically involve an
increasing number, say, $\ell$ of lag-covariance estimators, which are
themselves quadratic functions of the observations. The unboundedness of the
dimension $\ell$ of these quadratic functions makes the derivation and the form
of the expansions nonstandard. It is shown that in contrast to the case of the
studentized means under independence, the derived Edgeworth expansion is a
superposition of three distinct series, respectively, given by one in powers of
$n^{-1/2}$, one in powers of $[n/\ell]^{-1/2}$ (resulting from the standard
error of the studentizing factor) and one in powers of the bias of the
studentizing factor, where $n$ denotes the sample size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1886</identifier>
 <datestamp>2010-01-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1886</id><created>2010-01-12</created><authors><author><keyname>Evans</keyname><forenames>Michael</forenames></author><author><keyname>Jang</keyname><forenames>Gun Ho</forenames></author></authors><title>Invariant $P$-values for model checking</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS727 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS727</report-no><msc-class>62F99 (Primary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 1, 512-525</journal-ref><doi>10.1214/09-AOS727</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $P$-values have been the focus of considerable criticism based on various
considerations. Still, the $P$-value represents one of the most commonly used
statistical tools. When assessing the suitability of a single hypothesized
distribution, it is not clear that there is a better choice for a measure of
surprise. This paper is concerned with the definition of appropriate
model-based $P$-values for model checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.1919</identifier>
 <datestamp>2011-01-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.1919</id><created>2010-01-12</created><updated>2011-01-21</updated><authors><author><keyname>Mougeot</keyname><forenames>Mathilde</forenames><affiliation>PMA</affiliation></author><author><keyname>Picard</keyname><forenames>Dominique</forenames><affiliation>PMA</affiliation></author><author><keyname>Tribouley</keyname><forenames>Karine</forenames><affiliation>PMA, MODAL'X</affiliation></author></authors><title>Learning Out of Leaders</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00445690</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the estimation problem in a regression-type model. To
be able to deal with potential high dimensions, we provide a procedure called
LOL, for Learning Out of Leaders with no optimization step. LOL is an
auto-driven algorithm with two thresholding steps. A first adaptive
thresholding helps to select leaders among the initial regressors in order to
obtain a first reduction of dimensionality. Then a second thresholding is
performed on the linear regression upon the leaders. The consistency of the
procedure is investigated. Exponential bounds are obtained, leading to minimax
and adaptive results for a wide class of sparse parameters, with (quasi) no
restriction on the number p of possible regressors. An extensive computational
experiment is conducted to emphasize the practical good performances of LOL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2055</identifier>
 <datestamp>2010-01-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2055</id><created>2010-01-12</created><authors><author><keyname>Fan</keyname><forenames>Y</forenames></author><author><keyname>Sisson</keyname><forenames>S A</forenames></author></authors><title>Reversible jump Markov chain Monte Carlo</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To appear to MCMC handbook, S. P. Brooks, A. Gelman, G. Jones and X.-L. Meng
(eds), Chapman &amp; Hall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2058</identifier>
 <datestamp>2010-01-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2058</id><created>2010-01-12</created><authors><author><keyname>Sisson</keyname><forenames>S A</forenames></author><author><keyname>Fan</keyname><forenames>Y</forenames></author></authors><title>Likelihood-free Markov chain Monte Carlo</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To appear to MCMC handbook, S. P. Brooks, A. Gelman, G. Jones and X.-L. Meng
(eds), Chapman &amp; Hall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2089</identifier>
 <datestamp>2010-01-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2089</id><created>2010-01-13</created><authors><author><keyname>Klemel</keyname><forenames>Jussi</forenames></author><author><keyname>Mammen</keyname><forenames>Enno</forenames></author></authors><title>Empirical risk minimization in inverse problems</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS726 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS726</report-no><msc-class>62G07 (Primary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 1, 482-511</journal-ref><doi>10.1214/09-AOS726</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study estimation of a multivariate function $f:\mathbf{R}^d\to\mathbf{R}$
when the observations are available from the function $Af$, where $A$ is a
known linear operator. Both the Gaussian white noise model and density
estimation are studied. We define an $L_2$-empirical risk functional which is
used to define a $\delta$-net minimizer and a dense empirical risk minimizer.
Upper bounds for the mean integrated squared error of the estimators are given.
The upper bounds show how the difficulty of the estimation depends on the
operator through the norm of the adjoint of the inverse of the operator and on
the underlying function class through the entropy of the class. Corresponding
lower bounds are also derived. As examples, we consider convolution operators
and the Radon transform. In these examples, the estimators achieve the optimal
rates of convergence. Furthermore, a new type of oracle inequality is given for
inverse problems in additive models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2094</identifier>
 <datestamp>2010-01-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2094</id><created>2010-01-13</created><authors><author><keyname>Mendelson</keyname><forenames>Shahar</forenames></author><author><keyname>Neeman</keyname><forenames>Joseph</forenames></author></authors><title>Regularization in kernel learning</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS728 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS728</report-no><msc-class>68Q32 (Primary) 60G99 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 1, 526-565</journal-ref><doi>10.1214/09-AOS728</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under mild assumptions on the kernel, we obtain the best known error rates in
a regularized learning scenario taking place in the corresponding reproducing
kernel Hilbert space (RKHS). The main novelty in the analysis is a proof that
one can use a regularization term that grows significantly slower than the
standard quadratic growth in the RKHS norm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2102</identifier>
 <datestamp>2010-01-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2102</id><created>2010-01-13</created><authors><author><keyname>Jacob</keyname><forenames>Christine</forenames></author></authors><title>Conditional least squares estimation in nonstationary nonlinear
  stochastic regression models</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS733 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS733</report-no><msc-class>62M10, 62J02, 62F12, 62M05, 62M09, 62P05, 62P10 (Primary), 60G46,
  60F15 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 1, 566-597</journal-ref><doi>10.1214/09-AOS733</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\{Z_n\}$ be a real nonstationary stochastic process such that
$E(Z_n|{\mathcaligr F}_{n-1})\stackrel{\mathrm{a.s.}}{&lt;}\infty$ and
$E(Z^2_n|{\mathcaligr F}_{n-1})\stackrel{\mathrm{a.s.}}{&lt;}\infty$, where
$\{{\mathcaligr F}_n\}$ is an increasing sequence of $\sigma$-algebras.
Assuming that $E(Z_n|{\mathcaligr
F}_{n-1})=g_n(\theta_0,\nu_0)=g^{(1)}_n(\theta_0)+g^{(2)}_n(\theta_0,\nu_0)$,
$\theta_0\in{\mathbb{R}}^p$, $p&lt;\infty$, $\nu_0\in{\mathbb{R}}^q$ and
$q\leq\infty$, we study the asymptotic properties of
$\hat{\theta}_n:=\arg\min_{\theta}\sum_{k=1}^n(Z_k-g_k({\theta,\hat{\nu}}))^2\lambda_k^{-1}$,
where $\lambda_k$ is ${\mathcaligr F}_{k-1}$-measurable,
$\hat{\nu}=\{\hat{\nu}_k\}$ is a sequence of estimations of $\nu_0$,
$g_n(\theta,\hat{\nu})$ is Lipschitz in $\theta$ and
$g^{(2)}_n(\theta_0,\hat{\nu})-g^{(2)}_n(\theta,\hat{\nu})$ is asymptotically
negligible relative to $g^{(1)}_n(\theta_0)-g^{(1)}_n(\theta)$. We first
generalize to this nonlinear stochastic model the necessary and sufficient
condition obtained for the strong consistency of $\{\hat{\theta}_n\}$ in the
linear model. For that, we prove a strong law of large numbers for a class of
submartingales. Again using this strong law, we derive the general conditions
leading to the asymptotic distribution of $\hat{\theta}_n$. We illustrate the
theoretical results with examples of branching processes, and extension to
quasi-likelihood estimators is also considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2122</identifier>
 <datestamp>2010-01-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2122</id><created>2010-01-13</created><authors><author><keyname>Paparoditis</keyname><forenames>Efstathios</forenames></author></authors><title>Testing temporal constancy of the spectral structure of a time series</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/08-BEJ179 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ179</report-no><journal-ref>Bernoulli 2009, Vol. 15, No. 4, 1190-1221</journal-ref><doi>10.3150/08-BEJ179</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical inference for stochastic processes with time-varying spectral
characteristics has received considerable attention in recent decades. We
develop a nonparametric test for stationarity against the alternative of a
smoothly time-varying spectral structure. The test is based on a comparison
between the sample spectral density calculated locally on a moving window of
data and a global spectral density estimator based on the whole stretch of
observations. Asymptotic properties of the nonparametric estimators involved
and of the test statistic under the null hypothesis of stationarity are
derived. Power properties under the alternative of a time-varying spectral
structure are discussed and the behavior of the test for fixed alternatives
belonging to the locally stationary processes class is investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2125</identifier>
 <datestamp>2010-01-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2125</id><created>2010-01-13</created><authors><author><keyname>Ambrosio</keyname><forenames>Luigi</forenames></author><author><keyname>Capasso</keyname><forenames>Vincenzo</forenames></author><author><keyname>Villa</keyname><forenames>Elena</forenames></author></authors><title>On the approximation of mean densities of random closed sets</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ186 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ186</report-no><journal-ref>Bernoulli 2009, Vol. 15, No. 4, 1222-1242</journal-ref><doi>10.3150/09-BEJ186</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real phenomena may be modelled as random closed sets in $\mathbb{R}^d$,
of different Hausdorff dimensions. In many real applications, such as fiber
processes and $n$-facets of random tessellations of dimension $n\leq d$ in
spaces of dimension $d\geq1$, several problems are related to the estimation of
such mean densities. In order to confront such problems in the general setting
of spatially inhomogeneous processes, we suggest and analyze an approximation
of mean densities for sufficiently regular random closed sets. We show how some
known results in literature follow as particular cases. A series of examples
throughout the paper are provided to illustrate various relevant situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2136</identifier>
 <datestamp>2010-06-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2136</id><created>2010-01-13</created><updated>2010-06-20</updated><authors><author><keyname>Arima</keyname><forenames>Serena</forenames></author><author><keyname>Tardella</keyname><forenames>Luca</forenames></author></authors><title>An alternative marginal likelihood estimator for phylogenetic models</title><categories>stat.CO q-bio.QM stat.AP stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian phylogenetic methods are generating noticeable enthusiasm in the
field of molecular systematics. Many phylogenetic models are often at stake and
different approaches are used to compare them within a Bayesian framework. The
Bayes factor, defined as the ratio of the marginal likelihoods of two competing
models, plays a key role in Bayesian model selection. We focus on an
alternative estimator of the marginal likelihood whose computation is still a
challenging problem. Several computational solutions have been proposed none of
which can be considered outperforming the others simultaneously in terms of
simplicity of implementation, computational burden and precision of the
estimates. Practitioners and researchers, often led by available software, have
privileged so far the simplicity of the harmonic mean estimator (HM) and the
arithmetic mean estimator (AM). However it is known that the resulting
estimates of the Bayesian evidence in favor of one model are biased and often
inaccurate up to having an infinite variance so that the reliability of the
corresponding conclusions is doubtful. Our new implementation of the
generalized harmonic mean (GHM) idea recycles MCMC simulations from the
posterior, shares the computational simplicity of the original HM estimator,
but, unlike it, overcomes the infinite variance issue. The alternative
estimator is applied to simulated phylogenetic data and produces fully
satisfactory results outperforming those simple estimators currently provided
by most of the publicly available software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2138</identifier>
 <datestamp>2010-01-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2138</id><created>2010-01-13</created><authors><author><keyname>Olofsson</keyname><forenames>Peter</forenames></author></authors><title>Size-biased branching population measures and the multi-type $x\log x$
  condition</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ211 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ211</report-no><journal-ref>Bernoulli 2009, Vol. 15, No. 4, 1287-1304</journal-ref><doi>10.3150/09-BEJ211</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the $x\log x$ condition for a general (Crump--Mode--Jagers)
multi-type branching process with a general type space by constructing a
size-biased population measure that relates to the ordinary population measure
via an intrinsic martingale $W_t$. Sufficiency of the $x\log x$ condition for a
non-degenerate limit of $W_t$ is proved and conditions for necessity are
investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2144</identifier>
 <datestamp>2010-01-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2144</id><created>2010-01-13</created><authors><author><keyname>Xia</keyname><forenames>Aihua</forenames></author><author><keyname>Zhang</keyname><forenames>Mei</forenames></author></authors><title>On approximation of Markov binomial distributions</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ194 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ194</report-no><journal-ref>Bernoulli 2009, Vol. 15, No. 4, 1335-1350</journal-ref><doi>10.3150/09-BEJ194</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a Markov chain $\mathbf{X}=\{X_i,i=1,2,...,n\}$ with the state space
$\{0,1\}$, the random variable $S:=\sum_{i=1}^nX_i$ is said to follow a Markov
binomial distribution. The exact distribution of $S$, denoted $\mathcal{L}S$,
is very computationally intensive for large $n$ (see Gabriel [Biometrika 46
(1959) 454--460] and Bhat and Lal [Adv. in Appl. Probab. 20 (1988) 677--680])
and this paper concerns suitable approximate distributions for $\mathcal{L}S$
when $\mathbf{X}$ is stationary. We conclude that the negative binomial and
binomial distributions are appropriate approximations for $\mathcal{L}S$ when
$\operatorname {Var}S$ is greater than and less than $\mathbb{E}S$,
respectively. Also, due to the unique structure of the distribution, we are
able to derive explicit error estimates for these approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2152</identifier>
 <datestamp>2010-01-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2152</id><created>2010-01-13</created><authors><author><keyname>Berti</keyname><forenames>Patrizia</forenames></author><author><keyname>Crimaldi</keyname><forenames>Irene</forenames></author><author><keyname>Pratelli</keyname><forenames>Luca</forenames></author><author><keyname>Rigo</keyname><forenames>Pietro</forenames></author></authors><title>Rate of convergence of predictive distributions for dependent data</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ191 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ191</report-no><journal-ref>Bernoulli 2009, Vol. 15, No. 4, 1351-1367</journal-ref><doi>10.3150/09-BEJ191</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with empirical processes of the type
\[C_n(B)=\sqrt{n}\{\mu_n(B)-P(X_{n+1}\in B\mid X_1,...,X_n)\},\] where $(X_n)$
is a sequence of random variables and $\mu_n=(1/n)\sum_{i=1}^n\delta_{X_i}$ the
empirical measure. Conditions for $\sup_B|C_n(B)|$ to converge stably (in
particular, in distribution) are given, where $B$ ranges over a suitable class
of measurable sets. These conditions apply when $(X_n)$ is exchangeable or,
more generally, conditionally identically distributed (in the sense of Berti et
al. [Ann. Probab. 32 (2004) 2029--2052]). By such conditions, in some relevant
situations, one obtains that $\sup_B|C_n(B)|\stackrel{P}{\to}0$ or even that
$\sqrt{n}\sup_B|C_n(B)|$ converges a.s. Results of this type are useful in
Bayesian statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2185</identifier>
 <datestamp>2010-01-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2185</id><created>2010-01-13</created><authors><author><keyname>Simas</keyname><forenames>Alexandre B.</forenames></author><author><keyname>Rocha</keyname><forenames>Andra V.</forenames></author><author><keyname>Barreto-Souza</keyname><forenames>Wagner</forenames></author></authors><title>Improved estimators for dispersion models with dispersion covariates</title><categories>stat.ME math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discuss improved estimators for the regression and the
dispersion parameters in an extended class of dispersion models (J{\o}rgensen,
1996). This class extends the regular dispersion models by letting the
dispersion parameter vary throughout the observations, and contains the
dispersion models as particular case. General formulae for the second-order
bias are obtained explicitly in dispersion models with dispersion covariates,
which generalize previous results by Botter and Cordeiro (1998), Cordeiro and
McCullagh (1991), Cordeiro and Vasconcellos (1999), and Paula (1992). The
practical use of the formulae is that we can derive closed-form expressions for
the second-order biases of the maximum likelihood estimators of the regression
and dispersion parameters when the information matrix has a closed-form.
Various expressions for the second-order biases are given for special models.
The formulae have advantages for numerical purposes because they require only a
supplementary weighted linear regression. We also compare these bias-corrected
estimators with two different estimators which are also bias-free to the
second-order that are based on bootstrap methods. These estimators are compared
by simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2187</identifier>
 <datestamp>2010-03-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2187</id><created>2010-01-13</created><authors><author><keyname>Simas</keyname><forenames>Alexandre B.</forenames></author><author><keyname>Cordeiro</keyname><forenames>Gauss M.</forenames></author><author><keyname>Rocha</keyname><forenames>Andra V.</forenames></author></authors><title>Skewness of maximum likelihood estimators in dispersion models</title><categories>stat.ME math.ST stat.TH</categories><journal-ref>Journal of Statistical Planning and Inference, 140, 2111-2121,
  2010.</journal-ref><doi>10.1016/j.jspi.2010.02.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the dispersion models with a regression structure to extend the
generalized linear models, the exponential family nonlinear models (Cordeiro
and Paula, 1989) and the proper dispersion models (J{\o}rgensen, 1997a). We
provide a matrix expression for the skewness of the maximum likelihood
estimators of the regression parameters in dispersion models. The formula is
suitable for computer implementation and can be applied for several important
submodels discussed in the literature. Expressions for the skewness of the
maximum likelihood estimators of the precision and dispersion parameters are
also derived. In particular, our results extend previous formulas obtained by
Cordeiro and Cordeiro (2001) and Cavalcanti et al. (2009). A simulation study
is perfomed to show the practice importance of our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2477</identifier>
 <datestamp>2010-01-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2477</id><created>2010-01-14</created><authors><author><keyname>Bertin</keyname><forenames>Karine</forenames></author><author><keyname>Klutchnikoff</keyname><forenames>Nicolas</forenames></author></authors><title>Minimax properties of beta kernel density estimators</title><categories>math.ST stat.TH</categories><comments>19 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we are interested in the study of beta kernel estimators from
an asymptotic minimax point of view. It is well known that beta kernel
estimators are, on the contrary of classical kernel estimators, "free of
boundary effect" and thus are very useful in practice. The goal of this paper
is to prove that there is a price to pay: for very regular functions or for
certain losses, these estimators are not minimax. Nevertheless they are minimax
for classical regularities such as regularity of order two or less than two,
supposed commonly in the practice and for some classical losses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2615</identifier>
 <datestamp>2010-01-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2615</id><created>2010-01-15</created><authors><author><keyname>Tomioka</keyname><forenames>Ryota</forenames></author><author><keyname>Suzuki</keyname><forenames>Taiji</forenames></author></authors><title>Sparsity-accuracy trade-off in MKL</title><categories>stat.ML stat.AP stat.ME</categories><comments>8pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We empirically investigate the best trade-off between sparse and
uniformly-weighted multiple kernel learning (MKL) using the elastic-net
regularization on real and simulated datasets. We find that the best trade-off
parameter depends not only on the sparsity of the true kernel-weight spectrum
but also on the linear dependence among kernels and the number of samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2685</identifier>
 <datestamp>2010-01-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2685</id><created>2010-01-15</created><authors><author><keyname>Greenland</keyname><forenames>Sander</forenames></author></authors><title>Relaxation Penalties and Priors for Plausible Modeling of Nonidentified
  Bias Sources</title><categories>stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/09-STS291 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS291</report-no><journal-ref>Statistical Science 2009, Vol. 24, No. 2, 195-210</journal-ref><doi>10.1214/09-STS291</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In designed experiments and surveys, known laws or design feat ures provide
checks on the most relevant aspects of a model and identify the target
parameters. In contrast, in most observational studies in the health and social
sciences, the primary study data do not identify and may not even bound target
parameters. Discrepancies between target and analogous identified parameters
(biases) are then of paramount concern, which forces a major shift in modeling
strategies. Conventional approaches are based on conditional testing of
equality constraints, which correspond to implausible point-mass priors. When
these constraints are not identified by available data, however, no such
testing is possible. In response, implausible constraints can be relaxed into
penalty functions derived from plausible prior distributions. The resulting
models can be fit within familiar full or partial likelihood frameworks. The
absence of identification renders all analyses part of a sensitivity analysis.
In this view, results from single models are merely examples of what might be
plausibly inferred. Nonetheless, just one plausible inference may suffice to
demonstrate inherent limitations of the data. Points are illustrated with
misclassified data from a study of sudden infant death syndrome. Extensions to
confounding, selection bias and more complex data structures are outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2697</identifier>
 <datestamp>2010-01-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2697</id><created>2010-01-15</created><authors><author><keyname>Kurland</keyname><forenames>Brenda F.</forenames></author><author><keyname>Johnson</keyname><forenames>Laura L.</forenames></author><author><keyname>Egleston</keyname><forenames>Brian L.</forenames></author><author><keyname>Diehr</keyname><forenames>Paula H.</forenames></author></authors><title>Longitudinal Data with Follow-up Truncated by Death: Match the Analysis
  Method to Research Aims</title><categories>stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/09-STS293 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS293</report-no><journal-ref>Statistical Science 2009, Vol. 24, No. 2, 211-222</journal-ref><doi>10.1214/09-STS293</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diverse analysis approaches have been proposed to distinguish data missing
due to death from nonresponse, and to summarize trajectories of longitudinal
data truncated by death. We demonstrate how these analysis approaches arise
from factorizations of the distribution of longitudinal data and survival
information. Models are illustrated using cognitive functioning data for older
adults. For unconditional models, deaths do not occur, deaths are independent
of the longitudinal response, or the unconditional longitudinal response is
averaged over the survival distribution. Unconditional models, such as random
effects models fit to unbalanced data, may implicitly impute data beyond the
time of death. Fully conditional models stratify the longitudinal response
trajectory by time of death. Fully conditional models are effective for
describing individual trajectories, in terms of either aging (age, or years
from baseline) or dying (years from death). Causal models (principal
stratification) as currently applied are fully conditional models, since group
differences at one timepoint are described for a cohort that will survive past
a later timepoint. Partly conditional models summarize the longitudinal
response in the dynamic cohort of survivors. Partly conditional models are
serial cross-sectional snapshots of the response, reflecting the average
response in survivors at a given timepoint rather than individual trajectories.
Joint models of survival and longitudinal response describe the evolving health
status of the entire cohort. Researchers using longitudinal data should
consider which method of accommodating deaths is consistent with research aims,
and use analysis methods accordingly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2753</identifier>
 <datestamp>2010-01-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2753</id><created>2010-01-15</created><updated>2010-01-22</updated><authors><author><keyname>Koutsourelakis</keyname><forenames>P. S.</forenames></author><author><keyname>Bilionis</keyname><forenames>Elias</forenames></author></authors><title>Scalable Bayesian reduced-order models for high-dimensional multiscale
  dynamical systems</title><categories>stat.ML math-ph math.MP math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While existing mathematical descriptions can accurately account for phenomena
at microscopic scales (e.g. molecular dynamics), these are often
high-dimensional, stochastic and their applicability over macroscopic time
scales of physical interest is computationally infeasible or impractical. In
complex systems, with limited physical insight on the coherent behavior of
their constituents, the only available information is data obtained from
simulations of the trajectories of huge numbers of degrees of freedom over
microscopic time scales. This paper discusses a Bayesian approach to deriving
probabilistic coarse-grained models that simultaneously address the problems of
identifying appropriate reduced coordinates and the effective dynamics in this
lower-dimensional representation. At the core of the models proposed lie
simple, low-dimensional dynamical systems which serve as the building blocks of
the global model. These approximate the latent, generating sources and
parameterize the reduced-order dynamics. We discuss parallelizable, online
inference and learning algorithms that employ Sequential Monte Carlo samplers
and scale linearly with the dimensionality of the observed dynamics. We propose
a Bayesian adaptive time-integration scheme that utilizes probabilistic
predictive estimates and enables rigorous concurrent s imulation over
macroscopic time scales. The data-driven perspective advocated assimilates
computational and experimental data and thus can materialize data-model fusion.
It can deal with applications that lack a mathematical description and where
only observational data is available. Furthermore, it makes non-intrusive use
of existing computational models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2797</identifier>
 <datestamp>2010-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2797</id><created>2010-01-15</created><authors><author><keyname>Latuszynski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Rosenthal</keyname><forenames>Jeffrey S.</forenames></author></authors><title>Adaptive Gibbs samplers</title><categories>stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider various versions of adaptive Gibbs and Metropolis within-Gibbs
samplers, which update their selection probabilities (and perhaps also their
proposal distributions) on the fly during a run, by learning as they go in an
attempt to optimise the algorithm. We present a cautionary example of how even
a simple-seeming adaptive Gibbs sampler may fail to converge. We then present
various positive results guaranteeing convergence of adaptive Gibbs samplers
under certain conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2813</identifier>
 <datestamp>2010-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2813</id><created>2010-01-17</created><authors><author><keyname>Di Franco</keyname><forenames>Anthony</forenames></author></authors><title>A Monte Carlo Algorithm for Universally Optimal Bayesian Sequence
  Prediction and Planning</title><categories>nlin.AO cond-mat.dis-nn cs.AI cs.LG stat.ML</categories><comments>Submitted to MDPI Algorithms Special Issue "Algorithmic Complexity in
  Physics &amp; Embedded Artificial Intelligences"</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this work is to address the question of whether we can in
principle design rational decision-making agents or artificial intelligences
embedded in computable physics such that their decisions are optimal in
reasonable mathematical senses. Recent developments in rare event probability
estimation, recursive bayesian inference, neural networks, and probabilistic
planning are sufficient to explicitly approximate reinforcement learners of the
AIXI style with non-trivial model classes (here, the class of resource-bounded
Turing machines). Consideration of the effects of resource limitations in a
concrete implementation leads to insights about possible architectures for
learning systems using optimal decision makers as components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2897</identifier>
 <datestamp>2010-04-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2897</id><created>2010-01-17</created><authors><author><keyname>Adell</keyname><forenames>Jose A.</forenames></author><author><keyname>Lekuona</keyname><forenames>Alberto</forenames></author><author><keyname>Yu</keyname><forenames>Yaming</forenames></author></authors><title>Sharp Bounds on the Entropy of the Poisson Law and Related Quantities</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>To appear, IEEE Trans. Inform. Theory</comments><journal-ref>IEEE Trans. Inform. Theory 56 (2010) 2299 - 2306</journal-ref><doi>10.1109/TIT.2010.2044057</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the difficulties in calculating the capacity of certain Poisson
channels is that H(lambda), the entropy of the Poisson distribution with mean
lambda, is not available in a simple form. In this work we derive upper and
lower bounds for H(lambda) that are asymptotically tight and easy to compute.
The derivation of such bounds involves only simple probabilistic and analytic
tools. This complements the asymptotic expansions of Knessl (1998), Jacquet and
Szpankowski (1999), and Flajolet (1999). The same method yields tight bounds on
the relative entropy D(n, p) between a binomial and a Poisson, thus refining
the work of Harremoes and Ruzankin (2004). Bounds on the entropy of the
binomial also follow easily.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2901</identifier>
 <datestamp>2010-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2901</id><created>2010-01-18</created><authors><author><keyname>Pinelis</keyname><forenames>Iosif</forenames></author></authors><title>Exact lower bounds on the exponential moments of Winsorized and
  truncated random variables</title><categories>math.PR math.ST stat.TH</categories><msc-class>Primary 60E15; secondary 60E10, 60F10, 60F05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exact lower bounds on the exponential moments of min(y,X) and XI{X&lt;y} are
provided given the first two moments of a random variable X. These bounds are
useful in work on large deviations probabilities and nonuniform Berry-Esseen
bounds, when the Cram\'er tilt transform may be employed. Asymptotic properties
of these lower bounds are presented. Comparative advantages of the
Winsorization min(y,X) over the truncation XI{X&lt;y} are demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2906</identifier>
 <datestamp>2010-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2906</id><created>2010-01-17</created><authors><author><keyname>Robert</keyname><forenames>Christian P.</forenames></author><author><keyname>Casella</keyname><forenames>George</forenames></author></authors><title>Introducing Monte Carlo Methods with R Solutions to Odd-Numbered
  Exercises</title><categories>stat.ME stat.CO</categories><comments>87 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the solution manual to the odd-numbered exercises in our book
"Introducing Monte Carlo Methods with R", published by Springer Verlag on
December 10, 2009, and made freely available to everyone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2916</identifier>
 <datestamp>2010-11-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2916</id><created>2010-01-17</created><updated>2010-09-15</updated><authors><author><keyname>Kulik</keyname><forenames>Rafal</forenames><affiliation>MODAL'X</affiliation></author><author><keyname>Soulier</keyname><forenames>Philippe</forenames><affiliation>MODAL'X</affiliation></author></authors><title>The tail empirical process for some long memory sequences</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><journal-ref>Stochastic Processes and their Applications 121, 1 (2011) 109-134</journal-ref><doi>10.1016/j.spa.2010.09.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes limiting behaviour of tail empirical process associated
with long memory stochastic volatility models. We show that such process has
dichotomous behaviour, according to an interplay between a Hurst parameter and
a tail index. In particular, the limit may be non-Gaussian and/or degenerate,
indicating an influence of long memory. On the other hand, tail empirical
process with random levels never suffers from long memory. This is very
desirable from a practical point of view, since such the process may be used to
construct Hill estimator of the tail index. To prove our results we need to
establish several new results for regularly varying distribution functions,
which may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2939</identifier>
 <datestamp>2011-09-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2939</id><created>2010-01-17</created><authors><author><keyname>Kabaila</keyname><forenames>Paul</forenames></author><author><keyname>Giri</keyname><forenames>Khageswor</forenames></author><author><keyname>Leeb</keyname><forenames>Hannes</forenames></author></authors><title>Admissibility of the usual confidence interval in linear regression</title><categories>math.ST stat.TH</categories><journal-ref>Electronic Journal of Statistics, 4, 300-312 (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a linear regression model with independent and identically normally
distributed random errors. Suppose that the parameter of interest is a
specified linear combination of the regression parameters. We prove that the
usual confidence interval for this parameter is admissible within a broad class
of confidence intervals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2967</identifier>
 <datestamp>2010-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2967</id><created>2010-01-18</created><authors><author><keyname>Bernardo</keyname><forenames>Jos M.</forenames></author></authors><title>Comment on "Harold Jeffreys's Theory of Probability Revisited"</title><categories>stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/09-STS284E the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS284E</report-no><journal-ref>Statistical Science 2009, Vol. 24, No. 2, 173-175</journal-ref><doi>10.1214/09-STS284E</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comment on "Harold Jeffreys's Theory of Probability Revisited"
[arXiv:0804.3173]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2968</identifier>
 <datestamp>2010-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2968</id><created>2010-01-18</created><authors><author><keyname>Gelman</keyname><forenames>Andrew</forenames></author></authors><title>Bayes, Jeffreys, Prior Distributions and the Philosophy of Statistics</title><categories>stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/09-STS284D the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS284D</report-no><journal-ref>Statistical Science 2009, Vol. 24, No. 2, 176-178</journal-ref><doi>10.1214/09-STS284D</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discussion of "Harold Jeffreys's Theory of Probability revisited," by
Christian Robert, Nicolas Chopin, and Judith Rousseau, for Statistical Science
[arXiv:0804.3173]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2970</identifier>
 <datestamp>2010-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2970</id><created>2010-01-18</created><authors><author><keyname>Kass</keyname><forenames>Robert</forenames></author></authors><title>Comment: The Importance of Jeffreys's Legacy</title><categories>stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/09-STS284A the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS284A</report-no><journal-ref>Statistical Science 2009, Vol. 24, No. 2, 179-182</journal-ref><doi>10.1214/09-STS284A</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theory of Probability is distinguished by several high-level philosophical
attitudes, some stressed by Jeffreys, some implicit. By reviewing these we may
recognize the importance in this work in the historical development of
statistics. [arXiv:0804.3173]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2975</identifier>
 <datestamp>2010-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2975</id><created>2010-01-18</created><authors><author><keyname>Senn</keyname><forenames>Stephen</forenames></author></authors><title>Comment on "Harold Jeffreys's Theory of Probability Revisited"</title><categories>stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/09-STS284B the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS284B</report-no><journal-ref>Statistical Science 2009, Vol. 24, No. 2, 185-186</journal-ref><doi>10.1214/09-STS284B</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comment on "Harold Jeffreys's Theory of Probability Revisited"
[arXiv:0804.3173]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.2985</identifier>
 <datestamp>2010-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.2985</id><created>2010-01-18</created><authors><author><keyname>Zellner</keyname><forenames>Arnold</forenames></author></authors><title>Comment on "Harold Jeffreys's Theory of Probability Revisited"</title><categories>stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/09-STS284C the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS284C</report-no><journal-ref>Statistical Science 2009, Vol. 24, No. 2, 187-190</journal-ref><doi>10.1214/09-STS284C</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comment on "Harold Jeffreys's Theory of Probability Revisited"
[arXiv:0804.3173]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3006</identifier>
 <datestamp>2010-01-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3006</id><created>2010-01-18</created><authors><author><keyname>Rei</keyname><forenames>Markus</forenames></author></authors><title>Asymptotic equivalence and sufficiency for volatility estimation under
  microstructure noise</title><categories>math.ST math.PR q-fin.ST stat.AP stat.TH</categories><msc-class>62G20, 62B15, 62M10,91B84</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The basic model for high-frequency data in finance is considered, where an
efficient price process is observed under microstructure noise. It is shown
that this nonparametric model is in Le Cam's sense asymptotically equivalent to
a Gaussian shift experiment in terms of the square root of the volatility
function $\sigma$. As an application, simple rate-optimal estimators of the
volatility and efficient estimators of the integrated volatility are
constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3011</identifier>
 <datestamp>2010-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3011</id><created>2010-01-18</created><authors><author><keyname>Booth</keyname><forenames>James G.</forenames></author><author><keyname>Federer</keyname><forenames>Walter T.</forenames></author><author><keyname>Wells</keyname><forenames>Martin T.</forenames></author><author><keyname>Wolfinger</keyname><forenames>Russell D.</forenames></author></authors><title>A Multivariate Variance Components Model for Analysis of Covariance in
  Designed Experiments</title><categories>stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/09-STS294 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS294</report-no><journal-ref>Statistical Science 2009, Vol. 24, No. 2, 223-237</journal-ref><doi>10.1214/09-STS294</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional methods for covariate adjustment of treatment means in designed
experiments are inherently conditional on the observed covariate values. In
order to develop a coherent general methodology for analysis of covariance, we
propose a multivariate variance components model for the joint distribution of
the response and covariates. It is shown that, if the design is orthogonal with
respect to (random) blocking factors, then appropriate adjustments to treatment
means can be made using the univariate variance components model obtained by
conditioning on the observed covariate values. However, it is revealed that
some widely used models are incorrectly specified, leading to biased estimates
and incorrect standard errors. The approach clarifies some issues that have
been the source of ongoing confusion in the statistics literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3073</identifier>
 <datestamp>2010-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3073</id><created>2010-01-18</created><authors><author><keyname>Lindley</keyname><forenames>Dennis</forenames></author></authors><title>Comment on "Harold Jeffreys's Theory of Probability Revisited"</title><categories>stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/09-STS284F the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS284F</report-no><journal-ref>Statistical Science 2009, Vol. 24, No. 2, 183-184</journal-ref><doi>10.1214/09-STS284F</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comment on "Harold Jeffreys's Theory of Probability Revisited"
[arXiv:0804.3173]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3084</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3084</id><created>2010-01-18</created><updated>2012-04-30</updated><authors><author><keyname>Mendo</keyname><forenames>Luis</forenames></author></authors><title>Asymptotically optimum estimation of a probability in inverse binomial
  sampling under general loss functions</title><categories>math.ST stat.TH</categories><comments>Journal of Statistical Planning and Inference. Published online 2012</comments><msc-class>62L12, 62F12</msc-class><doi>10.1016/j.jspi.2012.03.026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimum quality that can be asymptotically achieved in the estimation of
a probability p using inverse binomial sampling is addressed. A general
definition of quality is used in terms of the risk associated with a loss
function that satisfies certain assumptions. It is shown that the limit
superior of the risk for p asymptotically small has a minimum over all
(possibly randomized) estimators. This minimum is achieved by certain
non-randomized estimators. The model includes commonly used quality criteria as
particular cases. Applications to the non-asymptotic regime are discussed
considering specific loss functions, for which minimax estimators are derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3090</identifier>
 <datestamp>2010-06-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3090</id><created>2010-01-18</created><updated>2010-06-13</updated><authors><author><keyname>Huang</keyname><forenames>Dayu</forenames></author><author><keyname>Meyn</keyname><forenames>Sean</forenames></author></authors><title>Feature Extraction for Universal Hypothesis Testing via Rank-constrained
  Optimization</title><categories>cs.IT cs.LG math.IT math.ST stat.TH</categories><comments>5 pages, 4 figures, submitted to ISIT 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns the construction of tests for universal hypothesis
testing problems, in which the alternate hypothesis is poorly modeled and the
observation space is large. The mismatched universal test is a feature-based
technique for this purpose. In prior work it is shown that its
finite-observation performance can be much better than the (optimal) Hoeffding
test, and good performance depends crucially on the choice of features. The
contributions of this paper include: 1) We obtain bounds on the number of
\epsilon distinguishable distributions in an exponential family. 2) This
motivates a new framework for feature extraction, cast as a rank-constrained
optimization problem. 3) We obtain a gradient-based algorithm to solve the
rank-constrained optimization problem and prove its local convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3109</identifier>
 <datestamp>2010-01-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3109</id><created>2010-01-18</created><authors><author><keyname>Haury</keyname><forenames>Anne-Claire</forenames><affiliation>CBIO</affiliation></author><author><keyname>Jacob</keyname><forenames>Laurent</forenames><affiliation>CBIO</affiliation></author><author><keyname>Vert</keyname><forenames>Jean-Philippe</forenames><affiliation>CBIO</affiliation></author></authors><title>Increasing stability and interpretability of gene expression signatures</title><categories>stat.ML q-bio.GN q-bio.QM stat.AP</categories><proxy>ccsd hal-00448395</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation : Molecular signatures for diagnosis or prognosis estimated from
large-scale gene expression data often lack robustness and stability, rendering
their biological interpretation challenging. Increasing the signature's
interpretability and stability across perturbations of a given dataset and, if
possible, across datasets, is urgently needed to ease the discovery of
important biological processes and, eventually, new drug targets. Results : We
propose a new method to construct signatures with increased stability and
easier interpretability. The method uses a gene network as side interpretation
and enforces a large connectivity among the genes in the signature, leading to
signatures typically made of genes clustered in a few subnetworks. It combines
the recently proposed graph Lasso procedure with a stability selection
procedure. We evaluate its relevance for the estimation of a prognostic
signature in breast cancer, and highlight in particular the increase in
interpretability and stability of the signature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3195</identifier>
 <datestamp>2012-02-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3195</id><created>2010-01-18</created><updated>2010-06-21</updated><authors><author><keyname>Drton</keyname><forenames>Mathias</forenames></author><author><keyname>Yu</keyname><forenames>Josephine</forenames></author></authors><title>On a parametrization of positive semidefinite matrices with zeros</title><categories>math.AG math.OC math.ST stat.TH</categories><comments>16 pages, 1 figure. New shorter proof of Lemma 5.6. Final version, to
  appear in SIAM Journal on Matrix Analysis and Applications</comments><msc-class>14P10, 15A99, 13P25, 62H05</msc-class><journal-ref>SIAM J. Matrix Anal. Appl. 31 (2010), no. 5, 2665--2680</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a class of parametrizations of convex cones of positive semidefinite
matrices with prescribed zeros. Each such cone corresponds to a graph whose
non-edges determine the prescribed zeros. Each parametrization in this class is
a polynomial map associated with a simplicial complex supported on cliques of
the graph. The images of the maps are convex cones, and the maps can only be
surjective onto the cone of zero-constrained positive semidefinite matrices
when the associated graph is chordal and the simplicial complex is the clique
complex of the graph. Our main result gives a semi-algebraic description of the
image of the parametrizations for chordless cycles. The work is motivated by
the fact that the considered maps correspond to Gaussian statistical models
with hidden variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3209</identifier>
 <datestamp>2011-03-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3209</id><created>2010-01-19</created><updated>2011-03-09</updated><authors><author><keyname>Arias-Castro</keyname><forenames>Ery</forenames></author><author><keyname>Cands</keyname><forenames>Emmanuel J.</forenames></author><author><keyname>Durand</keyname><forenames>Arnaud</forenames></author></authors><title>Detection of an anomalous cluster in a network</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS839 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS839</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 1, 278-304</journal-ref><doi>10.1214/10-AOS839</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of detecting whether or not, in a given sensor
network, there is a cluster of sensors which exhibit an "unusual behavior."
Formally, suppose we are given a set of nodes and attach a random variable to
each node. We observe a realization of this process and want to decide between
the following two hypotheses: under the null, the variables are i.i.d. standard
normal; under the alternative, there is a cluster of variables that are i.i.d.
normal with positive mean and unit variance, while the rest are i.i.d. standard
normal. We also address surveillance settings where each sensor in the network
collects information over time. The resulting model is similar, now with a time
series attached to each node. We again observe the process over time and want
to decide between the null, where all the variables are i.i.d. standard normal,
and the alternative, where there is an emerging cluster of i.i.d. normal
variables with positive mean and unit variance. The growth models used to
represent the emerging cluster are quite general and, in particular, include
cellular automata used in modeling epidemics. In both settings, we consider
classes of clusters that are quite general, for which we obtain a lower bound
on their respective minimax detection rate and show that some form of scan
statistic, by far the most popular method in practice, achieves that same rate
to within a logarithmic factor. Our results are not limited to the normal
location model, but generalize to any one-parameter exponential family when the
anomalous clusters are large enough.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3253</identifier>
 <datestamp>2010-01-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3253</id><created>2010-01-19</created><authors><author><keyname>Kadane</keyname><forenames>Joseph B.</forenames></author></authors><title>Bayesian Thought in Early Modern Detective Stories: Monsieur Lecoq, C.
  Auguste Dupin and Sherlock Holmes</title><categories>stat.ME math.HO</categories><comments>Published in the Statistical Science (http://www.imstat.org/sts/) by
  the Institute of Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS298</report-no><journal-ref>Statistical Science 2009, Vol. 24, No. 2, 238-243</journal-ref><doi>10.1214/09-STS298</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reviews the maxims used by three early modern fictional
detectives: Monsieur Lecoq, C. Auguste Dupin and Sherlock Holmes. It find
similarities between these maxims and Bayesian thought. Poe's Dupin uses ideas
very similar to Bayesian game theory. Sherlock Holmes' statements also show
thought patterns justifiable in Bayesian terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3272</identifier>
 <datestamp>2010-01-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3272</id><created>2010-01-19</created><authors><author><keyname>Wells</keyname><forenames>Martin T.</forenames></author></authors><title>A Conversation with Shayle R. Searle</title><categories>stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/08-STS259 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS259</report-no><journal-ref>Statistical Science 2009, Vol. 24, No. 2, 244-254</journal-ref><doi>10.1214/08-STS259</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Born in New Zealand, Shayle Robert Searle earned a bachelor's degree (1949)
and a master's degree (1950) from Victoria University, Wellington, New Zealand.
After working for an actuary, Searle went to Cambridge University where he
earned a Diploma in mathematical statistics in 1953. Searle won a Fulbright
travel award to Cornell University, where he earned a doctorate in animal
breeding, with a strong minor in statistics in 1959, studying under Professor
Charles Henderson. In 1962, Cornell invited Searle to work in the university's
computing center, and he soon joined the faculty as an assistant professor of
biological statistics. He was promoted to associate professor in 1965, and
became a professor of biological statistics in 1970. Searle has also been a
visiting professor at Texas A&amp;M University, Florida State University,
Universit\"{a}t Augsburg and the University of Auckland. He has published
several statistics textbooks and has authored more than 165 papers. Searle is a
Fellow of the American Statistical Association, the Royal Statistical Society,
and he is an elected member of the International Statistical Institute. He also
has received the prestigious Alexander von Humboldt U.S. Senior Scientist
Award, is an Honorary Fellow of the Royal Society of New Zealand and was
recently awarded the D.Sc. Honoris Causa by his alma mater, Victoria University
of Wellington, New Zealand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3327</identifier>
 <datestamp>2010-01-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3327</id><created>2010-01-19</created><authors><author><keyname>Lopez-Ruiz</keyname><forenames>Ricardo</forenames></author><author><keyname>Sanudo</keyname><forenames>Jaime</forenames></author><author><keyname>Calbet</keyname><forenames>Xavier</forenames></author></authors><title>Equilibrium Distributions in Open and Closed Statistical Systems</title><categories>nlin.CD cond-mat.stat-mech math.ST stat.TH</categories><comments>5 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this communication, the derivation of the Boltzmann-Gibbs and the
Maxwellian distributions is presented from a geometrical point of view under
the hypothesis of equiprobability. It is shown that both distributions can be
obtained by working out the properties of the volume or the surface of the
respective geometries delimited in phase space by an additive constraint. That
is, the asymptotic equilibrium distributions in the thermodynamic limit are
independent of considering open or closed homogeneous statistical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3355</identifier>
 <datestamp>2011-04-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3355</id><created>2010-01-19</created><updated>2011-04-15</updated><authors><author><keyname>Sutton</keyname><forenames>Charles</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Bayesian inference for queueing networks and modeling of internet
  services</title><categories>stat.ML cs.NI cs.PF stat.AP stat.CO</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOAS392 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS392</report-no><journal-ref>Annals of Applied Statistics 2011, Vol. 5, No. 1, 254-282</journal-ref><doi>10.1214/10-AOAS392</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern Internet services, such as those at Google, Yahoo!, and Amazon, handle
billions of requests per day on clusters of thousands of computers. Because
these services operate under strict performance requirements, a statistical
understanding of their performance is of great practical interest. Such
services are modeled by networks of queues, where each queue models one of the
computers in the system. A key challenge is that the data are incomplete,
because recording detailed information about every request to a heavily used
system can require unacceptable overhead. In this paper we develop a Bayesian
perspective on queueing models in which the arrival and departure times that
are not observed are treated as latent variables. Underlying this viewpoint is
the observation that a queueing model defines a deterministic transformation
between the data and a set of independent variables called the service times.
With this viewpoint in hand, we sample from the posterior distribution over
missing data and model parameters using Markov chain Monte Carlo. We evaluate
our framework on data from a benchmark Web application. We also present a
simple technique for selection among nested queueing models. We are unaware of
any previous work that considers inference in networks of queues in the
presence of missing data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3404</identifier>
 <datestamp>2011-12-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3404</id><created>2010-01-20</created><updated>2011-12-14</updated><authors><author><keyname>Gamal</keyname><forenames>Abbas El</forenames></author><author><keyname>Kim</keyname><forenames>Young-Han</forenames></author></authors><title>Lecture Notes on Network Information Theory</title><categories>cs.IT cs.NI math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These lecture notes have been converted to a book titled Network Information
Theory published recently by Cambridge University Press. This book provides a
significantly expanded exposition of the material in the lecture notes as well
as problems and bibliographic notes at the end of each chapter. The authors are
currently preparing a set of slides based on the book that will be posted in
the second half of 2012. More information about the book can be found at
http://www.cambridge.org/9781107008731/. The previous (and obsolete) version of
the lecture notes can be found at http://arxiv.org/abs/1001.3404v4/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3448</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3448</id><created>2010-01-19</created><updated>2011-01-27</updated><authors><author><keyname>Bayati</keyname><forenames>Mohsen</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>The dynamics of message passing on dense graphs, with applications to
  compressed sensing</title><categories>cs.IT cs.LG math.IT math.ST stat.TH</categories><comments>41 pages</comments><journal-ref>IEEE Transactions on Information Theory, Vol 57, Issue 2 pp.
  764-785, 2011</journal-ref><doi>10.1109/TIT.2010.2094817</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate message passing algorithms proved to be extremely effective in
reconstructing sparse signals from a small number of incoherent linear
measurements. Extensive numerical experiments further showed that their
dynamics is accurately tracked by a simple one-dimensional iteration termed
state evolution. In this paper we provide the first rigorous foundation to
state evolution. We prove that indeed it holds asymptotically in the large
system limit for sensing matrices with independent and identically distributed
gaussian entries.
  While our focus is on message passing algorithms for compressed sensing, the
analysis extends beyond this setting, to a general class of algorithms on dense
graphs. In this context, state evolution plays the role that density evolution
has for sparse graphs.
  The proof technique is fundamentally different from the standard approach to
density evolution, in that it copes with large number of short loops in the
underlying factor graph. It relies instead on a conditioning technique recently
developed by Erwin Bolthausen in the context of spin glass theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3480</identifier>
 <datestamp>2011-09-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3480</id><created>2010-01-20</created><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Roch</keyname><forenames>Sebastien</forenames></author><author><keyname>Sly</keyname><forenames>Allan</forenames></author></authors><title>On the inference of large phylogenies with long branches: How long is
  too long?</title><categories>math.PR cs.CE cs.DS math.ST q-bio.PE stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has highlighted deep connections between sequence-length
requirements for high-probability phylogeny reconstruction and the related
problem of the estimation of ancestral sequences. In [Daskalakis et al.'09],
building on the work of [Mossel'04], a tight sequence-length requirement was
obtained for the CFN model. In particular the required sequence length for
high-probability reconstruction was shown to undergo a sharp transition (from
$O(\log n)$ to $\hbox{poly}(n)$, where $n$ is the number of leaves) at the
"critical" branch length $\critmlq$ (if it exists) of the ancestral
reconstruction problem.
  Here we consider the GTR model. For this model, recent results of [Roch'09]
show that the tree can be accurately reconstructed with sequences of length
$O(\log(n))$ when the branch lengths are below $\critksq$, known as the
Kesten-Stigum (KS) bound. Although for the CFN model $\critmlq = \critksq$, it
is known that for the more general GTR models one has $\critmlq \geq \critksq$
with a strict inequality in many cases. Here, we show that this phenomenon also
holds for phylogenetic reconstruction by exhibiting a family of symmetric
models $Q$ and a phylogenetic reconstruction algorithm which recovers the tree
from $O(\log n)$-length sequences for some branch lengths in the range
$(\critksq,\critmlq)$. Second we prove that phylogenetic reconstruction under
GTR models requires a polynomial sequence-length for branch lengths above
$\critmlq$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3742</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3742</id><created>2010-01-21</created><updated>2013-02-13</updated><authors><author><keyname>Dou</keyname><forenames>Winston Wei</forenames></author><author><keyname>Pollard</keyname><forenames>David</forenames></author><author><keyname>Zhou</keyname><forenames>Harrison H.</forenames></author></authors><title>Estimation in functional regression for general exponential families</title><categories>math.ST math.PR stat.ME stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOS1027 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1027</report-no><journal-ref>Annals of Statistics 2012, Vol. 40, No. 5, 2421-2451</journal-ref><doi>10.1214/12-AOS1027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a class of exponential family models whose canonical
parameters are specified as linear functionals of an unknown
infinite-dimensional slope function. The optimal minimax rates of convergence
for slope function estimation are established. The estimators that achieve the
optimal rates are constructed by constrained maximum likelihood estimation with
parameters whose dimension grows with sample size. A change-of-measure
argument, inspired by Le Cam's theory of asymptotic equivalence, is used to
eliminate the bias caused by the nonlinearity of exponential family models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3859</identifier>
 <datestamp>2010-02-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3859</id><created>2010-01-21</created><authors><author><keyname>Yu</keyname><forenames>Yaming</forenames></author></authors><title>Strict Monotonicity and Convergence Rate of Titterington's Algorithm for
  Computing D-optimal Designs</title><categories>stat.ME stat.CO</categories><journal-ref>Computational Statistics and Data Analysis 54 (2010) 1419--1425.</journal-ref><doi>10.1016/j.csda.2010.01.026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a class of multiplicative algorithms introduced by Silvey et al.
(1978) for computing D-optimal designs. Strict monotonicity is established for
a variant considered by Titterington (1978). A formula for the rate of
convergence is also derived. This is used to explain why modifications
considered by Titterington (1978) and Dette et al. (2008) usually converge
faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3886</identifier>
 <datestamp>2010-01-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3886</id><created>2010-01-21</created><authors><author><keyname>Delaigle</keyname><forenames>Aurore</forenames></author><author><keyname>Hall</keyname><forenames>Peter</forenames></author><author><keyname>Jin</keyname><forenames>Jiashun</forenames></author></authors><title>Robustness and accuracy of methods for high dimensional data analysis
  based on Student's t statistic</title><categories>stat.ME</categories><comments>37 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Student's $t$ statistic is finding applications today that were never
envisaged when it was introduced more than a century ago. Many of these
applications rely on properties, for example robustness against heavy tailed
sampling distributions, that were not explicitly considered until relatively
recently. In this paper we explore these features of the $t$ statistic in the
context of its application to very high dimensional problems, including feature
selection and ranking, highly multiple hypothesis testing, and sparse, high
dimensional signal detection. Robustness properties of the $t$-ratio are
highlighted, and it is established that those properties are preserved under
applications of the bootstrap. In particular, bootstrap methods correct for
skewness, and therefore lead to second-order accuracy, even in the extreme
tails. Indeed, it is shown that the bootstrap, and also the more popular but
less accurate $t$-distribution and normal approximations, are more effective in
the tails than towards the middle of the distribution. These properties
motivate new methods, for example bootstrap-based techniques for signal
detection, that confine attention to the significant tail of a statistic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3895</identifier>
 <datestamp>2010-06-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3895</id><created>2010-01-21</created><updated>2010-06-13</updated><authors><author><keyname>Qi</keyname><forenames>Lei</forenames></author><author><keyname>Xiu</keyname><forenames>Dacheng</forenames></author><author><keyname>Fan</keyname><forenames>Jianqing</forenames></author></authors><title>Non-Gaussian Quasi Maximum Likelihood Estimation of GARCH Models</title><categories>stat.ME math.ST stat.AP stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The non-Gaussian quasi maximum likelihood estimator is frequently used in
GARCH models with intension to improve the efficiency of the GARCH parameters.
However, unless the quasi-likelihood happens to be the true one, non-Gaussian
QMLE methods suffers inconsistency even if shape parameters in the
quasi-likelihood are estimated. To correct this bias, we identify an unknown
scale parameter that is critical to the consistent estimation of non-Gaussian
QMLE, and propose a two-step non-Gaussian QMLE (2SNG-QMLE) for estimation of
the scale parameter and GARCH parameters. This novel approach is consistent and
asymptotically normal. Moreover, it has higher efficiency than the Gaussian
QMLE, particularly when the innovation error has heavy tails. Two extensions
are proposed to further improve the efficiency of 2SNG-QMLE. The impact of
relative heaviness of tails of the innovation and quasi-likelihood
distributions on the asymptotic efficiency has been thoroughly investigated.
Monte Carlo simulations and an empirical study confirm the advantages of the
proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.3907</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.3907</id><created>2010-01-21</created><updated>2011-11-11</updated><authors><author><keyname>Aravkin</keyname><forenames>Aleksandr Y.</forenames></author><author><keyname>Burke</keyname><forenames>James V.</forenames></author><author><keyname>Pillonetto</keyname><forenames>Gianluigi</forenames></author></authors><title>Robust and Trend-following Kalman Smoothers using Student's t</title><categories>stat.CO math.OC stat.AP</categories><comments>7 pages, 4 figures</comments><msc-class>62F35, 65K10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two nonlinear Kalman smoothers that rely on Student's t
distributions. The T-Robust smoother finds the maximum a posteriori likelihood
(MAP) solution for Gaussian process noise and Student's t observation noise,
and is extremely robust against outliers, outperforming the recently proposed
l1-Laplace smoother in extreme situations (e.g. 50% or more outliers). The
second estimator, which we call the T-Trend smoother, is able to follow sudden
changes in the process model, and is derived as a MAP solver for a model with
Student's t-process noise and Gaussian observation noise. We design specialized
methods to solve both problems which exploit the special structure of the
Student's t-distribution, and provide a convergence theory. Both smoothers can
be implemented with only minor modifications to an existing L2 smoother
implementation. Numerical results for linear and nonlinear models illustrating
both robust and fast tracking applications are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.4019</identifier>
 <datestamp>2010-01-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.4019</id><created>2010-01-22</created><authors><author><keyname>Tang</keyname><forenames>Xiao</forenames></author><author><keyname>Zhu</keyname><forenames>Mu</forenames></author></authors><title>Classifying Network Data with Deep Kernel Machines</title><categories>stat.ML stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by a growing interest in analyzing network data, we study the
problem of node classification on graphs, focusing on approaches based on
kernel machines. Conventionally, kernel machines are linear classifiers in the
implicit feature space. We argue that linear classification in the feature
space of kernels commonly used for graphs is often not enough to produce good
results. When this is the case, one naturally considers nonlinear classifiers
in the feature space. We show that repeating this process produces something we
call "deep kernel machines." We provide some examples where deep kernel
machines can make a big difference in classification performance, and point out
some connections to various recent literature on deep architectures in
artificial intelligence and machine learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.4070</identifier>
 <datestamp>2010-01-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.4070</id><created>2010-01-22</created><authors><author><keyname>Delorme</keyname><forenames>Nicolas</forenames><affiliation>SENS</affiliation></author><author><keyname>Boich</keyname><forenames>Julie</forenames><affiliation>SENS</affiliation></author><author><keyname>Raspaud</keyname><forenames>Michel</forenames><affiliation>SENS</affiliation></author></authors><title>Relative Age Effect in Elite Sports: Methodological Bias or Real
  Discrimination?</title><categories>stat.ME physics.data-an physics.soc-ph</categories><proxy>ccsd hal-00410098</proxy><journal-ref>European Journal of Sport Science 10, 2 (2010) 91-96</journal-ref><doi>10.1080/17461390903271584</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sport sciences researchers talk about a relative age effect when they observe
a biased distribution of elite athletes' birthdates, with an
over-representation of those born at the beginning of the competitive year and
an under-representation of those born at the end. Using the whole sample of the
French male licensed soccer players (n = 1,831,524), our study suggests that
there could be an important bias in the statistical test of this effect. This
bias could in turn lead to falsely conclude to a systemic discrimination in the
recruitment of professional players. Our findings question the accuracy of past
results concerning the existence of this effect at the elite level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.4083</identifier>
 <datestamp>2010-01-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.4083</id><created>2010-01-22</created><authors><author><keyname>Bornn</keyname><forenames>Luke</forenames></author><author><keyname>Gottardo</keyname><forenames>Raphael</forenames></author><author><keyname>Doucet</keyname><forenames>Arnaud</forenames></author></authors><title>Grouping Priors and the Bayesian Elastic Net</title><categories>stat.ME</categories><report-no>Technical Report #254, Department of Statistics, University of
  British Columbia</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the literature surrounding Bayesian penalized regression, the two primary
choices of prior distribution on the regression coefficients are zero-mean
Gaussian and Laplace. While both have been compared numerically and
theoretically, there remains little guidance on which to use in real-life
situations. We propose two viable solutions to this problem in the form of
prior distributions which combine and compromise between Laplace and Gaussian
priors, respectively. Through cross-validation the prior which optimizes
prediction performance is automatically selected. We then demonstrate the
improved performance of these new prior distributions relative to Laplace and
Gaussian priors in both a simulated and experimental environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.4208</identifier>
 <datestamp>2010-01-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.4208</id><created>2010-01-23</created><authors><author><keyname>Rodriguez</keyname><forenames>Abel</forenames></author><author><keyname>Lenkoski</keyname><forenames>Alex</forenames></author><author><keyname>Dobra</keyname><forenames>Adrian</forenames></author></authors><title>Sparse covariance estimation in heterogeneous samples</title><categories>stat.ME stat.AP stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard Gaussian graphical models (GGMs) implicitly assume that the
conditional independence among variables is common to all observations in the
sample. However, in practice, observations are usually collected form
heterogeneous populations where such assumption is not satisfied, leading in
turn to nonlinear relationships among variables. To tackle these problems we
explore mixtures of GGMs; in particular, we consider both infinite mixture
models of GGMs and infinite hidden Markov models with GGM emission
distributions. Such models allow us to divide a heterogeneous population into
homogenous groups, with each cluster having its own conditional independence
structure. The main advantage of considering infinite mixtures is that they
allow us easily to estimate the number of number of subpopulations in the
sample. As an illustration, we study the trends in exchange rate fluctuations
in the pre-Euro era. This example demonstrates that the models are very
flexible while providing extremely interesting interesting insights into
real-life applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.4351</identifier>
 <datestamp>2010-11-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.4351</id><created>2010-01-25</created><authors><author><keyname>Gunnarsson</keyname><forenames>O.</forenames></author><author><keyname>Haverkort</keyname><forenames>M. W.</forenames></author><author><keyname>Sangiovanni</keyname><forenames>G.</forenames></author></authors><title>Analytical continuation of imaginary axis data using maximum entropy</title><categories>physics.data-an cond-mat.str-el stat.ME</categories><comments>10 pages and 8 figures</comments><journal-ref>Phys. Rev. B 81, 155107 (2010)</journal-ref><doi>10.1103/PhysRevB.81.155107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the maximum entropy (MaxEnt) approach for analytical continuation of
spectral data from imaginary times to real frequencies. The total error is
divided in a statistical error, due to the noise in the input data, and a
systematic error, due to deviations of the default function, used in the MaxEnt
approach, from the exact spectrum. We find that the MaxEnt approach in its
classical formulation can lead to a nonoptimal balance between the two types of
errors, leading to an unnecessary large statistical error. The statistical
error can be reduced by splitting up the data in several batches, performing a
MaxEnt calculation for each batch and averaging. This can outweigh an increase
in the systematic error resulting from this approach. The output from the
MaxEnt result can be used as a default function for a new MaxEnt calculation.
Such iterations often lead to worse results due to an increase in the
statistical error. By splitting up the data in batches, the statistical error
is reduced and and the increase resulting from iterations can be outweighed by
a decrease in the systematic error. Finally we consider a linearized version to
obtain a better understanding of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.4425</identifier>
 <datestamp>2010-01-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.4425</id><created>2010-01-25</created><authors><author><keyname>Niang</keyname><forenames>Sophie Dabo</forenames><affiliation>EQUIPPE</affiliation></author><author><keyname>Thiam</keyname><forenames>Baba</forenames><affiliation>EQUIPPE</affiliation></author></authors><title>Robust quantile estimation and prediction for spatial processes</title><categories>math.ST stat.TH</categories><comments>13 pages</comments><proxy>ccsd hal-00450101</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a statistical framework for modeling conditional
quantiles of spatial processes assumed to be strongly mixing in space. We
establish the $L_1$ consistency and the asymptotic normality of the kernel
conditional quantile estimator in the case of random fields. We also define a
nonparametric spatial predictor and illustrate the methodology used with some
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.4432</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.4432</id><created>2010-01-25</created><updated>2010-05-27</updated><authors><author><keyname>Harremos</keyname><forenames>Peter</forenames></author><author><keyname>Vajda</keyname><forenames>Igor</forenames></author></authors><title>Joint Range of f-divergences</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>Accepted for presentation at ISIT 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a general method for evaluation of the joint range of
f-divergences for two different functions f. Via topological arguments we prove
that the joint range for general distributions equals the convex hull of the
joint range achieved by the distributions on a two-element set. The joint range
technique provides important inequalities between different f-divergences with
various applications in information theory and statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.4475</identifier>
 <datestamp>2011-04-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.4475</id><created>2010-01-25</created><updated>2011-04-13</updated><authors><author><keyname>Bubeck</keyname><forenames>Sbastien</forenames><affiliation>INRIA Futurs</affiliation></author><author><keyname>Munos</keyname><forenames>Rmi</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Stoltz</keyname><forenames>Gilles</forenames><affiliation>DMA, GREGH, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Szepesvari</keyname><forenames>Csaba</forenames></author></authors><title>X-Armed Bandits</title><categories>cs.LG cs.SY math.OC math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a generalization of stochastic bandits where the set of arms,
$\cX$, is allowed to be a generic measurable space and the mean-payoff function
is "locally Lipschitz" with respect to a dissimilarity function that is known
to the decision maker. Under this condition we construct an arm selection
policy, called HOO (hierarchical optimistic optimization), with improved regret
bounds compared to previous results for a large class of problems. In
particular, our results imply that if $\cX$ is the unit hypercube in a
Euclidean space and the mean-payoff function has a finite number of global
maxima around which the behavior of the function is locally continuous with a
known smoothness degree, then the expected regret of HOO is bounded up to a
logarithmic factor by $\sqrt{n}$, i.e., the rate of growth of the regret is
independent of the dimension of the space. We also prove the minimax optimality
of our algorithm when the dissimilarity is a metric. Our basic strategy has
quadratic computational complexity as a function of the number of time steps
and does not rely on the doubling trick. We also introduce a modified strategy,
which relies on the doubling trick but runs in linearithmic time. Both results
are improvements with respect to previous approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.4656</identifier>
 <datestamp>2010-02-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.4656</id><created>2010-01-26</created><updated>2010-02-09</updated><authors><author><keyname>Robert</keyname><forenames>Christian P.</forenames></author><author><keyname>Rousseau</keyname><forenames>Judith</forenames></author></authors><title>On Bayesian Data Analysis</title><categories>stat.ME</categories><comments>16 pages, 2 figures, 2 tables, chapter of the contributed volume
  "Bayesian Methods and Expert Elicitation", Risk Book, London</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This introduction to Bayesian statistics presents the main concepts as well
as the principal reasons advocated in favour of a Bayesian modelling. We cover
the various approaches to prior determination as well as the basis asymptotic
arguments in favour of using Bayes estimators. The testing aspects of Bayesian
inference are also examined in details.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.4684</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.4684</id><created>2010-01-26</created><updated>2010-08-16</updated><authors><author><keyname>Hashorva</keyname><forenames>Enkelejd</forenames></author></authors><title>On Beta-Product Convolutions</title><categories>math.PR math.ST stat.TH</categories><comments>12 pages</comments><msc-class>60F05, 60G70</msc-class><doi>10.1080/03461238.2011.555939</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let R be a positive random variable independent of S which is beta
distributed. In this paper we are interested on the relation between the
distribution function of R and that of RS. For this model we derive first some
distributional properties, and then investigate the lower tail asymptotics of
RS when R is regularly varying at 0, and vice-versa. Our first application
concerns the asymptotic behaviour of the componentwise sample minima related to
an elliptical distributions. Further, we derive the lower tails asymptotic of
the aggregated risk for bivariate polar distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.4762</identifier>
 <datestamp>2010-01-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.4762</id><created>2010-01-26</created><authors><author><keyname>Wang</keyname><forenames>Peijie</forenames></author><author><keyname>Jones</keyname><forenames>Trefor</forenames></author></authors><title>A Spectral Analysis of Business Cycle Patterns in UK Sectoral Output</title><categories>q-fin.GN q-fin.ST stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies business cycle patterns in UK sectoral output. It analyzes
the distinction between white noise processes and their non-white noise
counterparts in the frequency domain and further examines the associated
features and patterns for the process where white noise conditions are
violated. The characteristics of these sectors, arising from their
institutional features that may influence business cycles behavior and
patterns, are discussed. The study then investigates the output of UK GDP
sectors empirically, revealing their similarities and differences in their
business cycle patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.4776</identifier>
 <datestamp>2011-01-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.4776</id><created>2010-01-26</created><updated>2011-01-21</updated><authors><author><keyname>Schifano</keyname><forenames>Elizabeth D.</forenames></author><author><keyname>Strawderman</keyname><forenames>Robert L.</forenames></author><author><keyname>Wells</keyname><forenames>Martin T.</forenames></author></authors><title>MM Algorithms for Minimizing Nonsmoothly Penalized Objective Functions</title><categories>stat.CO math.ST stat.TH</categories><comments>A revised version of this paper has been published in the Electronic
  Journal of Statistics</comments><journal-ref>Electronic Journal of Statistics, Volume 4 (2010), pages 1258-1299</journal-ref><doi>10.1214/10-EJS582</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a general class of algorithms for optimizing an
extensive variety of nonsmoothly penalized objective functions that satisfy
certain regularity conditions. The proposed framework utilizes the
majorization-minimization (MM) algorithm as its core optimization engine. The
resulting algorithms rely on iterated soft-thresholding, implemented
componentwise, allowing for fast, stable updating that avoids the need for any
high-dimensional matrix inversion. We establish a local convergence theory for
this class of algorithms under weaker assumptions than previously considered in
the statistical literature. We also demonstrate the exceptional effectiveness
of new acceleration methods, originally proposed for the EM algorithm, in this
class of problems. Simulation results and a microarray data example are
provided to demonstrate the algorithm's capabilities and versatility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.4802</identifier>
 <datestamp>2010-01-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.4802</id><created>2010-01-26</created><authors><author><keyname>Shao</keyname><forenames>Yongwu</forenames></author><author><keyname>Cook</keyname><forenames>R Dennis</forenames></author><author><keyname>Weisberg</keyname><forenames>Sanford</forenames></author></authors><title>The linearity condition and adaptive estimation in single-index
  regressions</title><categories>math.ST stat.TH</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that under a linearity condition on the distribution of the
predictors, the coefficient in single-index regression can be estimated with
the same efficiency as in the case when the link function is known. Thus, the
linearity condition seems to substitute for knowing the exact conditional
distribution of the response given the linear combinations of the predictors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.5004</identifier>
 <datestamp>2010-01-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.5004</id><created>2010-01-27</created><authors><author><keyname>Rmon</keyname><forenames>M.</forenames></author></authors><title>"Additivity" versus "Maxitivity" at the heart of the paradoxical and
  efficient nature of Statistics</title><categories>stat.ME math.ST stat.TH</categories><comments>19 pages, 40 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike the Probability Theory based on additivity, Statistical Inference
seems to hesitate between "Additivity" and a so-called "Maxitivity" approach.
After a brief overview of three types of principles for any (parametric)
statistical theory and the proof that these principles are mutually exclusive,
the paper shows that two kinds of support measures are conceivable, an additive
one and a maxitive one (based on maximization operators). Unfortunately, none
of them is able to cope with the ignorance part of the statistical experiment
and, in the meantime, with the partial information given through the structure
of the data. To conclude, the author promotes the combined use of both
approaches, as an efficient middle-of-the-road position for the statistician.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.5058</identifier>
 <datestamp>2010-09-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.5058</id><created>2010-01-27</created><updated>2010-09-03</updated><authors><author><keyname>Mitra</keyname><forenames>Abhimanyu</forenames></author><author><keyname>Resnick</keyname><forenames>Sidney I.</forenames></author></authors><title>Hidden Regular Variation: Detection and Estimation</title><categories>math.PR math.ST q-fin.RM stat.TH</categories><comments>37 pages, 7 figures</comments><msc-class>60F99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hidden regular variation defines a subfamily of distributions satisfying
multivariate regular variation on $\mathbb{E} = [0, \infty]^d \backslash
\{(0,0, ..., 0) \} $ and models another regular variation on the sub-cone
$\mathbb{E}^{(2)} = \mathbb{E} \backslash \cup_{i=1}^d \mathbb{L}_i$, where
$\mathbb{L}_i$ is the $i$-th axis. We extend the concept of hidden regular
variation to sub-cones of $\mathbb{E}^{(2)}$ as well. We suggest a procedure
for detecting the presence of hidden regular variation, and if it exists,
propose a method of estimating the limit measure exploiting its semi-parametric
structure. We exhibit examples where hidden regular variation yields better
estimates of probabilities of risk sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.5103</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.5103</id><created>2010-01-28</created><updated>2010-05-31</updated><authors><author><keyname>Juditsky</keyname><forenames>Anatoli</forenames><affiliation>LJK</affiliation></author><author><keyname>Karzan</keyname><forenames>Fatma Kilinc</forenames><affiliation>ISyE</affiliation></author><author><keyname>Nemirovski</keyname><forenames>Arkadii S.</forenames><affiliation>ISyE</affiliation></author></authors><title>On Low Rank Matrix Approximations with Applications to Synthesis Problem
  in Compressed Sensing</title><categories>math.OC math.ST stat.TH</categories><proxy>ccsd</proxy><journal-ref>Mathematical Programming B 127, 1 (2011) 57-88</journal-ref><doi>10.1007/s10107-010-0417-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the synthesis problem of Compressed Sensing - given s and an MXn
matrix A, extract from it an mXn submatrix A', certified to be s-good, with m
as small as possible. Starting from the verifiable sufficient conditions of
s-goodness, we express the synthesis problem as the problem of approximating a
given matrix by a matrix of specified low rank in the uniform norm. We propose
randomized algorithms for efficient construction of rank k approximation of
matrices of size mXn achieving accuracy bounds O(1)sqrt({ln(mn)/k) which hold
in expectation or with high probability. We also supply derandomized versions
of the approximation algorithms which does not require random sampling of
matrices and attains the same accuracy bounds. We further demonstrate that our
algorithms are optimal up to the logarithmic in m and n factor. We provide
preliminary numerical results on the performance of our algorithms for the
synthesis problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.5109</identifier>
 <datestamp>2010-04-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.5109</id><created>2010-01-28</created><updated>2010-03-31</updated><authors><author><keyname>Robert</keyname><forenames>Christian P.</forenames></author></authors><title>The Search for Certainty: a critical assessment</title><categories>math.ST math.HO stat.TH</categories><comments>This is a revision of a book review of K. Burdzy's "The Search for
  Certainty", revision that is submitted to Bayesian Analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Search for Certainty was published in 2009 by Krzysztof Burdzy. It
examines the "philosophical duopoly" of von Mises and de Finetti at the
foundation of probability and statistics and find this duopoly missing. This
review exposes the weakness of the arguments presented in the book, it
questions the relevance of introducing a new set of probability axioms from a
methodological perspective, and it concludes at the lack of impact of this book
on statistical foundations and practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.5176</identifier>
 <datestamp>2012-01-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.5176</id><created>2010-01-28</created><updated>2010-07-15</updated><authors><author><keyname>van de Geer</keyname><forenames>Sara</forenames></author><author><keyname>Buhlmann</keyname><forenames>Peter</forenames></author><author><keyname>Zhou</keyname><forenames>Shuheng</forenames></author></authors><title>The adaptive and the thresholded Lasso for potentially misspecified
  models</title><categories>math.ST stat.TH</categories><comments>45 pages</comments><msc-class>62J07 62G08</msc-class><journal-ref>The Electronic Journal of Statistics 5 (2011) 688-749</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the adaptive Lasso as well as the thresholded Lasso with
refitting, in a high-dimensional linear model, and study prediction error,
$\ell_q$-error ($q \in \{1, 2 \} $), and number of false positive selections.
Our theoretical results for the two methods are, at a rather fine scale,
comparable. The differences only show up in terms of the (minimal) restricted
and sparse eigenvalues, favoring thresholding over the adaptive Lasso. As
regards prediction and estimation, the difference is virtually negligible, but
our bound for the number of false positives is larger for the adaptive Lasso
than for thresholding. Moreover, both these two-stage methods add value to the
one-stage Lasso in the sense that, under appropriate restricted and sparse
eigenvalue conditions, they have similar prediction and estimation error as the
one-stage Lasso, but substantially less false positives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.5265</identifier>
 <datestamp>2010-02-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.5265</id><created>2010-01-28</created><updated>2010-02-01</updated><authors><author><keyname>Withers</keyname><forenames>C. S.</forenames></author><author><keyname>Nadarajah</keyname><forenames>S.</forenames></author></authors><title>The distribution of the maximum of a second order autoregressive
  process: the continuous case</title><categories>math.ST stat.TH</categories><comments>8 pages This version removes an inappropriate note</comments><report-no>138cor/ar2.tex</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the distribution function of $M_n$, the maximum of a sequence of $n$
observations from an autoregressive process of order 2. Solutions are first
given in terms of repeated integrals and then for the case, where the
underlying random variables are absolutely continuous. When the correlations
are positive, P(M_n \leq x) =a_{n,x}, where a_{n,x}= \sum_{j=1}^\infty
\beta_{jx} \nu_{jx}^{n} = O (\nu_{1x}^{n}), where $\{\nu_{jx}\}$ are the
eigenvalues of a non-symmetric Fredholm kernel, and $\nu_{1x}$ is the
eigenvalue of maximum magnitude. The weights $\beta_{jx}$ depend on the $j$th
left and right eigenfunctions of the kernel.
  These results are large deviations expansions for estimates, since the
maximum need not be standardized to have a limit. In fact such a limit need not
exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.5311</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.5311</id><created>2010-01-29</created><updated>2010-05-27</updated><authors><author><keyname>Haupt</keyname><forenames>Jarvis</forenames></author><author><keyname>Castro</keyname><forenames>Rui</forenames></author><author><keyname>Nowak</keyname><forenames>Robert</forenames></author></authors><title>Distilled Sensing: Adaptive Sampling for Sparse Detection and Estimation</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><comments>23 pages, 2 figures. Revision includes minor clarifications, along
  with more illustrative experimental results (cf. Figure 2)</comments><report-no>Rice University ECE Technical Report TREE1001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive sampling results in dramatic improvements in the recovery of sparse
signals in white Gaussian noise. A sequential adaptive sampling-and-refinement
procedure called Distilled Sensing (DS) is proposed and analyzed. DS is a form
of multi-stage experimental design and testing. Because of the adaptive nature
of the data collection, DS can detect and localize far weaker signals than
possible from non-adaptive measurements. In particular, reliable detection and
localization (support estimation) using non-adaptive samples is possible only
if the signal amplitudes grow logarithmically with the problem dimension. Here
it is shown that using adaptive sampling, reliable detection is possible
provided the amplitude exceeds a constant, and localization is possible when
the amplitude exceeds any arbitrarily slowly growing function of the dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.5357</identifier>
 <datestamp>2010-02-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.5357</id><created>2010-01-29</created><authors><author><keyname>Barbour</keyname><forenames>A. D.</forenames></author><author><keyname>Reinert</keyname><forenames>G.</forenames></author></authors><title>The shortest distance in random multi-type intersection graphs</title><categories>math.PR math.ST stat.TH</categories><comments>32 pages</comments><msc-class>05C80, 60J85, 60E05, 60F05, 60E17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using an associated branching process as the basis of our approximation, we
show that typical inter-point distances in a multitype random intersection
graph have a defective distribution, which is well described by a mixture of
translated and scaled Gumbel distributions, the missing mass corresponding to
the event that the vertices are not in the same component of the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1001.5447</identifier>
 <datestamp>2010-02-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1001.5447</id><created>2010-01-29</created><authors><author><keyname>Hotz</keyname><forenames>Thomas</forenames></author><author><keyname>Marnitz</keyname><forenames>Philipp</forenames></author><author><keyname>Stichtenoth</keyname><forenames>Rahel</forenames></author><author><keyname>Davies</keyname><forenames>Laurie</forenames></author><author><keyname>Kabluchko</keyname><forenames>Zakhar</forenames></author><author><keyname>Munk</keyname><forenames>Axel</forenames></author></authors><title>Locally adaptive image denoising by a statistical multiresolution
  criterion</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate how one can choose the smoothing parameter in image denoising
by a statistical multiresolution criterion, both globally and locally. Using
inhomogeneous diffusion and total variation regularization as examples for
localized regularization schemes, we present an efficient method for locally
adaptive image denoising. As expected, the smoothing parameter serves as an
edge detector in this framework. Numerical examples illustrate the usefulness
of our approach. We also present an application in confocal microscopy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0042</identifier>
 <datestamp>2011-02-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0042</id><created>2010-01-30</created><updated>2011-02-18</updated><authors><author><keyname>Guntuboyina</keyname><forenames>Adityanand</forenames></author></authors><title>Lower bounds for the minimax risk using $f$-divergences and applications</title><categories>math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lower bounds involving $f$-divergences between the underlying probability
measures are proved for the minimax risk in estimation problems. Our proofs
just use simple convexity facts. Special cases and straightforward corollaries
of our bounds include well known inequalities for establishing minimax lower
bounds such as Fano's inequality, Pinsker's inequality and inequalities based
on global entropy conditions. Two applications are provided: a new minimax
lower bound for the reconstruction of convex bodies from noisy support function
measurements and a different proof of a recent minimax lower bound for the
estimation of a covariance matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0140</identifier>
 <datestamp>2010-02-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0140</id><created>2010-01-31</created><authors><author><keyname>Alabert</keyname><forenames>Aureli</forenames></author><author><keyname>Rangel</keyname><forenames>Luz Ma.</forenames></author></authors><title>Classifying the typefaces of the Gutenberg 42-line bible</title><categories>stat.ML math.OC physics.data-an stat.AP</categories><comments>21 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have measured the dissimilarities among several printed characters of a
single page in the Gutenberg 42-line bible and we prove statistically the
existence of several different matrices from which the metal types where
constructed. This is in contrast with the prevailing theory, which states that
only one matrix per character was used in the printing process of Gutenberg's
greatest work.
  The main mathematical tool for this purpose is cluster analysis, combined
with a statistical test for outliers. We carry out the research with two
letters, i and a. In the first case, an exact clustering method is employed; in
the second, with more specimens to be classified, we resort to an approximate
agglomerative clustering method.
  The results show that the letters form clusters according to their shape,
with significant shape differences among clusters, and allow to conclude, with
a very small probability of error, that indeed the metal types used to print
them were cast from several different matrices.
  Mathematics Subject Classification: 62H30
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0152</identifier>
 <datestamp>2011-07-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0152</id><created>2010-01-31</created><updated>2011-07-05</updated><authors><author><keyname>Espinasse</keyname><forenames>Thibault</forenames><affiliation>IMT</affiliation></author><author><keyname>Gamboa</keyname><forenames>Fabrice</forenames><affiliation>IMT</affiliation></author><author><keyname>Loubes</keyname><forenames>Jean-Michel</forenames><affiliation>IMT</affiliation></author></authors><title>Estimation error for blind Gaussian time series prediction</title><categories>stat.ME math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tackle the issue of the blind prediction of a Gaussian time series. For
this, we construct a projection operator build by plugging an empirical
covariance estimation into a Schur complement decomposition of the projector.
This operator is then used to compute the predictor. Rates of convergence of
the estimates are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0175</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0175</id><created>2010-01-31</created><updated>2013-07-23</updated><authors><author><keyname>Cheridito</keyname><forenames>Patrick</forenames></author><author><keyname>Stadje</keyname><forenames>Mitja</forenames></author></authors><title>BS\Delta Es and BSDEs with non-Lipschitz drivers: Comparison,
  convergence and robustness</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/12-BEJ445 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ445</report-no><journal-ref>Bernoulli 2013, Vol. 19, No. 3, 1047-1085</journal-ref><doi>10.3150/12-BEJ445</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide existence results and comparison principles for solutions of
backward stochastic difference equations (BS$\Delta$Es) and then prove
convergence of these to solutions of backward stochastic differential equations
(BSDEs) when the mesh size of the time-discretizaton goes to zero. The
BS$\Delta$Es and BSDEs are governed by drivers $f^N(t,\omega,y,z)$ and
$f(t,\omega,y,z),$ respectively. The new feature of this paper is that they may
be non-Lipschitz in z. For the convergence results it is assumed that the
BS$\Delta$Es are based on d-dimensional random walks $W^N$ approximating the
d-dimensional Brownian motion W underlying the BSDE and that $f^N$ converges to
f. Conditions are given under which for any bounded terminal condition $\xi$
for the BSDE, there exist bounded terminal conditions $\xi^N$ for the sequence
of BS$\Delta$Es converging to $\xi$, such that the corresponding solutions
converge to the solution of the limiting BSDE. An important special case is
when $f^N$ and f are convex in z. We show that in this situation, the solutions
of the BS$\Delta$Es converge to the solution of the BSDE for every uniformly
bounded sequence $\xi^N$ converging to $\xi$. As a consequence, one obtains
that the BSDE is robust in the sense that if $(W^N,\xi^N)$ is close to
$(W,\xi)$ in distribution, then the solution of the Nth BS$\Delta$E is close to
the solution of the BSDE in distribution too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0224</identifier>
 <datestamp>2010-02-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0224</id><created>2010-02-01</created><authors><author><keyname>Del Moral</keyname><forenames>P.</forenames></author><author><keyname>Patras</keyname><forenames>F.</forenames></author><author><keyname>Rubenthaler</keyname><forenames>S.</forenames></author></authors><title>Convergence of U-statistics for interacting particle systems</title><categories>math.PR math.ST stat.TH</categories><msc-class>82C22, 62E20, 60F05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The convergence of U-statistics has been intensively studied for estimators
based on families of i.i.d. random variables and variants of them. In most
cases, the independence assumption is crucial [Lee90, de99]. When dealing with
Feynman-Kac and other interacting particle systems of Monte Carlo type, one
faces a new type of problem. Namely, in a sample of N particles obtained
through the corresponding algorithms, the distributions of the particles are
correlated -although any finite number of them is asymptotically independent
with respect to the total number N of particles. In the present article,
exploiting the fine asymptotics of particle systems, we prove convergence
theorems for U-statistics in this framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0299</identifier>
 <datestamp>2010-04-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0299</id><created>2010-02-01</created><updated>2010-04-13</updated><authors><author><keyname>Barrett</keyname><forenames>Adam B.</forenames></author><author><keyname>Barnett</keyname><forenames>Lionel</forenames></author><author><keyname>Seth</keyname><forenames>Anil K.</forenames></author></authors><title>Multivariate Granger Causality and Generalized Variance</title><categories>q-bio.NC physics.data-an q-bio.QM stat.ME</categories><comments>added 1 reference, minor change to discussion, typos corrected; 28
  pages, 3 figures, 1 table, LaTeX</comments><journal-ref>Physical Rev E, Vol 81, 041907 (2010)</journal-ref><doi>10.1103/PhysRevE.81.041907</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Granger causality analysis is a popular method for inference on directed
interactions in complex systems of many variables. A shortcoming of the
standard framework for Granger causality is that it only allows for examination
of interactions between single (univariate) variables within a system, perhaps
conditioned on other variables. However, interactions do not necessarily take
place between single variables, but may occur among groups, or "ensembles", of
variables. In this study we establish a principled framework for Granger
causality in the context of causal interactions among two or more multivariate
sets of variables. Building on Geweke's seminal 1982 work, we offer new
justifications for one particular form of multivariate Granger causality based
on the generalized variances of residual errors. Taken together, our results
support a comprehensive and theoretically consistent extension of Granger
causality to the multivariate case. Treated individually, they highlight
several specific advantages of the generalized variance measure, which we
illustrate using applications in neuroscience as an example. We further show
how the measure can be used to define "partial" Granger causality in the
multivariate context and we also motivate reformulations of "causal density"
and "Granger autonomy". Our results are directly applicable to experimental
data and promise to reveal new types of functional relations in complex
systems, neural and otherwise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0425</identifier>
 <datestamp>2010-02-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0425</id><created>2010-02-02</created><authors><author><keyname>Commenges</keyname><forenames>D.</forenames></author><author><keyname>Jolly</keyname><forenames>D.</forenames></author><author><keyname>Putter</keyname><forenames>H.</forenames></author><author><keyname>Thiebaut</keyname><forenames>R.</forenames></author></authors><title>Inference in HIV dynamics models via hierarchical likelihood</title><categories>math.ST stat.TH</categories><comments>27 pages, 2 figure</comments><msc-class>62H10; 65C60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  HIV dynamical models are often based on non-linear systems of ordinary
differential equations (ODE), which do not have analytical solution.
Introducing random effects in such models leads to very challenging non-linear
mixed-effects models. To avoid the numerical computation of multiple integrals
involved in the likelihood, we propose a hierarchical likelihood (h-likelihood)
approach, treated in the spirit of a penalized likelihood. We give the
asymptotic distribution of the maximum h-likelihood estimators (MHLE) for fixed
effects, a result that may be relevant in a more general setting. The MHLE are
slightly biased but the bias can be made negligible by using a parametric
bootstrap procedure. We propose an efficient algorithm for maximizing the
h-likelihood. A simulation study, based on a classical HIV dynamical model,
confirms the good properties of the MHLE. We apply it to the analysis of a
clinical trial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0535</identifier>
 <datestamp>2010-02-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0535</id><created>2010-02-02</created><authors><author><keyname>Cerquetti</keyname><forenames>Annalisa</forenames></author></authors><title>On some Bayesian nonparametric estimators for species richness under
  two-parameter Poisson-Dirichlet priors</title><categories>math.PR math.ST stat.TH</categories><comments>11 pages</comments><msc-class>60G58; 60G09</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an alternative approach to the Bayesian nonparametric analysis of
conditional species richness under two-parameter Poisson Dirichlet priors. We
rely on a known characterization by deletion of classes property and on results
for Beta-Binomial distributions. Besides leading to simplified and much more
direct proofs, our proposal provides a new scale mixture representation of the
conditional asymptotic law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0567</identifier>
 <datestamp>2010-02-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0567</id><created>2010-02-02</created><updated>2010-02-03</updated><authors><author><keyname>Voutier</keyname><forenames>Paul M.</forenames></author></authors><title>A New Approximation to the Normal Distribution Quantile Function</title><categories>stat.CO math.NA math.ST q-fin.CP stat.TH</categories><comments>added contact details</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a new approximation to the normal distribution quantile function.
It has a similar form to the approximation of Beasley and Springer [3],
providing a maximum absolute error of less than $2.5 \cdot 10^{-5}$. This is
less accurate than [3], but still sufficient for many applications. However it
is faster than [3]. This is its primary benefit, which can be crucial to many
applications, including in financial markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0616</identifier>
 <datestamp>2010-02-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0616</id><created>2010-02-03</created><authors><author><keyname>Huckemann</keyname><forenames>Stephan</forenames></author></authors><title>Dynamic shape analysis and comparison of leaf growth</title><categories>stat.ME math.MG</categories><comments>19 pages, 16 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the statistical analysis of shape a goal beyond the analysis of static
shapes lies in the quantification of `same' deformation of different shapes.
Typically, shape spaces are modelled as Riemannian manifolds on which parallel
transport along geodesics naturally qualifies as a measure for the `similarity'
of deformation. Since these spaces are usually defined as combinations of
Riemannian immersions and submersions, only for few well featured spaces such
as spheres or complex projective spaces (which are Kendall's spaces for 2D
shapes), parallel transport along geodesics can be computed explicitly. In this
contribution a general numerical method to compute parallel transport along
geodesics when no explicit formula is available is provided. This method is
applied to the shape spaces of closed 2D contours based on angular direction
and to Kendall's spaces of shapes of arbitrary dimension. In application to the
temporal evolution of leaf shape over a growing period, one leaf's shape-growth
dynamics can be applied to another leaf. For a specific poplar tree
investigated it is found that leaves of initially and terminally different
shape evolve rather parallel, i.e. with comparable dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0651</identifier>
 <datestamp>2010-11-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0651</id><created>2010-02-03</created><updated>2010-11-12</updated><authors><author><keyname>Gill</keyname><forenames>Richard D.</forenames></author></authors><title>The Monty Hall Problem is not a Probability Puzzle (it's a challenge in
  mathematical modelling)</title><categories>math.HO stat.AP</categories><comments>Mathematical Humour, Mathematical Education. Submitted to Statistica
  Neerlandica</comments><msc-class>97K50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose you're on a game show, and you're given the choice of three doors:
Behind one door is a car; behind the others, goats. You pick a door, say No. 1,
and the host, who knows what's behind the doors, opens another door, say No. 3,
which has a goat. He then says to you, ``Do you want to pick door No. 2?'' Is
it to your advantage to switch your choice? The answer is ``yes'' but the
literature offers many reasons why this is the correct answer. The present
paper argues that the most common reasoning found in introductory statistics
texts, depending on making a number of ``obvious'' or ``natural'' assumptions
and then computing a conditional probability, is a classical example of
solution driven science. The best reason to switch is to be found in von
Neumann's minimax theorem from game theory, rather than in Bayes' theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0675</identifier>
 <datestamp>2013-02-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0675</id><created>2010-02-03</created><updated>2013-01-31</updated><authors><author><keyname>Aurzada</keyname><forenames>Frank</forenames></author><author><keyname>Doering</keyname><forenames>Leif</forenames></author><author><keyname>Savov</keyname><forenames>Mladen</forenames></author></authors><title>Small time Chung-type LIL for L\'{e}vy processes</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/11-BEJ395 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ395</report-no><journal-ref>Bernoulli 2013, Vol. 19, No. 1, 115-136</journal-ref><doi>10.3150/11-BEJ395</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove Chung-type laws of the iterated logarithm for general L\'{e}vy
processes at zero. In particular, we provide tools to translate small deviation
estimates directly into laws of the iterated logarithm. This reveals laws of
the iterated logarithm for L\'{e}vy processes at small times in many concrete
examples. In some cases, exotic norming functions are derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0729</identifier>
 <datestamp>2010-02-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0729</id><created>2010-02-03</created><authors><author><keyname>Gonzalez-Lopez</keyname><forenames>Jesus E. Garcia Veronica A.</forenames></author></authors><title>Minimal Markov Models</title><categories>math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we introduce a new and richer class of finite order Markov chain
models and address the following model selection problem: find the Markov model
with the minimal set of parameters (minimal Markov model) which is necessary to
represent a source as a Markov chain of finite order. Let us call $M$ the order
of the chain and $A$ the finite alphabet, to determine the minimal Markov
model, we define an equivalence relation on the state space $A^{M}$, such that
all the sequences of size $M$ with the same transition probabilities are put in
the same category. In this way we have one set of $(|A|-1)$ transition
probabilities for each category, obtaining a model with a minimal number of
parameters. We show that the model can be selected consistently using the
Bayesian information criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0730</identifier>
 <datestamp>2011-11-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0730</id><created>2010-02-03</created><updated>2011-11-11</updated><authors><author><keyname>Broniatowski</keyname><forenames>Michel</forenames><affiliation>LSTA</affiliation></author><author><keyname>Keziou</keyname><forenames>Amor</forenames><affiliation>LM-Reims, LSTA</affiliation></author></authors><title>Divergences and Duality for Estimation and Test under Moment Condition
  Models</title><categories>math.ST stat.TH</categories><comments>37 pages, 4 figures</comments><proxy>ccsd</proxy><msc-class>62G05, 62G10, 62G15, 62G20, 62G35</msc-class><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce estimation and test procedures through divergence minimiza- tion
for models satisfying linear constraints with unknown parameter. These
procedures extend the empirical likelihood (EL) method and share common
features with generalized empirical likelihood approach. We treat the problems
of existence and characterization of the divergence projections of probability
distributions on sets of signed finite measures. We give a precise
characterization of duality, for the proposed class of estimates and test
statistics, which is used to derive their limiting distributions (including the
EL estimate and the EL ratio statistic) both under the null hypotheses and
under alterna- tives or misspeci?cation. An approximation to the power function
is deduced as well as the sample size which ensures a desired power for a given
alternative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0747</identifier>
 <datestamp>2010-04-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0747</id><created>2010-02-03</created><updated>2010-04-18</updated><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Tamuz</keyname><forenames>Omer</forenames></author></authors><title>Efficient Bayesian Learning in Social Networks with Gaussian Estimators</title><categories>stat.AP cs.LG stat.ML</categories><comments>10 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Bayesian model of iterative learning on social networks that is
computationally tractable; the agents of this model are fully rational, and
their calculations can be performed with modest computational resources for
large networks. Furthermore, learning is efficient, in the sense that the
process results in an information-theoretically optimal belief. This result
extends Condorcet's Jury Theorem to general social networks, preserving
rationality, computational feasibility and efficient learning. The model
consists of a group of agents who belong to a social network, so that a pair of
agents can observe each other's actions only if they are neighbors. We assume
that the network is connected and that the agents have full knowledge of the
structure of the network, so that they know the members of the network and
their social connections. The agents try to estimate some state of the world S
(say, the price of oil a year from today). Each agent has a private
measurement: an independently acquired piece of information regarding S. This
is modeled, for agent v, by a number S_v picked from a Gaussian distribution
with mean S and standard deviation one. Accordingly, agent v's prior belief
regarding S is a normal distribution with mean S_v and standard deviation one.
The agents start acting iteratively. At each iteration, each agent takes the
optimal action given its current belief. This action reveals its mean estimate
of S to its neighbors. Then, observing its neighbors' actions, each agent
updates its belief, using Bayes' Law. We show that this process is efficient:
all the agents converge to the belief that they would have, had they access to
all the private measurements. Additionally, and in contrast to other iterative
Bayesian models on networks, it is computationally efficient, so that each
agent's calculation can be easily carried out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0757</identifier>
 <datestamp>2010-02-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0757</id><created>2010-02-03</created><authors><author><keyname>Grnwald</keyname><forenames>Peter</forenames></author><author><keyname>Kotowski</keyname><forenames>Wojciech</forenames></author></authors><title>Prequential Plug-In Codes that Achieve Optimal Redundancy Rates even if
  the Model is Wrong</title><categories>cs.IT cs.LG math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse the prequential plug-in codes relative to one-parameter
exponential families M. We show that if data are sampled i.i.d. from some
distribution outside M, then the redundancy of any plug-in prequential code
grows at rate larger than 1/2 ln(n) in the worst case. This means that plug-in
codes, such as the Rissanen-Dawid ML code, may behave inferior to other
important universal codes such as the 2-part MDL, Shtarkov and Bayes codes, for
which the redundancy is always 1/2 ln(n) + O(1). However, we also show that a
slight modification of the ML plug-in code, "almost" in the model, does achieve
the optimal redundancy even if the the true distribution is outside M.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0795</identifier>
 <datestamp>2011-05-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0795</id><created>2010-02-03</created><updated>2011-05-12</updated><authors><author><keyname>Huckemann</keyname><forenames>Stephan</forenames></author></authors><title>On the meaning of mean shape</title><categories>stat.ME math.MG</categories><comments>32 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various concepts of mean shape previously unrelated in the literature are
brought into relation. In particular for non-manifolds such as Kendall's 3D
shape space, this paper answers the question, for which means one may apply a
two-sample test. The answer is positive if intrinsic or Ziezold means are used.
The underlying general result of manifold stability of a mean on a shape space,
the quotient due to an isometric action of a compact Lie group on a Riemannian
manifold, blends the Slice Theorem from differential geometry with the
statistics of shape. For 3D Procrustes means, however, a counterexample is
given. To further elucidate on subtleties of means, for spheres and Kendall's
shape spaces, a first order relationship between intrinsic,
residual/Procrustean and extrinsic/Ziezold means is derived stating that for
high concentration the latter approximately divides the (generalized) geodesic
segment between the former two by the ratio $1:3$. This fact, consequences of
coordinate choices for the power of tests and other details, e.g. that
extrinsic Schoenberg means may increase dimension are discussed and illustrated
by simulations and exemplary datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0832</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0832</id><created>2010-02-03</created><authors><author><keyname>Pontil</keyname><forenames>Andreas Maurer Massimiliano</forenames></author></authors><title>K-Dimensional Coding Schemes in Hilbert Spaces</title><categories>stat.ML math.ST stat.TH</categories><comments>17 pages</comments><journal-ref>IEEE Transactions on Information Theory, 56(11): 5839-5846, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a general coding method where data in a Hilbert space are
represented by finite dimensional coding vectors. The method is based on
empirical risk minimization within a certain class of linear operators, which
map the set of coding vectors to the Hilbert space. Two results bounding the
expected reconstruction error of the method are derived, which highlight the
role played by the codebook and the class of linear operators. The results are
specialized to some cases of practical importance, including K-means
clustering, nonnegative matrix factorization and other sparse coding methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.0857</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.0857</id><created>2010-02-03</created><authors><author><keyname>Coeurjolly</keyname><forenames>Jean-Franois</forenames><affiliation>GIPSA-lab, LJK</affiliation></author><author><keyname>Lavancier</keyname><forenames>Frdric</forenames><affiliation>LMJL</affiliation></author></authors><title>Residuals and goodness-of-fit tests for stationary marked Gibbs point
  processes</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00453102</proxy><doi>10.1111/j.1467-9868.2012.01043.x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The inspection of residuals is a fundamental step to investigate the quality
of adjustment of a parametric model to data. For spatial point processes, the
concept of residuals has been recently proposed by Baddeley et al. (2005) as an
empirical counterpart of the {\it Campbell equilibrium} equation for marked
Gibbs point processes. The present paper focuses on stationary marked Gibbs
point processes and deals with asymptotic properties of residuals for such
processes. In particular, the consistency and the asymptotic normality are
obtained for a wide class of residuals including the classical ones (raw
residuals, inverse residuals, Pearson residuals). Based on these asymptotic
results, we define goodness-of-fit tests with Type-I error theoretically
controlled. One of these tests constitutes an extension of the quadrat counting
test widely used to test the null hypothesis of a homogeneous Poisson point
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.1059</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.1059</id><created>2010-02-04</created><updated>2012-09-04</updated><authors><author><keyname>Eches</keyname><forenames>Olivier</forenames></author><author><keyname>Dobigeon</keyname><forenames>Nicolas</forenames></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames></author></authors><title>Enhancing hyperspectral image unmixing with spatial correlations</title><categories>stat.ME physics.data-an stat.AP</categories><comments>Manuscript accepted for publication in IEEE Trans. Geoscience and
  Remote Sensing</comments><journal-ref>IEEE Trans. Geoscience and Remote Sensing, vol. 49, no. 11, pp.
  4239-4247, Nov. 2011</journal-ref><doi>10.1109/TGRS.2011.2140119</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new algorithm for hyperspectral image unmixing. Most
of the unmixing algorithms proposed in the literature do not take into account
the possible spatial correlations between the pixels. In this work, a Bayesian
model is introduced to exploit these correlations. The image to be unmixed is
assumed to be partitioned into regions (or classes) where the statistical
properties of the abundance coefficients are homogeneous. A Markov random field
is then proposed to model the spatial dependency of the pixels within any
class. Conditionally upon a given class, each pixel is modeled by using the
classical linear mixing model with additive white Gaussian noise. This strategy
is investigated the well known linear mixing model. For this model, the
posterior distributions of the unknown parameters and hyperparameters allow
ones to infer the parameters of interest. These parameters include the
abundances for each pixel, the means and variances of the abundances for each
class, as well as a classification map indicating the classes of all pixels in
the image. To overcome the complexity of the posterior distribution of
interest, we consider Markov chain Monte Carlo methods that generate samples
distributed according to the posterior of interest. The generated samples are
then used for parameter and hyperparameter estimation. The accuracy of the
proposed algorithms is illustrated on synthetic and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.1111</identifier>
 <datestamp>2011-06-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.1111</id><created>2010-02-04</created><updated>2010-07-06</updated><authors><author><keyname>Demortier</keyname><forenames>Luc</forenames></author><author><keyname>Jain</keyname><forenames>Supriya</forenames></author><author><keyname>Prosper</keyname><forenames>Harrison B.</forenames></author></authors><title>Reference priors for high energy physics</title><categories>stat.AP hep-ex hep-ph physics.data-an</categories><journal-ref>Phys.Rev.D82:034002,2010</journal-ref><doi>10.1103/PhysRevD.82.034002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian inferences in high energy physics often use uniform prior
distributions for parameters about which little or no information is available
before data are collected. The resulting posterior distributions are therefore
sensitive to the choice of parametrization for the problem and may even be
improper if this choice is not carefully considered. Here we describe an
extensively tested methodology, known as reference analysis, which allows one
to construct parametrization-invariant priors that embody the notion of minimal
informativeness in a mathematically well-defined sense. We apply this
methodology to general cross section measurements and show that it yields
sensible results. A recent measurement of the single top quark cross section
illustrates the relevant techniques in a realistic situation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.1142</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.1142</id><created>2010-02-05</created><updated>2014-03-08</updated><authors><author><keyname>Bontemps</keyname><forenames>Dominique</forenames><affiliation>IMT</affiliation></author><author><keyname>Toussile</keyname><forenames>Wilson</forenames></author></authors><title>Clustering and variable selection for categorical multivariate data</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><journal-ref>Electronic Journal of Statistics 7 (2013) 2344-2371</journal-ref><doi>10.1214/13-EJS844</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article investigates unsupervised classification techniques for
categorical multivariate data. The study employs multivariate multinomial
mixture modeling, which is a type of model particularly applicable to
multilocus genotypic data. A model selection procedure is used to
simultaneously select the number of components and the relevant variables. A
non-asymptotic oracle inequality is obtained, leading to the proposal of a new
penalized maximum likelihood criterion. The selected model proves to be
asymptotically consistent under weak assumptions on the true probability
underlying the observations. The main theoretical result obtained in this study
suggests a penalty function defined to within a multiplicative parameter. In
practice, the data-driven calibration of the penalty function is made possible
by slope heuristics. Based on simulated data, this procedure is found to
improve the performance of the selection procedure with respect to classical
criteria such as BIC and AIC. The new criterion provides an answer to the
question "Which criterion for which sample size?" Examples of real dataset
applications are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.1247</identifier>
 <datestamp>2010-02-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.1247</id><created>2010-02-05</created><authors><author><keyname>Wakin</keyname><forenames>Michael B.</forenames></author></authors><title>Manifold-Based Signal Recovery and Parameter Estimation from Compressive
  Measurements</title><categories>stat.ML math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A field known as Compressive Sensing (CS) has recently emerged to help
address the growing challenges of capturing and processing high-dimensional
signals and data sets. CS exploits the surprising fact that the information
contained in a sparse signal can be preserved in a small number of compressive
(or random) linear measurements of that signal. Strong theoretical guarantees
have been established on the accuracy to which sparse or near-sparse signals
can be recovered from noisy compressive measurements. In this paper, we address
similar questions in the context of a different modeling framework. Instead of
sparse models, we focus on the broad class of manifold models, which can arise
in both parametric and non-parametric signal families. Building upon recent
results concerning the stable embeddings of manifolds within the measurement
space, we establish both deterministic and probabilistic instance-optimal
bounds in $\ell_2$ for manifold-based signal recovery and parameter estimation
from noisy compressive measurements. In line with analogous results for
sparsity-based CS, we conclude that much stronger bounds are possible in the
probabilistic setting. Our work supports the growing empirical evidence that
manifold-based models can be used with high accuracy in compressive signal
processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.1280</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.1280</id><created>2010-02-05</created><updated>2012-02-15</updated><authors><author><keyname>Gassiat</keyname><forenames>Elisabeth</forenames><affiliation>LM-Orsay</affiliation></author><author><keyname>Van Handel</keyname><forenames>Ramon</forenames></author></authors><title>Consistent order estimation and minimal penalties</title><categories>math.ST stat.TH</categories><comments>26 pages</comments><proxy>ccsd</proxy><journal-ref>IEEE Trans. Inform. Theory 59, 1115-1128 (2013)</journal-ref><doi>10.1109/TIT.2012.2221122</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider an i.i.d. sequence of random variables whose distribution f* lies in
one of a nested family of models M_q, q&gt;=1. The smallest index q* such that
M_{q*} contains f* is called the model order. We establish strong consistency
of the penalized likelihood order estimator in a general setting with penalties
of order \eta(q) log log n, where \eta(q) is a dimensional quantity. Moreover,
such penalties are shown to be minimal. In contrast to previous work, an a
priori upper bound on the model order is not assumed. The results rely on a
sharp characterization of the pathwise fluctuations of the generalized
likelihood ratio statistic under entropy assumptions on the model classes. Our
results are applied to the geometrically complex problem of location mixture
order estimation, which is widely used but poorly understood.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.1312</identifier>
 <datestamp>2010-02-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.1312</id><created>2010-02-05</created><authors><author><keyname>De Gregorio</keyname><forenames>A.</forenames></author><author><keyname>Iacus</keyname><forenames>S. M.</forenames></author></authors><title>Adaptive LASSO-type estimation for ergodic diffusion processes</title><categories>math.ST math.PR stat.AP stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The LASSO is a widely used statistical methodology for simultaneous
estimation and variable selection. In the last years, many authors analyzed
this technique from a theoretical and applied point of view. We introduce and
study the adaptive LASSO problem for discretely observed ergodic diffusion
processes. We prove oracle properties also deriving the asymptotic distribution
of the LASSO estimator. Our theoretical framework is based on the random field
approach and it applied to more general families of regular statistical
experiments in the sense of Ibragimov-Hasminskii (1981). Furthermore, we
perform a simulation and real data analysis to provide some evidence on the
applicability of this method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.1493</identifier>
 <datestamp>2010-02-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.1493</id><created>2010-02-07</created><authors><author><keyname>Harremos</keyname><forenames>Peter</forenames></author><author><keyname>Vajda</keyname><forenames>Igor</forenames></author></authors><title>On Bahadur Efficiency of Power Divergence Statistics</title><categories>math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is proved that the information divergence statistic is infinitely more
Bahadur efficient than the power divergence statistics of the orders $\alpha
&gt;1$ as long as the sequence of alternatives is contiguous with respect to the
sequence of null-hypotheses and the the number of observations per bin
increases to infinity is not very slow. This improves the former result in
Harremo\"es and Vajda (2008) where the the sequence of null-hypotheses was
assumed to be uniform and the restrictions on on the numbers of observations
per bin were sharper. Moreover, this paper evaluates also the Bahadur
efficiency of the power divergence statistics of the remaining positive orders
$0&lt; \alpha \leq 1.$ The statistics of these orders are mutually
Bahadur-comparable and all of them are more Bahadur efficient than the
statistics of the orders $\alpha &gt; 1.$ A detailed discussion of the technical
definitions and conditions is given, some unclear points are resolved, and the
results are illustrated by examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.1537</identifier>
 <datestamp>2010-02-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.1537</id><created>2010-02-08</created><authors><author><keyname>Galtchouk</keyname><forenames>Leonid</forenames><affiliation>IRMA</affiliation></author><author><keyname>Pergamenchtchikov</keyname><forenames>Serguei</forenames><affiliation>LMRS</affiliation></author></authors><title>Adaptive asymptotically efficient estimation in heteroscedastic
  nonparametric regression</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00454081</proxy><msc-class>62G08</msc-class><journal-ref>Journal of the Korean Statistical Society 38, 4 (2008) 305-322</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper deals with asymptotic properties of the adaptive procedure proposed
in the author paper, 2007, for estimating an unknown nonparametric regression.
%\cite{GaPe1}. We prove that this procedure is asymptotically efficient for a
quadratic risk, i.e. the asymptotic quadratic risk for this procedure coincides
with the Pinsker constant which gives a sharp lower bound for the quadratic
risk over all possible estimates
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.1538</identifier>
 <datestamp>2010-02-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.1538</id><created>2010-02-08</created><authors><author><keyname>Galtchouk</keyname><forenames>Leonid</forenames><affiliation>IRMA</affiliation></author><author><keyname>Pergamenchtchikov</keyname><forenames>Serguei</forenames><affiliation>LMRS</affiliation></author></authors><title>Sharp non-asymptotic oracle inequalities for nonparametric
  heteroscedastic regression models</title><categories>math.ST stat.TH</categories><proxy>ccsd hal-00454079</proxy><msc-class>62G08</msc-class><journal-ref>Journal of Nonparametric Statistics 21, 1 (2009) 1-16</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An adaptive nonparametric estimation procedure is constructed for
heteroscedastic regression when the noise variance depends on the unknown
regression. A non-asymptotic upper bound for a quadratic risk (oracle
inequality) is obtained
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.1583</identifier>
 <datestamp>2010-02-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.1583</id><created>2010-02-08</created><updated>2010-02-11</updated><authors><author><keyname>Zhou</keyname><forenames>Shuheng</forenames></author></authors><title>Thresholded Lasso for high dimensional variable selection and
  statistical estimation</title><categories>math.ST stat.TH</categories><comments>37 Pages, 4 Figures; Parts of the paper have appeared in Proceedings
  of Advances in Neural Information Processing Systems 22, December, 2009 (NIPS
  2009)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $n$ noisy samples with $p$ dimensions, where $n \ll p$, we show that
the multi-step thresholding procedure based on the Lasso -- we call it the {\it
Thresholded Lasso}, can accurately estimate a sparse vector $\beta \in \R^p$ in
a linear model $Y = X \beta + \epsilon$, where $X_{n \times p}$ is a design
matrix normalized to have column $\ell_2$ norm $\sqrt{n}$, and $\epsilon \sim
N(0, \sigma^2 I_n)$. We show that under the restricted eigenvalue (RE)
condition (Bickel-Ritov-Tsybakov 09), it is possible to achieve the $\ell_2$
loss within a logarithmic factor of the ideal mean square error one would
achieve with an {\em oracle} while selecting a sufficiently sparse model --
hence achieving {\it sparse oracle inequalities}; the oracle would supply
perfect information about which coordinates are non-zero and which are above
the noise level. In some sense, the Thresholded Lasso recovers the choices that
would have been made by the $\ell_0$ penalized least squares estimators, in
that it selects a sufficiently sparse model without sacrificing the accuracy in
estimating $\beta$ and in predicting $X \beta$. We also show for the
Gauss-Dantzig selector (Cand\`{e}s-Tao 07), if $X$ obeys a uniform uncertainty
principle and if the true parameter is sufficiently sparse, one will achieve
the sparse oracle inequalities as above, while allowing at most $s_0$
irrelevant variables in the model in the worst case, where $s_0 \leq s$ is the
smallest integer such that for $\lambda = \sqrt{2 \log p/n}$, $\sum_{i=1}^p
\min(\beta_i^2, \lambda^2 \sigma^2) \leq s_0 \lambda^2 \sigma^2$. Our
simulation results on the Thresholded Lasso match our theoretical analysis
excellently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.1940</identifier>
 <datestamp>2010-02-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.1940</id><created>2010-02-09</created><authors><author><keyname>Kugiumtzis</keyname><forenames>Dimitris</forenames></author><author><keyname>Tsimpiris</keyname><forenames>Alkiviadis</forenames></author></authors><title>Measures of Analysis of Time Series (MATS): A MATLAB Toolkit for
  Computation of Multiple Measures on Time Series Data Bases</title><categories>stat.CO</categories><comments>25 pages, 9 figures, two tables, the software can be downloaded at
  http://eeganalysis.web.auth.gr/indexen.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications, such as physiology and finance, large time series data
bases are to be analyzed requiring the computation of linear, nonlinear and
other measures. Such measures have been developed and implemented in commercial
and freeware softwares rather selectively and independently. The Measures of
Analysis of Time Series ({\tt MATS}) {\tt MATLAB} toolkit is designed to handle
an arbitrary large set of scalar time series and compute a large variety of
measures on them, allowing for the specification of varying measure parameters
as well. The variety of options with added facilities for visualization of the
results support different settings of time series analysis, such as the
detection of dynamics changes in long data records, resampling (surrogate or
bootstrap) tests for independence and linearity with various test statistics,
and discrimination power of different measures and for different combinations
of their parameters. The basic features of {\tt MATS} are presented and the
implemented measures are briefly described. The usefulness of {\tt MATS} is
illustrated on some empirical examples along with screenshots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.1994</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.1994</id><created>2010-02-09</created><updated>2012-04-19</updated><authors><author><keyname>Lerman</keyname><forenames>Gilad</forenames></author><author><keyname>Zhang</keyname><forenames>Teng</forenames></author></authors><title>Probabilistic Recovery of Multiple Subspaces in Point Clouds by
  Geometric lp Minimization</title><categories>stat.ML</categories><comments>This paper was split into two different papers: 1.
  http://arxiv.org/abs/1012.4116 2. http://arxiv.org/abs/1104.3770</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We assume data independently sampled from a mixture distribution on the unit
ball of the D-dimensional Euclidean space with K+1 components: the first
component is a uniform distribution on that ball representing outliers and the
other K components are uniform distributions along K d-dimensional linear
subspaces restricted to that ball. We study both the simultaneous recovery of
all K underlying subspaces and the recovery of the best l0 subspace (i.e., with
largest number of points) by minimizing the lp-averaged distances of data
points from d-dimensional subspaces of the D-dimensional space. Unlike other lp
minimization problems, this minimization is non-convex for all p&gt;0 and thus
requires different methods for its analysis. We show that if 0&lt;p &lt;= 1, then
both all underlying subspaces and the best l0 subspace can be precisely
recovered by lp minimization with overwhelming probability. This result extends
to additive homoscedastic uniform noise around the subspaces (i.e., uniform
distribution in a strip around them) and near recovery with an error
proportional to the noise level. On the other hand, if K&gt;1 and p&gt;1, then we
show that both all underlying subspaces and the best l0 subspace cannot be
recovered and even nearly recovered. Further relaxations are also discussed. We
use the results of this paper for partially justifying recent effective
algorithms for modeling data by mixtures of multiple subspaces as well as for
discussing the effect of using variants of lp minimizations in RANSAC-type
strategies for single subspace recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.2017</identifier>
 <datestamp>2010-02-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.2017</id><created>2010-02-09</created><authors><author><keyname>Xu</keyname><forenames>Weijun</forenames></author><author><keyname>Yang</keyname><forenames>Li</forenames></author><author><keyname>Kohn</keyname><forenames>Robert</forenames></author></authors><title>Computationally Efficient Estimation of Factor Multivariate Stochastic
  Volatility Models</title><categories>stat.CO stat.ME</categories><comments>32 pages, 3 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An MCMC simulation method based on a two stage delayed rejection
Metropolis-Hastings algorithm is proposed to estimate a factor multivariate
stochastic volatility model. The first stage uses kstep iteration towards the
mode, with k small, and the second stage uses an adaptive random walk proposal
density. The marginal likelihood approach of Chib (1995) is used to choose the
number of factors, with the posterior density ordinates approximated by
Gaussian copula. Simulation and real data applications suggest that the
proposed simulation method is computationally much more efficient than the
approach of Chib. Nardari and Shephard (2006}. This increase in computational
efficiency is particularly important in calculating marginal likelihoods
because it is necessary to carry out the simulation a number of times to
estimate the posterior ordinates for a given marginal likelihood. In addition
to the MCMC method, the paper also proposes a fast approximate EM method to
estimate the factor multivariate stochastic volatility model. The estimates
from the approximate EM method are of interest in their own right, but are
especially useful as initial inputs to MCMC methods, making them more efficient
computationally. The methodology is illustrated using simulated and real
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.2080</identifier>
 <datestamp>2010-02-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.2080</id><created>2010-02-10</created><authors><author><keyname>Robert</keyname><forenames>Christian P.</forenames></author><author><keyname>Marin</keyname><forenames>Jean-Michel</forenames></author><author><keyname>Rousseau</keyname><forenames>Judith</forenames></author></authors><title>Bayesian Inference</title><categories>stat.ME stat.AP</categories><comments>This is a 20 page chapter for the upcoming Handbook of Statistical
  Systems Biology (D. Balding, M. Stumpf, M. Girolami, eds.)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter provides a overview of Bayesian inference, mostly emphasising
that it is a universal method for summarising uncertainty and making estimates
and predictions using probability statements conditional on observed data and
an assumed model (Gelman 2008). The Bayesian perspective is thus applicable to
all aspects of statistical inference, while being open to the incorporation of
information items resulting from earlier experiments and from expert opinions.
We provide here the basic elements of Bayesian analysis when considered for
standard models, refering to Marin and Robert (2007) and to Robert (2007) for
book-length entries.1 In the following, we refrain from embarking upon
philosophical discussions about the nature of knowledge (see, e.g., Robert
2007, Chapter 10), opting instead for a mathematically sound presentation of an
eminently practical statistical methodology. We indeed believe that the most
convincing arguments for adopting a Bayesian version of data analyses are in
the versatility of this tool and in the large range of existing applications,
rather than in those polemical arguments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.2168</identifier>
 <datestamp>2011-12-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.2168</id><created>2010-02-10</created><updated>2011-11-29</updated><authors><author><keyname>Kasza</keyname><forenames>Jessica</forenames></author><author><keyname>Glonek</keyname><forenames>Gary</forenames></author><author><keyname>Solomon</keyname><forenames>Patty</forenames></author></authors><title>Estimating Bayesian networks for high-dimensional data with complex mean
  structure and random effects</title><categories>stat.ME</categories><comments>24 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The estimation of Bayesian networks given high-dimensional data, in
particular gene expression data, has been the focus of much recent research.
Whilst there are several methods available for the estimation of such networks,
these typically assume that the data consist of independent and identically
distributed samples. However, it is often the case that the available data have
a more complex mean structure plus additional components of variance, which
must then be accounted for in the estimation of a Bayesian network. In this
paper, score metrics that take account of such complexities are proposed for
use in conjunction with score-based methods for the estimation of Bayesian
networks. We propose firstly, a fully Bayesian score metric, and secondly, a
metric inspired by the notion of restricted maximum likelihood. We demonstrate
the performance of these new metrics for the estimation of Bayesian networks
using simulated data with known complex mean structures. We then present the
analysis of expression levels of grape berry genes adjusting for exogenous
variables believed to affect the expression levels of the genes. Demonstrable
biological effects can be inferred from the estimated conditional independence
relationships and correlations amongst the grape-berry genes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.2243</identifier>
 <datestamp>2010-02-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.2243</id><created>2010-02-10</created><authors><author><keyname>George</keyname><forenames>Sam O.</forenames></author><author><keyname>George</keyname><forenames>H. Bola</forenames></author><author><keyname>Nguyen</keyname><forenames>Scott V.</forenames></author></authors><title>Effect of Wind Intermittency on the Electric Grid: Mitigating the Risk
  of Energy Deficits</title><categories>stat.AP physics.ao-ph physics.data-an physics.soc-ph stat.CO stat.ML</categories><comments>8 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Successful implementation of California's Renewable Portfolio Standard (RPS)
mandating 33 percent renewable energy generation by 2020 requires inclusion of
a robust strategy to mitigate increased risk of energy deficits (blackouts) due
to short time-scale (sub 1 hour) intermittencies in renewable energy sources.
Of these RPS sources, wind energy has the fastest growth rate--over 25%
year-over-year. If these growth trends continue, wind energy could make up 15
percent of California's energy portfolio by 2016 (wRPS15). However, the
hour-to-hour variations in wind energy (speed) will create large hourly energy
deficits that require installation of other, more predictable, compensation
generation capacity and infrastructure. Compensating for the energy deficits of
wRPS15 could potentially cost tens of billions in additional dollar-expenditure
for fossil and / or nuclear generation capacity. There is a real possibility
that carbon dioxide and other greenhouse gas (GHG) emission reductions will
miss the California Assembly Bill 32 (CA AB 32) target by a wide margin once
the wRPS15 compensation system is in place. This work presents a set of
analytics tools that show the impact of short-term intermittencies to help
policy makers understand and plan for wRPS15 integration. What are the right
policy choices for RPS that include wind energy?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.2313</identifier>
 <datestamp>2010-02-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.2313</id><created>2010-02-11</created><authors><author><keyname>Pelletier</keyname><forenames>Bruno</forenames><affiliation>IRMAR</affiliation></author><author><keyname>Pudlo</keyname><forenames>Pierre</forenames><affiliation>I3M</affiliation></author></authors><title>Operator norm convergence of spectral clustering on level sets</title><categories>stat.ML math.ST stat.TH</categories><proxy>ccsd hal-00455730</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following Hartigan, a cluster is defined as a connected component of the
t-level set of the underlying density, i.e., the set of points for which the
density is greater than t. A clustering algorithm which combines a density
estimate with spectral clustering techniques is proposed. Our algorithm is
composed of two steps. First, a nonparametric density estimate is used to
extract the data points for which the estimated density takes a value greater
than t. Next, the extracted points are clustered based on the eigenvectors of a
graph Laplacian matrix. Under mild assumptions, we prove the almost sure
convergence in operator norm of the empirical graph Laplacian operator
associated with the algorithm. Furthermore, we give the typical behavior of the
representation of the dataset into the feature space, which establishes the
strong consistency of our proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.2341</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.2341</id><created>2010-02-11</created><updated>2012-05-08</updated><authors><author><keyname>Galtchouk</keyname><forenames>Leonid</forenames><affiliation>IRMA</affiliation></author><author><keyname>Pergamenchtchikov</keyname><forenames>Serguei</forenames><affiliation>LMRS</affiliation></author></authors><title>Geometric ergodicity for families of homogeneous Markov chains</title><categories>math.PR math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we find nonasymptotic exponential upper bounds for the
deviation in the ergodic theorem for families of homogeneous Markov processes.
We find some sufficient conditions for geometric ergodicity uniformly over a
parametric family. We apply this property to the nonasymptotic nonparametric
estimation problem for ergodic diffusion processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.2426</identifier>
 <datestamp>2012-01-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.2426</id><created>2010-02-11</created><updated>2010-02-16</updated><authors><author><keyname>Lu</keyname><forenames>Xin</forenames></author><author><keyname>Bengtsson</keyname><forenames>Linus</forenames></author><author><keyname>Britton</keyname><forenames>Tom</forenames></author><author><keyname>Camitz</keyname><forenames>Martin</forenames></author><author><keyname>Kim</keyname><forenames>Beom Jun</forenames></author><author><keyname>Thorson</keyname><forenames>Anna</forenames></author><author><keyname>Liljeros</keyname><forenames>Fredrik</forenames></author></authors><title>The Sensitivity of Respondent-driven Sampling Method</title><categories>stat.AP physics.data-an q-bio.QM</categories><comments>21 pages, 17 figures, 1 table</comments><journal-ref>Journal of the Royal Statistical Society: Series A 175: 191-216
  (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Researchers in many scientific fields make inferences from individuals to
larger groups. For many groups however, there is no list of members from which
to take a random sample. Respondent-driven sampling (RDS) is a relatively new
sampling methodology that circumvents this difficulty by using the social
networks of the groups under study. The RDS method has been shown to provide
unbiased estimates of population proportions given certain conditions. The
method is now widely used in the study of HIV-related high-risk populations
globally. In this paper, we test the RDS methodology by simulating RDS studies
on the social networks of a large LGBT web community. The robustness of the RDS
method is tested by violating, one by one, the conditions under which the
method provides unbiased estimates. Results reveal that the risk of bias is
large if networks are directed, or respondents choose to invite persons based
on characteristics that are correlated with the study outcomes. If these two
problems are absent, the RDS method shows strong resistance to low response
rates and certain errors in the participants' reporting of their network sizes.
Other issues that might affect the RDS estimates, such as the method for
choosing initial participants, the maximum number of recruitments per
participant, sampling with or without replacement and variations in network
structures, are also simulated and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.2677</identifier>
 <datestamp>2010-02-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.2677</id><created>2010-02-12</created><authors><author><keyname>Subramanian</keyname><forenames>Sushil</forenames></author></authors><title>Compressed Sensing for Sparse Underwater Channel Estimation: Some
  Practical Considerations</title><categories>stat.AP cs.IT math.IT</categories><comments>10 pages, 6 figures, 17 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the use of a structured thresholding algorithm for sparse
underwater channel estimation using compressed sensing. This method shows some
improvements over standard algorithms for sparse channel estimation such as
matching pursuit, iterative detection and least squares.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.2684</identifier>
 <datestamp>2010-02-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.2684</id><created>2010-02-15</created><updated>2010-02-25</updated><authors><author><keyname>Robert</keyname><forenames>Christian P.</forenames></author><author><keyname>Marin</keyname><forenames>Jean-Michel</forenames></author></authors><title>On computational tools for Bayesian data analysis</title><categories>stat.CO stat.ME</categories><comments>This is a chapter for the book "Bayesian Methods and Expert
  Elicitation" edited by Klaus Bocker, 23 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While Robert and Rousseau (2010) addressed the foundational aspects of
Bayesian analysis, the current chapter details its practical aspects through a
review of the computational methods available for approximating Bayesian
procedures. Recent innovations like Monte Carlo Markov chain, sequential Monte
Carlo methods and more recently Approximate Bayesian Computation techniques
have considerably increased the potential for Bayesian applications and they
have also opened new avenues for Bayesian inference, first and foremost
Bayesian model choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.2702</identifier>
 <datestamp>2010-02-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.2702</id><created>2010-02-13</created><authors><author><keyname>Robert</keyname><forenames>Christian P.</forenames></author></authors><title>Bayesian computational methods</title><categories>stat.CO stat.ME</categories><comments>This is a revised version of a chapter written for the Handbook of
  Computational Statistics, edited by J. Gentle, W. Hardle and Y. Mori in 2003,
  in preparation for the second edition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this chapter, we will first present the most standard computational
challenges met in Bayesian Statistics, focussing primarily on mixture
estimation and on model choice issues, and then relate these problems with
computational solutions. Of course, this chapter is only a terse introduction
to the problems and solutions related to Bayesian computations. For more
complete references, see Robert and Casella (2004, 2009), or Marin and Robert
(2007), among others. We also restrain from providing an introduction to
Bayesian Statistics per se and for comprehensive coverage, address the reader
to Robert (2007), (again) among others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.2706</identifier>
 <datestamp>2010-02-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.2706</id><created>2010-02-13</created><authors><author><keyname>Bottolo</keyname><forenames>Leonardo</forenames></author><author><keyname>Richardson</keyname><forenames>Sylvia</forenames></author></authors><title>Evolutionary Stochastic Search for Bayesian model exploration</title><categories>stat.CO stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implementing Bayesian variable selection for linear Gaussian regression
models for analysing high dimensional data sets is of current interest in many
fields. In order to make such analysis operational, we propose a new sampling
algorithm based upon Evolutionary Monte Carlo and designed to work under the
"large p, small n" paradigm, thus making fully Bayesian multivariate analysis
feasible, for example, in genetics/genomics experiments. Two real data examples
in genomics are presented, demonstrating the performance of the algorithm in a
space of up to 10,000 covariates. Finally the methodology is compared with a
recently proposed search algorithms in an extensive simulation study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.2749</identifier>
 <datestamp>2010-02-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.2749</id><created>2010-02-13</created><authors><author><keyname>George</keyname><forenames>Sam O.</forenames></author><author><keyname>George</keyname><forenames>H. Bola</forenames></author><author><keyname>Nguyen</keyname><forenames>Scott V.</forenames></author></authors><title>Risk Quantification Associated with Wind Energy Intermittency in
  California</title><categories>stat.AP physics.soc-ph</categories><comments>8 pages, 6 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As compared to load demand, frequent wind energy intermittencies produce
large short-term (sub 1-hr to 3-hr) deficits (and surpluses) in the energy
supply. These intermittent deficits pose systemic and structural risks that
will likely lead to energy deficits that have significant reliability
implications for energy system operators and consumers. This work provides a
toolset to help policy makers quantify these first-order risks. The thinking
methodology / framework shows that increasing wind energy penetration
significantly increases the risk of loss in California. In addition, the work
presents holistic risk tables as a general innovation to help decision makers
quickly grasp the full impact of risk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.2751</identifier>
 <datestamp>2010-10-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.2751</id><created>2010-02-14</created><updated>2010-10-15</updated><authors><author><keyname>Ghosh</keyname><forenames>Souvik</forenames></author><author><keyname>Samorodnitsky</keyname><forenames>Gennady</forenames></author></authors><title>Long Strange Segments, Ruin Probabilities and the Effect of Memory on
  Moving Average Processes</title><categories>math.PR math.ST stat.TH</categories><comments>29 pages, minor changes and a few typo correction from last version</comments><msc-class>60F10, 60G10, 62M12</msc-class><journal-ref>Stochastic Processes and their Applications, 120 (2010), 2302-2330</journal-ref><doi>10.1016/j.spa.2010.08.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We obtain the rate of growth of long strange segments and the rate of decay
of infinite horizon ruin probabilities for a class of infinite moving average
processes with exponentially light tails. The rates are computed explicitly. We
show that the rates are very similar to those of an i.i.d. process as long as
the moving average coefficients decay fast enough. If they do not, then the
rates are significantly different. This demonstrates the change in the length
of memory in a moving average process associated with certain changes in the
rate of decay of the coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.2845</identifier>
 <datestamp>2011-06-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.2845</id><created>2010-02-15</created><updated>2010-05-26</updated><authors><author><keyname>Roquain</keyname><forenames>Etienne</forenames><affiliation>LPMA</affiliation></author><author><keyname>Villers</keyname><forenames>Fanny</forenames><affiliation>LPMA</affiliation></author></authors><title>Exact calculations for false discovery proportion with application to
  least favorable configurations</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><journal-ref>The Annals of Statistics 39, 1 (2011) 584-612</journal-ref><doi>10.1214/10-AOS847</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a context of multiple hypothesis testing, we provide several new exact
calculations related to the false discovery proportion (FDP) of step-up and
step-down procedures. For step-up procedures, we show that the number of
erroneous rejections conditionally on the rejection number is simply a binomial
variable, which leads to explicit computations of the c.d.f., the {$s$-th}
moment and the mean of the FDP, the latter corresponding to the false discovery
rate (FDR). For step-down procedures, we derive what is to our knowledge the
first explicit formula for the FDR valid for any alternative c.d.f. of the
$p$-values. We also derive explicit computations of the power for both step-up
and step-down procedures. These formulas are "explicit" in the sense that they
only involve the parameters of the model and the c.d.f. of the order statistics
of i.i.d. uniform variables. The $p$-values are assumed either independent or
coming from an equicorrelated multivariate normal model and an additional
mixture model for the true/false hypotheses is used. This new approach is used
to investigate new results which are of interest in their own right, related to
least/most favorable configurations for the FDR and the variance of the FDP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.2890</identifier>
 <datestamp>2012-01-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.2890</id><created>2010-02-15</created><updated>2012-01-05</updated><authors><author><keyname>Wang</keyname><forenames>Feng-Yu</forenames></author></authors><title>Coupling for Ornstein--Uhlenbeck processes with jumps</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ308 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ308</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 4, 1136-1158</journal-ref><doi>10.3150/10-BEJ308</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the linear stochastic differential equation (SDE) on $\mathbb{R}^n$:
\[\mathrm {d}{X}_t=AX_t\,\mathrm{d}t+B\,\mathrm{d}L_t,\] where $A$ is a real
$n\times n$ matrix, $B$ is a real $n\times d$ real matrix and $L_t$ is a
L\'{e}vy process with L\'{e}vy measure $\nu$ on $\mathbb{R}^d$. Assume that
$\nu(\mathrm {d}{z})\ge \rho_0(z)\,\mathrm{d}z$ for some $\rho_0\ge 0$. If
$A\le 0,\operatorname {Rank}(B)=n$ and
$\int_{\{|z-z_0|\le\varepsilon\}}\rho_0(z)^{-1}\,\mathrm{d}z&lt;\infty$ holds for
some $z_0\in \mathbb{R}^d$ and some $\varepsilon&gt;0$, then the associated Markov
transition probability $P_t(x,\mathrm {d}{y})$ satisfies
\[\|P_t(x,\cdot)-P_t(y,\cdot)\|_{\mathrm{var}}\le \frac{C(1+|x-y|)}{\sqrt{t}},
x,y\in \mathbb{R}^d,t&gt;0,\] for some constant $C&gt;0$, which is sharp for large
$t$ and implies that the process has successful couplings. The Harnack
inequality, ultracontractivity and the strong Feller property are also
investigated for the (conditional) transition semigroup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.2928</identifier>
 <datestamp>2011-06-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.2928</id><created>2010-02-15</created><updated>2011-05-10</updated><authors><author><keyname>Ensslin</keyname><forenames>Torsten</forenames></author><author><keyname>Frommert</keyname><forenames>Mona</forenames></author></authors><title>Reconstruction of signals with unknown spectra in information field
  theory with parameter uncertainty</title><categories>astro-ph.IM astro-ph.CO cs.IT math.IT physics.data-an stat.ME</categories><comments>21 pages, 5 figures, accepted by PRD</comments><journal-ref>Phys.Rev.D83:105014,2011</journal-ref><doi>10.1103/PhysRevD.83.105014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimal reconstruction of cosmic metric perturbations and other signals
requires knowledge of their power spectra and other parameters. If these are
not known a priori, they have to be measured simultaneously from the same data
used for the signal reconstruction. We formulate the general problem of signal
inference in the presence of unknown parameters within the framework of
information field theory. We develop a generic parameter uncertainty
renormalized estimation (PURE) technique and address the problem of
reconstructing Gaussian signals with unknown power-spectrum with five different
approaches: (i) separate maximum-a-posteriori power spectrum measurement and
subsequent reconstruction, (ii) maximum-a-posteriori power reconstruction with
marginalized power-spectrum, (iii) maximizing the joint posterior of signal and
spectrum, (iv) guessing the spectrum from the variance in the Wiener filter
map, and (v) renormalization flow analysis of the field theoretical problem
providing the PURE filter. In all cases, the reconstruction can be described or
approximated as Wiener filter operations with assumed signal spectra derived
from the data according to the same recipe, but with differing coefficients.
All of these filters, except the renormalized one, exhibit a perception
threshold in case of a Jeffreys prior for the unknown spectrum. Data modes,
with variance below this threshold do not affect the signal reconstruction at
all. Filter (iv) seems to be similar to the so called Karhune-Loeve and
Feldman-Kaiser-Peacock estimators for galaxy power spectra used in cosmology,
which therefore should also exhibit a marginal perception threshold if
correctly implemented. We present statistical performance tests and show that
the PURE filter is superior to the others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.3045</identifier>
 <datestamp>2010-02-17</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.3045</id><created>2010-02-16</created><authors><author><keyname>Munk</keyname><forenames>Axel</forenames></author><author><keyname>Schmidt-Hieber</keyname><forenames>Johannes</forenames></author></authors><title>Lower bounds for volatility estimation in microstructure noise models</title><categories>math.ST stat.TH</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we derive lower bounds in minimax sense for estimation of the
instantaneous volatility if the diffusion type part cannot be observed directly
but under some additional Gaussian noise. Three different models are
considered. Our technique is based on a general inequality for Kullback-Leibler
divergence of multivariate normal random variables and spectral analysis of the
processes. The derived lower bounds are indeed optimal. Upper bounds can be
found in Munk and Schmidt-Hieber [18]. Our major finding is that the Gaussian
microstructure noise introduces an additional degree of ill-posedness for each
model, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.3128</identifier>
 <datestamp>2011-08-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.3128</id><created>2010-02-16</created><updated>2011-08-01</updated><authors><author><keyname>Percival</keyname><forenames>Daniel</forenames></author><author><keyname>Roeder</keyname><forenames>Kathryn</forenames></author><author><keyname>Rosenfeld</keyname><forenames>Roni</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Structured, sparse regression with application to HIV drug resistance</title><categories>stat.ME stat.AP</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOAS428 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS428</report-no><journal-ref>Annals of Applied Statistics 2011, Vol. 5, No. 2A, 628-644</journal-ref><doi>10.1214/10-AOAS428</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new version of forward stepwise regression. Our modification
finds solutions to regression problems where the selected predictors appear in
a structured pattern, with respect to a predefined distance measure over the
candidate predictors. Our method is motivated by the problem of predicting
HIV-1 drug resistance from protein sequences. We find that our method improves
the interpretability of drug resistance while producing comparable predictive
accuracy to standard methods. We also demonstrate our method in a simulation
study and present some theoretical results and connections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.3241</identifier>
 <datestamp>2010-02-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.3241</id><created>2010-02-17</created><authors><author><keyname>Papavasiliou</keyname><forenames>Anastasia</forenames></author></authors><title>Coarse-grained modeling of multiscale diffusions: the p-variation
  estimates</title><categories>math.ST math.PR stat.ME stat.TH</categories><comments>18 pages, ISAAC conference 2009</comments><msc-class>60G15, 62M05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of estimating parameters of the limiting equation of a
multiscale diffusion in the case of averaging and homogenization, given data
from the corresponding multiscale system. First, we review some recent results
that make use of the maximum likelihood of the limiting equation. In
particular, it has been shown that in the averaging case, the MLE will be
asymptotically consistent in the limit while in the homogenization case, the
MLE will be asymptotically consistent only if we subsample the data. Then, we
focus on the problem of estimating the diffusion coefficient. We suggest a
novel approach that makes use of the total $p$-variation, as defined in the
theory of rough paths and avoids the subsampling step. The method is applied to
a multiscale OU process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.3315</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.3315</id><created>2010-02-17</created><updated>2010-05-19</updated><authors><author><keyname>Fan</keyname><forenames>Jianqing</forenames></author><author><keyname>Feng</keyname><forenames>Yang</forenames></author><author><keyname>Wu</keyname><forenames>Yichao</forenames></author></authors><title>High-dimensional variable selection for Cox's proportional hazards model</title><categories>stat.ML stat.ME</categories><comments>17 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variable selection in high dimensional space has challenged many contemporary
statistical problems from many frontiers of scientific disciplines. Recent
technology advance has made it possible to collect a huge amount of covariate
information such as microarray, proteomic and SNP data via bioimaging
technology while observing survival information on patients in clinical
studies. Thus, the same challenge applies to the survival analysis in order to
understand the association between genomics information and clinical
information about the survival time. In this work, we extend the sure screening
procedure Fan and Lv (2008) to Cox's proportional hazards model with an
iterative version available. Numerical simulation studies have shown
encouraging performance of the proposed method in comparison with other
techniques such as LASSO. This demonstrates the utility and versatility of the
iterative sure independent screening scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.3448</identifier>
 <datestamp>2011-10-17</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.3448</id><created>2010-02-18</created><updated>2011-05-11</updated><authors><author><keyname>Duembgen</keyname><forenames>Lutz</forenames></author><author><keyname>Samworth</keyname><forenames>Richard</forenames></author><author><keyname>Schuhmacher</keyname><forenames>Dominic</forenames></author></authors><title>Approximation by log-concave distributions, with applications to
  regression</title><categories>math.ST math.PR stat.ME stat.TH</categories><comments>Version 3 is the technical report cited in the published paper.
  Published in at http://dx.doi.org/10.1214/10-AOS853 the Annals of Statistics
  (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics
  (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS853</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 2, 702-730</journal-ref><doi>10.1214/10-AOS853</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the approximation of arbitrary distributions $P$ on $d$-dimensional
space by distributions with log-concave density. Approximation means minimizing
a Kullback--Leibler-type functional. We show that such an approximation exists
if and only if $P$ has finite first moments and is not supported by some
hyperplane. Furthermore we show that this approximation depends continuously on
$P$ with respect to Mallows distance $D_1(\cdot,\cdot)$. This result implies
consistency of the maximum likelihood estimator of a log-concave density under
fairly general conditions. It also allows us to prove existence and consistency
of estimators in regression models with a response $Y=\mu(X)+\epsilon$, where
$X$ and $\epsilon$ are independent, $\mu(\cdot)$ belongs to a certain class of
regression functions while $\epsilon$ is a random error with log-concave
density and mean zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.3501</identifier>
 <datestamp>2012-11-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.3501</id><created>2010-02-18</created><updated>2012-11-21</updated><authors><author><keyname>Bogdan</keyname><forenames>Magorzata</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Arijit</forenames></author><author><keyname>Frommlet</keyname><forenames>Florian</forenames></author><author><keyname>Ghosh</keyname><forenames>Jayanta K.</forenames></author></authors><title>Asymptotic Bayes-optimality under sparsity of some multiple testing
  procedures</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS869 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS869</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 3, 1551-1579</journal-ref><doi>10.1214/10-AOS869</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within a Bayesian decision theoretic framework we investigate some asymptotic
optimality properties of a large class of multiple testing rules. A parametric
setup is considered, in which observations come from a normal scale mixture
model and the total loss is assumed to be the sum of losses for individual
tests. Our model can be used for testing point null hypotheses, as well as to
distinguish large signals from a multitude of very small effects. A rule is
defined to be asymptotically Bayes optimal under sparsity (ABOS), if within our
chosen asymptotic framework the ratio of its Bayes risk and that of the Bayes
oracle (a rule which minimizes the Bayes risk) converges to one. Our main
interest is in the asymptotic scheme where the proportion p of "true"
alternatives converges to zero. We fully characterize the class of fixed
threshold multiple testing rules which are ABOS, and hence derive conditions
for the asymptotic optimality of rules controlling the Bayesian False Discovery
Rate (BFDR). We finally provide conditions under which the popular
Benjamini-Hochberg (BH) and Bonferroni procedures are ABOS and show that for a
wide class of sparsity levels, the threshold of the former can be approximated
by a nonrandom threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.3509</identifier>
 <datestamp>2010-12-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.3509</id><created>2010-02-18</created><updated>2010-12-13</updated><authors><author><keyname>Kuljus</keyname><forenames>Kristi</forenames></author><author><keyname>Lember</keyname><forenames>Jri</forenames></author></authors><title>Asymptotic risks of Viterbi segmentation</title><categories>math.PR stat.ML</categories><comments>23 pages</comments><msc-class>60F15, 62M5</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the maximum likelihood (Viterbi) alignment of a hidden Markov
model (HMM). In an HMM, the underlying Markov chain is usually hidden and the
Viterbi alignment is often used as the estimate of it. This approach will be
referred to as the Viterbi segmentation. The goodness of the Viterbi
segmentation can be measured by several risks. In this paper, we prove the
existence of asymptotic risks. Being independent of data, the asymptotic risks
can be considered as the characteristics of the model that illustrate the
long-run behavior of the Viterbi segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.3640</identifier>
 <datestamp>2010-02-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.3640</id><created>2010-02-18</created><authors><author><keyname>Yu</keyname><forenames>Yaming</forenames></author></authors><title>Improved EM for Mixture Proportions with Applications to Nonparametric
  ML Estimation for Censored Data</title><categories>stat.CO stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improved EM strategies, based on the idea of efficient data augmentation
(Meng and van Dyk 1997, 1998), are presented for ML estimation of mixture
proportions. The resulting algorithms inherit the simplicity, ease of
implementation, and monotonic convergence properties of EM, but have
considerably improved speed. Because conventional EM tends to be slow when
there exists a large overlap between the mixture components, we can improve the
speed without sacrificing the simplicity or stability, if we can reformulate
the problem so as to reduce the amount of overlap. We propose simple
"squeezing" strategies for that purpose. Moreover, for high-dimensional
problems, such as computing the nonparametric MLE of the distribution function
with censored data, a natural and effective remedy for conventional EM is to
add exchange steps (based on improved EM) between adjacent mixture components,
where the overlap is most severe. Theoretical considerations show that the
resulting EM-type algorithms, when carefully implemented, are globally
convergent. Simulated and real data examples show dramatic improvement in speed
in realistic situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.3684</identifier>
 <datestamp>2010-02-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.3684</id><created>2010-02-19</created><authors><author><keyname>Zarzoso</keyname><forenames>Vicente</forenames></author><author><keyname>Comon</keyname><forenames>Pierre</forenames></author></authors><title>Robust Independent Component Analysis by Iterative Maximization of the
  Kurtosis Contrast with Algebraic Optimal Step Size</title><categories>stat.ML</categories><proxy>ccsd hal-00457300</proxy><journal-ref>IEEE Transactions on Neural Networks 21, 2 (2010) 248-261</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Independent component analysis (ICA) aims at decomposing an observed random
vector into statistically independent variables. Deflation-based
implementations, such as the popular one-unit FastICA algorithm and its
variants, extract the independent components one after another. A novel method
for deflationary ICA, referred to as RobustICA, is put forward in this paper.
This simple technique consists of performing exact line search optimization of
the kurtosis contrast function. The step size leading to the global maximum of
the contrast along the search direction is found among the roots of a
fourth-degree polynomial. This polynomial rooting can be performed
algebraically, and thus at low cost, at each iteration. Among other practical
benefits, RobustICA can avoid prewhitening and deals with real- and
complex-valued mixtures of possibly noncircular sources alike. The absence of
prewhitening improves asymptotic performance. The algorithm is robust to local
extrema and shows a very high convergence speed in terms of the computational
cost required to reach a given source extraction quality, particularly for
short data records. These features are demonstrated by a comparative numerical
analysis on synthetic data. RobustICA's capabilities in processing real-world
data involving noncircular complex strongly super-Gaussian sources are
illustrated by the biomedical problem of atrial activity (AA) extraction in
atrial fibrillation (AF) electrocardiograms (ECGs), where it outperforms an
alternative ICA-based technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.3744</identifier>
 <datestamp>2010-02-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.3744</id><created>2010-02-19</created><authors><author><keyname>Girard</keyname><forenames>R.</forenames></author></authors><title>Plugin procedure in segmentation and application to hyperspectral image
  segmentation</title><categories>stat.ML math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we give our contribution to the problem of segmentation with
plug-in procedures. We give general sufficient conditions under which plug in
procedure are efficient. We also give an algorithm that satisfy these
conditions. We give an application of the used algorithm to hyperspectral
images segmentation. Hyperspectral images are images that have both spatial and
spectral coherence with thousands of spectral bands on each pixel. In the
proposed procedure we combine a reduction dimension technique and a spatial
regularisation technique. This regularisation is based on the mixlet
modelisation of Kolaczyck and Al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.3784</identifier>
 <datestamp>2011-05-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.3784</id><created>2010-02-19</created><updated>2010-11-25</updated><authors><author><keyname>Schelldorfer</keyname><forenames>Jrg</forenames></author><author><keyname>Bhlmann</keyname><forenames>Peter</forenames></author><author><keyname>van de Geer</keyname><forenames>Sara</forenames></author></authors><title>Estimation for High-Dimensional Linear Mixed-Effects Models Using
  $\ell_1$-Penalization</title><categories>stat.ME stat.CO</categories><journal-ref>Scandinavian Journal of Statistics 2011, 38: 197-214</journal-ref><doi>10.1111/j.1467-9469.2011.00740.x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an $\ell_1$-penalized estimation procedure for high-dimensional
linear mixed-effects models. The models are useful whenever there is a grouping
structure among high-dimensional observations, i.e. for clustered data. We
prove a consistency and an oracle optimality result and we develop an algorithm
with provable numerical convergence. Furthermore, we demonstrate the
performance of the method on simulated and a real high-dimensional data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.3786</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.3786</id><created>2010-02-19</created><authors><author><keyname>Maruyama</keyname><forenames>Yuzo</forenames></author><author><keyname>Strawderman</keyname><forenames>William E.</forenames></author></authors><title>Bayesian predictive densities for linear regression models under
  alpha-divergence loss: some results and open problems</title><categories>math.ST stat.TH</categories><msc-class>62C20, 62J07</msc-class><journal-ref>Institute of Mathematical Statistics Collections, 2012, Volume 8,
  42-56</journal-ref><doi>10.1214/11-IMSCOLL803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers estimation of the predictive density for a normal linear
model with unknown variance under alpha-divergence loss for -1 &lt;= alpha &lt;= 1.
We first give a general canonical form for the problem, and then give general
expressions for the generalized Bayes solution under the above loss for each
alpha. For a particular class of hierarchical generalized priors studied in
Maruyama and Strawderman (2005, 2006) for the problems of estimating the mean
vector and the variance respectively, we give the generalized Bayes predictive
density. Additionally, we show that, for a subclass of these priors, the
resulting estimator dominates the generalized Bayes estimator with respect to
the right invariant prior when alpha=1, i.e., the best (fully) equivariant
minimax estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.3807</identifier>
 <datestamp>2010-02-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.3807</id><created>2010-02-19</created><authors><author><keyname>Catalan</keyname><forenames>Raquel G.</forenames></author><author><keyname>Garay</keyname><forenames>Jose</forenames></author><author><keyname>Lopez-Ruiz</keyname><forenames>Ricardo</forenames></author></authors><title>Some Proofs on Statistical Magnitudes for Continuous Phenomena</title><categories>nlin.AO math.ST physics.data-an stat.TH</categories><comments>4 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, the proofs concerning the continuity of the disequilibrium,
Shannon information and statistical complexity in the space of distributions
are presented. Also, some results on the existence of Shannon information for
continuous systems are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.3878</identifier>
 <datestamp>2010-03-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.3878</id><created>2010-02-20</created><updated>2010-03-01</updated><authors><author><keyname>Gill</keyname><forenames>Richard D.</forenames></author></authors><title>The Three Doors Problem...-s</title><categories>stat.AP math.HO</categories><comments>Submitted to Springer Lexicon of Statistics. Version 2: some minor
  improvements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I argue that we must distinguish between:
  (0) the Three-Doors-Problem Problem [sic], which is to make sense of some
real world question of a real person.
  (1) a large number of solutions to this meta-problem, i.e., many specific
Three-Doors-Problem problems, which are competing mathematizations of the
meta-problem (0).
  Each of the solutions at level (1) can well have a number of different
solutions: nice ones and ugly ones; correct ones and incorrect ones. I discuss
three level (1) solutions, i.e., three different Monty Hall problems; and try
to give three short correct and attractive solutions. These are: an
unconditional probability question; a conditional probability question; and a
game-theory question.
  The meta-message of the article is that applied statisticians should beware
of solution-driven science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.3911</identifier>
 <datestamp>2010-05-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.3911</id><created>2010-02-20</created><updated>2010-05-26</updated><authors><author><keyname>Cialenco</keyname><forenames>Igor</forenames></author></authors><title>Parameter estimations for SPDEs with multiplicative fractional noise</title><categories>math.PR math.ST stat.TH</categories><msc-class>60H15, 62F12, 60G22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study parameter estimation problem for diagonalizable stochastic partial
differential equations driven by a multiplicative fractional noise with any
Hurst parameter $H\in(0,1)$. Two classes of estimators are investigated:
traditional maximum likelihood type estimators, and a new class called
closed-form exact estimators. Finally the general results are applied to
stochastic heat equation driven by a fractional Brownian motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4019</identifier>
 <datestamp>2010-02-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4019</id><created>2010-02-21</created><authors><author><keyname>Bellala</keyname><forenames>Gowtham</forenames></author><author><keyname>Bhavnani</keyname><forenames>Suresh</forenames></author><author><keyname>Scott</keyname><forenames>Clayton</forenames></author></authors><title>Query Learning with Exponential Query Costs</title><categories>stat.ML cs.IT math.IT</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In query learning, the goal is to identify an unknown object while minimizing
the number of "yes" or "no" questions (queries) posed about that object. A
well-studied algorithm for query learning is known as generalized binary search
(GBS). We show that GBS is a greedy algorithm to optimize the expected number
of queries needed to identify the unknown object. We also generalize GBS in two
ways. First, we consider the case where the cost of querying grows
exponentially in the number of queries and the goal is to minimize the expected
exponential cost. Then, we consider the case where the objects are partitioned
into groups, and the objective is to identify only the group to which the
object belongs. We derive algorithms to address these issues in a common,
information-theoretic framework. In particular, we present an exact formula for
the objective function in each case involving Shannon or Renyi entropy, and
develop a greedy algorithm for minimizing it. Our algorithms are demonstrated
on two applications of query learning, active learning and emergency response.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4121</identifier>
 <datestamp>2010-02-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4121</id><created>2010-02-22</created><authors><author><keyname>Gut</keyname><forenames>Allan</forenames></author><author><keyname>Jonsson</keyname><forenames>Fredrik</forenames></author><author><keyname>Stadtmller</keyname><forenames>Ulrich</forenames></author></authors><title>Between the LIL and the LSL</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ195 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ195</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 1, 1-22</journal-ref><doi>10.3150/09-BEJ195</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In two earlier papers, two of the present authors (A.G. and U.S.) extended
Lai's [Ann. Probab. 2 (1974) 432--440] law of the single logarithm for delayed
sums to a multiindex setting in which the edges of the $\mathbf{n}$th window
grow like $|\mathbf {n}|^{\alpha}$, or with different $\alpha$'s, where the
$\alpha$'s belong to $(0,1)$. In this paper, the edge of the $n$th window
typically grows like $n/\log n$, thus at a higher rate than any power less than
one, but not quite at the LIL-rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4257</identifier>
 <datestamp>2010-02-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4257</id><created>2010-02-23</created><authors><author><keyname>Fasen</keyname><forenames>Vicky</forenames></author></authors><title>Asymptotic results for sample autocovariance functions and extremes of
  integrated generalized Ornstein-Uhlenbeck processes</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/08-BEJ174 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ174</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 1, 51-79</journal-ref><doi>10.3150/08-BEJ174</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a positive stationary generalized Ornstein--Uhlenbeck process
\[V_t=\mathrm{e}^{-\xi_t}\biggl(\int_0^t\mathrm{e}^{\xi_{s-}}\
,\mathrm{d}\eta_s+V_0\biggr)\qquadfor t\geq0,\] and the increments of the
integrated generalized Ornstein--Uhlenbeck process
$I_k=\int_{k-1}^k\sqrt{V_{t-}} \mathrm{d}L_t$, $k\in\mathbb{N}$, where
$(\xi_t,\eta_t,L_t)_{t\geq0}$ is a three-dimensional L\'{e}vy process
independent of the starting random variable $V_0$. The genOU model is a
continuous-time version of a stochastic recurrence equation. Hence, our models
include, in particular, continuous-time versions of $\operatorname {ARCH}(1)$
and $\operatorname {GARCH}(1,1)$ processes. In this paper we investigate the
asymptotic behavior of extremes and the sample autocovariance function of
$(V_t)_{t\geq0}$ and $(I_k)_{k\in\mathbb{N}}$. Furthermore, we present a
central limit result for $(I_k)_{k\in\mathbb{N}}$. Regular variation and point
process convergence play a crucial role in establishing the statistics of
$(V_t)_{t\geq0}$ and $(I_k)_{k\in\mathbb{N}}$. The theory can be applied to the
$\operatorname {COGARCH}(1,1)$ and the Nelson diffusion model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4261</identifier>
 <datestamp>2010-02-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4261</id><created>2010-02-23</created><authors><author><keyname>Stelzer</keyname><forenames>Robert</forenames></author></authors><title>Multivariate COGARCH(1,1) processes</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ196 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ196</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 1, 80-115</journal-ref><doi>10.3150/09-BEJ196</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multivariate $\operatorname {COGARCH}(1,1)$ processes are introduced as a
continuous-time models for multidimensional heteroskedastic observations. Our
model is driven by a single multivariate L\'{e}vy process and the latent
time-varying covariance matrix is directly specified as a stochastic process in
the positive semidefinite matrices. After defining the $\operatorname
{COGARCH}(1,1)$ process, we analyze its probabilistic properties. We show a
sufficient condition for the existence of a stationary distribution for the
stochastic covariance matrix process and present criteria ensuring the
finiteness of moments. Under certain natural assumptions on the moments of the
driving L\'{e}vy process, explicit expressions for the first and second-order
moments and (asymptotic) second-order stationarity of the covariance matrix
process are obtained. Furthermore, we study the stationarity and second-order
structure of the increments of the multivariate $\operatorname {COGARCH}(1,1)$
process and their "squares".
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4276</identifier>
 <datestamp>2010-02-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4276</id><created>2010-02-23</created><authors><author><keyname>James</keyname><forenames>Lancelot F.</forenames></author><author><keyname>Lijoi</keyname><forenames>Antonio</forenames></author><author><keyname>Prnster</keyname><forenames>Igor</forenames></author></authors><title>On the posterior distribution of classes of random means</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ200 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ200</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 1, 155-180</journal-ref><doi>10.3150/09-BEJ200</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of properties of mean functionals of random probability measures is
an important area of research in the theory of Bayesian nonparametric
statistics. Many results are now known for random Dirichlet means, but little
is known, especially in terms of posterior distributions, for classes of priors
beyond the Dirichlet process. In this paper, we consider normalized random
measures with independent increments (NRMI's) and mixtures of NRMI. In both
cases, we are able to provide exact expressions for the posterior distribution
of their means. These general results are then specialized, leading to
distributional results for means of two important particular cases of NRMI's
and also of the two-parameter Poisson--Dirichlet process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4283</identifier>
 <datestamp>2010-02-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4283</id><created>2010-02-23</created><authors><author><keyname>Mukherjee</keyname><forenames>Sayan</forenames></author><author><keyname>Wu</keyname><forenames>Qiang</forenames></author><author><keyname>Zhou</keyname><forenames>Ding-Xuan</forenames></author></authors><title>Learning gradients on manifolds</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ206 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ206</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 1, 181-207</journal-ref><doi>10.3150/09-BEJ206</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common belief in high-dimensional data analysis is that data are
concentrated on a low-dimensional manifold. This motivates simultaneous
dimension reduction and regression on manifolds. We provide an algorithm for
learning gradients on manifolds for dimension reduction for high-dimensional
data with few observations. We obtain generalization error bounds for the
gradient estimates and show that the convergence rate depends on the intrinsic
dimension of the manifold and not on the dimension of the ambient space. We
illustrate the efficacy of this approach empirically on simulated and real data
and compare the method to other dimension reduction procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4295</identifier>
 <datestamp>2010-02-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4295</id><created>2010-02-23</created><authors><author><keyname>Budhiraja</keyname><forenames>Amarjit</forenames></author><author><keyname>Dupuis</keyname><forenames>Paul</forenames></author><author><keyname>Maroulas</keyname><forenames>Vasileios</forenames></author></authors><title>Large deviations for stochastic flows of diffeomorphisms</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ203 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ203</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 1, 234-257</journal-ref><doi>10.3150/09-BEJ203</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large deviation principle is established for a general class of stochastic
flows in the small noise limit. This result is then applied to a Bayesian
formulation of an image matching problem, and an approximate maximum likelihood
property is shown for the solution of an optimization problem involving the
large deviations rate function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4297</identifier>
 <datestamp>2010-09-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4297</id><created>2010-02-23</created><updated>2010-09-04</updated><authors><author><keyname>Zhang</keyname><forenames>Xicheng</forenames></author></authors><title>Well-posedness and large deviation for degenerate SDEs with Sobolev
  coefficients</title><categories>math.PR math.ST stat.TH</categories><comments>23Pages. Some details were added</comments><msc-class>60H10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we prove the existence and uniqueness for degenerate
stochastic differential equations with Sobolev (possibly singular) drift and
diffusion coefficients in a generalized sense. In particular, our result covers
the classical DiPerna-Lions flows and, we also obtain the well-posedness for
degenerate Fokker-Planck equations with irregular coefficients. Moreover, a
large deviation principle of Freidlin-Wenzell type for this type of SDEs is
established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4329</identifier>
 <datestamp>2010-02-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4329</id><created>2010-02-23</created><authors><author><keyname>Ma</keyname><forenames>Yanyuan</forenames></author><author><keyname>Li</keyname><forenames>Runze</forenames></author></authors><title>Variable selection in measurement error models</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ205 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ205</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 1, 274-300</journal-ref><doi>10.3150/09-BEJ205</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measurement error data or errors-in-variable data have been collected in many
studies. Natural criterion functions are often unavailable for general
functional measurement error models due to the lack of information on the
distribution of the unobservable covariates. Typically, the parameter
estimation is via solving estimating equations. In addition, the construction
of such estimating equations routinely requires solving integral equations,
hence the computation is often much more intensive compared with ordinary
regression models. Because of these difficulties, traditional best subset
variable selection procedures are not applicable, and in the measurement error
model context, variable selection remains an unsolved issue. In this paper, we
develop a framework for variable selection in measurement error models via
penalized estimating equations. We first propose a class of selection
procedures for general parametric measurement error models and for general
semi-parametric measurement error models, and study the asymptotic properties
of the proposed procedures. Then, under certain regularity conditions and with
a properly chosen regularization parameter, we demonstrate that the proposed
procedure performs as well as an oracle procedure. We assess the finite sample
performance via Monte Carlo simulation studies and illustrate the proposed
methodology through the empirical analysis of a familiar data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4338</identifier>
 <datestamp>2010-02-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4338</id><created>2010-02-23</created><authors><author><keyname>Withers</keyname><forenames>C. S.</forenames></author><author><keyname>Nadarajah</keyname><forenames>S.</forenames></author></authors><title>The distribution and quantiles of functionals of weighted empirical
  distributions when observations have different distributions</title><categories>stat.ME math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends Edgeworth-Cornish-Fisher expansions for the distribution
and quantiles of nonparametric estimates in two ways. Firstly it allows
observations to have different distributions. Secondly it allows the
observations to be weighted in a predetermined way. The use of weighted
estimates has a long history including applications to regression, rank
statistics and Bayes theory. However, asymptotic results have generally been
only first order (the CLT and weak convergence). We give third order
asymptotics for the distribution and percentiles of any smooth functional of a
weighted empirical distribution, thus allowing a considerable increase in
accuracy over earlier CLT results.
  Consider independent non-identically distributed ({\it non-iid}) observations
$X_{1n}, ..., X_{nn}$ in $R^s$. Let $\hat{F}(x)$ be their {\it weighted
empirical distribution} with weights $w_{1n}, ..., w_{nn}$. We obtain cumulant
expansions and hence Edgeworth-Cornish-Fisher expansions for $T(\hat{F})$ for
any smooth functional $T(\cdot)$ by extending the concepts of von Mises
derivatives to signed measures of total measure 1. As an example we give the
cumulant coefficients needed for Edgeworth-Cornish-Fisher expansions to
$O(n^{-3/2})$ for the sample variance when observations are non-iid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4486</identifier>
 <datestamp>2010-02-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4486</id><created>2010-02-24</created><authors><author><keyname>Hallin</keyname><forenames>Marc</forenames></author><author><keyname>Paindaveine</keyname><forenames>Davy</forenames></author><author><keyname>iman</keyname><forenames>Miroslav</forenames></author></authors><title>Multivariate quantiles and multiple-output regression quantiles: From
  $L_1$ optimization to halfspace depth</title><categories>math.ST stat.TH</categories><comments>This paper discussed in: [arXiv:1002.4494], [arXiv:1002.4496],
  [arXiv:1002.4509]. Rejoinder in [arXiv:1002.4515]. Published in at
  http://dx.doi.org/10.1214/09-AOS723 the Annals of Statistics
  (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics
  (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS723</report-no><msc-class>62H05 (Primary) 62J05 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 635-669</journal-ref><doi>10.1214/09-AOS723</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new multivariate concept of quantile, based on a directional version of
Koenker and Bassett's traditional regression quantiles, is introduced for
multivariate location and multiple-output regression problems. In their
empirical version, those quantiles can be computed efficiently via linear
programming techniques. Consistency, Bahadur representation and asymptotic
normality results are established. Most importantly, the contours generated by
those quantiles are shown to coincide with the classical halfspace depth
contours associated with the name of Tukey. This relation does not only allow
for efficient depth contour computations by means of parametric linear
programming, but also for transferring from the quantile to the depth universe
such asymptotic results as Bahadur representations. Finally, linear programming
duality opens the way to promising developments in depth-related multivariate
rank-based inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4494</identifier>
 <datestamp>2010-02-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4494</id><created>2010-02-24</created><authors><author><keyname>Wei</keyname><forenames>Ying</forenames></author></authors><title>Discussion of "Multivariate quantiles and multiple-output regression
  quantiles: From $L_1$ optimization to halfspace depth"</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS723A the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS723A</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 670-675</journal-ref><doi>10.1214/09-AOS723A</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discussion of "Multivariate quantiles and multiple-output regression
quantiles: From $L_1$ optimization to halfspace depth" by M. Hallin, D.
Paindaveine and M. Siman [arXiv:1002.4486]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4496</identifier>
 <datestamp>2010-02-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4496</id><created>2010-02-24</created><authors><author><keyname>Serfling</keyname><forenames>Robert</forenames></author><author><keyname>Zuo</keyname><forenames>Yijun</forenames></author></authors><title>Discussion of "Multivariate quantiles and multiple-output regression
  quantiles: From $L_1$ optimization to halfspace depth"</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS723B the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS723B</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 676-684</journal-ref><doi>10.1214/09-AOS723B</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discussion of "Multivariate quantiles and multiple-output regression
quantiles: From $L_1$ optimization to halfspace depth" by M. Hallin, D.
Paindaveine and M. Siman [arXiv:1002.4486]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4509</identifier>
 <datestamp>2010-02-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4509</id><created>2010-02-24</created><authors><author><keyname>Kong</keyname><forenames>Linglong</forenames></author><author><keyname>Mizera</keyname><forenames>Ivan</forenames></author></authors><title>Discussion of "Multivariate quantiles and multiple-output regression
  quantiles: From $L_1$ optimization to halfspace depth"</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS723C the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS723C</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 685-693</journal-ref><doi>10.1214/09-AOS723C</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discussion of "Multivariate quantiles and multiple-output regression
quantiles: From $L_1$ optimization to halfspace depth" by M. Hallin, D.
Paindaveine and M. Siman [arXiv:1002.4486]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4515</identifier>
 <datestamp>2010-02-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4515</id><created>2010-02-24</created><authors><author><keyname>Hallin</keyname><forenames>Marc</forenames></author><author><keyname>Paindaveine</keyname><forenames>Davy</forenames></author><author><keyname>iman</keyname><forenames>Miroslav</forenames></author></authors><title>Rejoinder to "Multivariate quantiles and multiple-output regression
  quantiles: From $L_1$ optimization to halfspace depth"</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS723REJ the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS723REJ</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 694-703</journal-ref><doi>10.1214/09-AOS723REJ</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rejoinder to "Multivariate quantiles and multiple-output regression
quantiles: From $L_1$ optimization to halfspace depth" by M. Hallin, D.
Paindaveine and M. Siman [arXiv:1002.4486]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4533</identifier>
 <datestamp>2010-02-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4533</id><created>2010-02-24</created><authors><author><keyname>Ferrari</keyname><forenames>Davide</forenames></author><author><keyname>Yang</keyname><forenames>Yuhong</forenames></author></authors><title>Maximum L$q$-likelihood estimation</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS687 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS687</report-no><msc-class>62F99 (Primary) 60F05, 94A17, 62G32 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 753-783</journal-ref><doi>10.1214/09-AOS687</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the maximum L$q$-likelihood estimator (ML$q$E), a new
parameter estimator based on nonextensive entropy [Kibernetika 3 (1967) 30--35]
is introduced. The properties of the ML$q$E are studied via asymptotic analysis
and computer simulations. The behavior of the ML$q$E is characterized by the
degree of distortion $q$ applied to the assumed model. When $q$ is properly
chosen for small and moderate sample sizes, the ML$q$E can successfully trade
bias for precision, resulting in a substantial reduction of the mean squared
error. When the sample size is large and $q$ tends to 1, a necessary and
sufficient condition to ensure a proper asymptotic normality and efficiency of
ML$q$E is established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4545</identifier>
 <datestamp>2010-02-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4545</id><created>2010-02-24</created><updated>2010-02-24</updated><authors><author><keyname>Bickel</keyname><forenames>Peter J.</forenames></author><author><keyname>Lindner</keyname><forenames>Marko</forenames></author></authors><title>Approximating the inverse of banded matrices by banded matrices with
  applications to probability and statistics</title><categories>math.ST math.FA stat.TH</categories><msc-class>60G15; 47B36, 47L10, 62H25, 62M10, 62M20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the first part of this paper we give an elementary proof of the fact that
if an infinite matrix $A$, which is invertible as a bounded operator on
$\ell^2$, can be uniformly approximated by banded matrices then so can the
inverse of $A$. We give explicit formulas for the banded approximations of
$A^{-1}$ as well as bounds on their accuracy and speed of convergence in terms
of their band-width. In the second part we apply these results to covariance
matrices $\Sigma$ of Gaussian processes and study mixing and beta mixing of
processes in terms of properties of $\Sigma$. Finally, we note some
applications of our results to statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4547</identifier>
 <datestamp>2010-02-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4547</id><created>2010-02-24</created><authors><author><keyname>Chen</keyname><forenames>Song Xi</forenames></author><author><keyname>Qin</keyname><forenames>Ying-Li</forenames></author></authors><title>A two-sample test for high-dimensional data with applications to
  gene-set testing</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS716 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS716</report-no><msc-class>62H15, 60K35 (Primary) 62G10 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 808-835</journal-ref><doi>10.1214/09-AOS716</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a two-sample test for the means of high-dimensional data when the
data dimension is much larger than the sample size. Hotelling's classical $T^2$
test does not work for this "large $p$, small $n$" situation. The proposed test
does not require explicit conditions in the relationship between the data
dimension and sample size. This offers much flexibility in analyzing
high-dimensional data. An application of the proposed test is in testing
significance for sets of genes which we demonstrate in an empirical study on a
leukemia data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4554</identifier>
 <datestamp>2010-02-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4554</id><created>2010-02-24</created><authors><author><keyname>Kuelbs</keyname><forenames>Jim</forenames></author><author><keyname>Vidyashankar</keyname><forenames>Anand N.</forenames></author></authors><title>Asymptotic inference for high-dimensional data</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS718 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS718</report-no><msc-class>60B10, 60B12, 60F05, 62A01, 62H15, 62G20, 62F40, 92B15 (Primary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 836-869</journal-ref><doi>10.1214/09-AOS718</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study inference for high-dimensional data characterized by
small sample sizes relative to the dimension of the data. In particular, we
provide an infinite-dimensional framework to study statistical models that
involve situations in which (i) the number of parameters increase with the
sample size (that is, allowed to be random) and (ii) there is a possibility of
missing data. Under a variety of tail conditions on the components of the data,
we provide precise conditions for the joint consistency of the estimators of
the mean. In the process, we clarify and improve some of the recent consistency
results that appeared in the literature. An important aspect of the work
presented is the development of asymptotic normality results for these models.
As a consequence, we construct different test statistics for one-sample and
two-sample problems concerning the mean vector and obtain their asymptotic
distributions as a corollary of the infinite-dimensional results. Finally, we
use these theoretical results to develop an asymptotically justifiable
methodology for data analyses. Simulation results presented here describe
situations where the methodology can be successfully applied. They also
evaluate its robustness under a variety of conditions, some of which are
substantially different from the technical conditions. Comparisons to other
methods used in the literature are provided. Analyses of real-life data is also
included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4658</identifier>
 <datestamp>2010-05-14</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4658</id><created>2010-02-24</created><updated>2010-05-12</updated><authors><author><keyname>Xu</keyname><forenames>Huan</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Principal Component Analysis with Contaminated Data: The High
  Dimensional Case</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the dimensionality-reduction problem (finding a subspace
approximation of observed data) for contaminated data in the high dimensional
regime, where the number of observations is of the same magnitude as the number
of variables of each observation, and the data set contains some (arbitrarily)
corrupted observations. We propose a High-dimensional Robust Principal
Component Analysis (HR-PCA) algorithm that is tractable, robust to contaminated
points, and easily kernelizable. The resulting subspace has a bounded deviation
from the desired one, achieves maximal robustness -- a breakdown point of 50%
while all existing algorithms have a breakdown point of zero, and unlike
ordinary PCA algorithms, achieves optimality in the limit case where the
proportion of corrupted points goes to zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4665</identifier>
 <datestamp>2010-03-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4665</id><created>2010-02-24</created><authors><author><keyname>Boyd-Graber</keyname><forenames>Jordan</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author></authors><title>Syntactic Topic Models</title><categories>cs.CL cs.AI math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The syntactic topic model (STM) is a Bayesian nonparametric model of language
that discovers latent distributions of words (topics) that are both
semantically and syntactically coherent. The STM models dependency parsed
corpora where sentences are grouped into documents. It assumes that each word
is drawn from a latent topic chosen by combining document-level features and
the local syntactic context. Each document has a distribution over latent
topics, as in topic models, which provides the semantic consistency. Each
element in the dependency parse tree also has a distribution over the topics of
its children, as in latent-state syntax models, which provides the syntactic
consistency. These distributions are convolved so that the topic of each word
is likely under both its document and syntactic context. We derive a fast
posterior inference algorithm based on variational methods. We report
qualitative and quantitative studies on both synthetic data and hand-parsed
documents. We show that the STM is a more predictive model of language than
current models based only on syntax or only on topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4682</identifier>
 <datestamp>2010-02-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4682</id><created>2010-02-24</created><authors><author><keyname>Kurihara</keyname><forenames>Kazutaka</forenames></author><author><keyname>Tutiya</keyname><forenames>Yohei</forenames></author></authors><title>Non-Central Limit Theorem Statistical Analysis for the "Long-tailed"
  Internet Society</title><categories>stat.AP stat.CO</categories><comments>8 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a statistical analysis method and introduces the
corresponding software package "tailstat," which is believed to be widely
applicable to today's internet society. The proposed method facilitates
statistical analyses with small sample sets from given populations, which
render the central limit theorem inapplicable. A large-scale case study
demonstrates the effectiveness of the method and provides implications for
applying similar analyses to other cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4734</identifier>
 <datestamp>2010-02-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4734</id><created>2010-02-25</created><authors><author><keyname>Zhang</keyname><forenames>Cun-Hui</forenames></author></authors><title>Nearly unbiased variable selection under minimax concave penalty</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS729 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS729</report-no><msc-class>62J05, 62J07 (Primary) 62H12, 62H25 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 894-942</journal-ref><doi>10.1214/09-AOS729</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose MC+, a fast, continuous, nearly unbiased and accurate method of
penalized variable selection in high-dimensional linear regression. The LASSO
is fast and continuous, but biased. The bias of the LASSO may prevent
consistent variable selection. Subset selection is unbiased but computationally
costly. The MC+ has two elements: a minimax concave penalty (MCP) and a
penalized linear unbiased selection (PLUS) algorithm. The MCP provides the
convexity of the penalized loss in sparse regions to the greatest extent given
certain thresholds for variable selection and unbiasedness. The PLUS computes
multiple exact local minimizers of a possibly nonconvex penalized loss function
in a certain main branch of the graph of critical points of the penalized loss.
Its output is a continuous piecewise linear path encompassing from the origin
for infinite penalty to a least squares solution for zero penalty. We prove
that at a universal penalty level, the MC+ has high probability of matching the
signs of the unknowns, and thus correct selection, without assuming the strong
irrepresentable condition required by the LASSO. This selection consistency
applies to the case of $p\gg n$, and is proved to hold for exactly the MC+
solution among possibly many local minimizers. We prove that the MC+ attains
certain minimax convergence rates in probability for the estimation of
regression coefficients in $\ell_r$ balls. We use the SURE method to derive
degrees of freedom and $C_p$-type risk estimates for general penalized LSE,
including the LASSO and MC+ estimators, and prove their unbiasedness. Based on
the estimated degrees of freedom, we propose an estimator of the noise level
for proper choice of the penalty level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4754</identifier>
 <datestamp>2010-02-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4754</id><created>2010-02-25</created><authors><author><keyname>Wang</keyname><forenames>Yazhen</forenames></author><author><keyname>Zou</keyname><forenames>Jian</forenames></author></authors><title>Vast volatility matrix estimation for high-frequency financial data</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS730 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS730</report-no><msc-class>62H12 (Primary) 62G05, 62M05, 62P20 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 943-978</journal-ref><doi>10.1214/09-AOS730</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-frequency data observed on the prices of financial assets are commonly
modeled by diffusion processes with micro-structure noise, and realized
volatility-based methods are often used to estimate integrated volatility. For
problems involving a large number of assets, the estimation objects we face are
volatility matrices of large size. The existing volatility estimators work well
for a small number of assets but perform poorly when the number of assets is
very large. In fact, they are inconsistent when both the number, $p$, of the
assets and the average sample size, $n$, of the price data on the $p$ assets go
to infinity. This paper proposes a new type of estimators for the integrated
volatility matrix and establishes asymptotic theory for the proposed estimators
in the framework that allows both $n$ and $p$ to approach to infinity. The
theory shows that the proposed estimators achieve high convergence rates under
a sparsity assumption on the integrated volatility matrix. The numerical
studies demonstrate that the proposed estimators perform well for large $p$ and
complex price and volatility models. The proposed method is applied to real
high-frequency financial data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4756</identifier>
 <datestamp>2010-02-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4756</id><created>2010-02-25</created><authors><author><keyname>Kyung</keyname><forenames>Minjung</forenames></author><author><keyname>Gill</keyname><forenames>Jeff</forenames></author><author><keyname>Casella</keyname><forenames>George</forenames></author></authors><title>Estimation in Dirichlet random effects models</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS731 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS731</report-no><msc-class>62F99 (Primary) 62P25, 62G99 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 979-1009</journal-ref><doi>10.1214/09-AOS731</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new Gibbs sampler for a linear mixed model with a Dirichlet
process random effect term, which is easily extended to a generalized linear
mixed model with a probit link function. Our Gibbs sampler exploits the
properties of the multinomial and Dirichlet distributions, and is shown to be
an improvement, in terms of operator norm and efficiency, over other commonly
used MCMC algorithms. We also investigate methods for the estimation of the
precision parameter of the Dirichlet process, finding that maximum likelihood
may not be desirable, but a posterior mode is a reasonable approach. Examples
are given to show how these models perform on real data. Our results complement
both the theoretical basis of the Dirichlet process nonparametric prior and the
computational work that has been done to date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4770</identifier>
 <datestamp>2010-02-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4770</id><created>2010-02-25</created><authors><author><keyname>Walther</keyname><forenames>Guenther</forenames></author></authors><title>Optimal and fast detection of spatial clusters with scan statistics</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS732 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS732</report-no><msc-class>62G10 (Primary) 62H30 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 1010-1033</journal-ref><doi>10.1214/09-AOS732</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the detection of multivariate spatial clusters in the Bernoulli
model with $N$ locations, where the design distribution has weakly dependent
marginals. The locations are scanned with a rectangular window with sides
parallel to the axes and with varying sizes and aspect ratios. Multivariate
scan statistics pose a statistical problem due to the multiple testing over
many scan windows, as well as a computational problem because statistics have
to be evaluated on many windows. This paper introduces methodology that leads
to both statistically optimal inference and computationally efficient
algorithms. The main difference to the traditional calibration of scan
statistics is the concept of grouping scan windows according to their sizes,
and then applying different critical values to different groups. It is shown
that this calibration of the scan statistic results in optimal inference for
spatial clusters on both small scales and on large scales, as well as in the
case where the cluster lives on one of the marginals. Methodology is introduced
that allows for an efficient approximation of the set of all rectangles while
still guaranteeing the statistical optimality results described above. It is
shown that the resulting scan statistic has a computational complexity that is
almost linear in $N$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4775</identifier>
 <datestamp>2010-02-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4775</id><created>2010-02-25</created><authors><author><keyname>Silva</keyname><forenames>Ralph</forenames></author><author><keyname>Kohn</keyname><forenames>Robert</forenames></author><author><keyname>Giordani</keyname><forenames>Paolo</forenames></author><author><keyname>Mun</keyname><forenames>Xiuyan</forenames></author></authors><title>A copula based approach to adaptive sampling</title><categories>stat.ME stat.CO</categories><comments>33 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our article is concerned with adaptive sampling schemes for Bayesian
inference that update the proposal densities using previous iterates. We
introduce a copula based proposal density which is made more efficient by
combining it with antithetic variable sampling. We compare the copula based
proposal to an adaptive proposal density based on a multivariate mixture of
normals and an adaptive random walk Metropolis proposal. We also introduce a
refinement of the random walk proposal which performs better for multimodal
target distributions. We compare the sampling schemes using challenging but
realistic models and priors applied to real data examples. The results show
that for the examples studied, the adaptive independent \MH{} proposals are
much more efficient than the adaptive random walk proposals and that in general
the copula based proposal has the best acceptance rates and lowest
inefficiencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4781</identifier>
 <datestamp>2010-02-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4781</id><created>2010-02-25</created><authors><author><keyname>Hall</keyname><forenames>Peter</forenames></author><author><keyname>Pham</keyname><forenames>Tung</forenames></author></authors><title>Optimal properties of centroid-based classifiers for very
  high-dimensional data</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS736 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS736</report-no><msc-class>62H30 (Primary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 1071-1093</journal-ref><doi>10.1214/09-AOS736</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that scale-adjusted versions of the centroid-based classifier enjoys
optimal properties when used to discriminate between two very high-dimensional
populations where the principal differences are in location. The scale
adjustment removes the tendency of scale differences to confound differences in
means. Certain other distance-based methods, for example, those founded on
nearest-neighbor distance, do not have optimal performance in the sense that we
propose. Our results permit varying degrees of sparsity and signal strength to
be treated, and require only mild conditions on dependence of vector
components. Additionally, we permit the marginal distributions of vector
components to vary extensively. In addition to providing theory we explore
numerical properties of a centroid-based classifier, and show that these
features reflect theoretical accounts of performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4789</identifier>
 <datestamp>2010-02-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4789</id><created>2010-02-25</created><authors><author><keyname>Li</keyname><forenames>Bing</forenames></author><author><keyname>Kim</keyname><forenames>Min Kyung</forenames></author><author><keyname>Altman</keyname><forenames>Naomi</forenames></author></authors><title>On dimension folding of matrix- or array-valued statistical objects</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS737 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS737</report-no><msc-class>62H12, 62G08, 62-09 (Primary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 1094-1121</journal-ref><doi>10.1214/09-AOS737</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider dimension reduction for regression or classification in which the
predictors are matrix- or array-valued. This type of predictor arises when
measurements are obtained for each combination of two or more underlying
variables--for example, the voltage measured at different channels and times in
electroencephalography data. For these applications, it is desirable to
preserve the array structure of the reduced predictor (e.g., time versus
channel), but this cannot be achieved within the conventional dimension
reduction formulation. In this paper, we introduce a dimension reduction
method, to be called dimension folding, for matrix- and array-valued predictors
that preserves the array structure. In an application of dimension folding to
an electroencephalography data set, we correctly classify 97 out of 122
subjects as alcoholic or nonalcoholic based on their electroencephalography in
a cross-validation sample.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4801</identifier>
 <datestamp>2010-02-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4801</id><created>2010-02-25</created><authors><author><keyname>Gin</keyname><forenames>Evarist</forenames></author><author><keyname>Nickl</keyname><forenames>Richard</forenames></author></authors><title>Confidence bands in density estimation</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS738 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS738</report-no><msc-class>62G07 (Primary), 60F05 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 1122-1170</journal-ref><doi>10.1214/09-AOS738</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a sample from some unknown continuous density
$f:\mathbb{R}\to\mathbb{R}$, we construct adaptive confidence bands that are
honest for all densities in a "generic" subset of the union of $t$-H\"older
balls, $0&lt;t\le r$, where $r$ is a fixed but arbitrary integer. The exceptional
("nongeneric") set of densities for which our results do not hold is shown to
be nowhere dense in the relevant H\"older-norm topologies. In the course of the
proofs we also obtain limit theorems for maxima of linear wavelet and kernel
density estimators, which are of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4802</identifier>
 <datestamp>2010-03-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4802</id><created>2010-02-25</created><updated>2010-03-12</updated><authors><author><keyname>Silva</keyname><forenames>Ricardo</forenames></author><author><keyname>Gramacy</keyname><forenames>Robert B.</forenames></author></authors><title>Gaussian Process Structural Equation Models with Latent Variables</title><categories>cs.LG stat.ML</categories><comments>12 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a variety of disciplines such as social sciences, psychology, medicine and
economics, the recorded data are considered to be noisy measurements of latent
variables connected by some causal structure. This corresponds to a family of
graphical models known as the structural equation model with latent variables.
While linear non-Gaussian variants have been well-studied, inference in
nonparametric structural equation models is still underdeveloped. We introduce
a sparse Gaussian process parameterization that defines a non-linear structure
connecting latent variables, unlike common formulations of Gaussian process
latent variable models. The sparse parameterization is given a full Bayesian
treatment without compromising Markov chain Monte Carlo efficiency. We compare
the stability of the sampling procedure and the predictive ability of the model
against the current practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4850</identifier>
 <datestamp>2011-05-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4850</id><created>2010-02-25</created><updated>2011-02-24</updated><authors><author><keyname>Loecherbach</keyname><forenames>Eva</forenames></author><author><keyname>Orlandi</keyname><forenames>Enza</forenames></author></authors><title>Neighborhood radius estimation in Variable-neighborhood Random Fields</title><categories>math.PR math.ST stat.TH</categories><msc-class>60D05, 62F12, 60G55, 60G60, 62M40</msc-class><doi>10.1016/j.spa.2011.05.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider random fields defined by finite-region conditional probabilities
depending on a neighborhood of the region which changes with the boundary
conditions. To predict the symbols within any finite region it is necessary to
inspect a random number of neighborhood symbols which might change according to
the value of them. In analogy to the one dimensional setting we call these
neighborhood symbols the context of the region. This framework is a natural
extension, to d-dimensional fields, of the notion of variable-length Markov
chains introduced by Rissanen (1983) in his classical paper. We define an
algorithm to estimate the radius of the smallest ball containing the context
based on a realization of the field. We prove the consistency of this
estimator. Our proofs are constructive and yield explicit upper bounds for the
probability of wrong estimation of the radius of the context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4931</identifier>
 <datestamp>2010-03-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4931</id><created>2010-02-26</created><updated>2010-03-01</updated><authors><author><keyname>Delaigle</keyname><forenames>Aurore</forenames></author><author><keyname>Hall</keyname><forenames>Peter</forenames></author></authors><title>Defining probability density for a distribution of random functions</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS741 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS741</report-no><msc-class>62G05 (Primary) 62G07 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 1171-1193</journal-ref><doi>10.1214/09-AOS741</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of probability density for a random function is not as
straightforward as in finite-dimensional cases. While a probability density
function generally does not exist for functional data, we show that it is
possible to develop the notion of density when functional data are considered
in the space determined by the eigenfunctions of principal component analysis.
This leads to a transparent and meaningful surrogate for density defined in
terms of the average value of the logarithms of the densities of the
distributions of principal components for a given dimension. This density
approximation is estimable readily from data. It accurately represents, in a
monotone way, key features of small-ball approximations to density. Our results
on estimators of the densities of principal component scores are also of
independent interest; they reveal interesting shape differences that have not
previously been considered. The statistical implications of these results and
properties are identified and discussed, and practical ramifications are
illustrated in numerical work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4945</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4945</id><created>2010-02-26</created><authors><author><keyname>Wang</keyname><forenames>Weizhen</forenames></author></authors><title>On construction of the smallest one-sided confidence interval for the
  difference of two proportions</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS744 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS744</report-no><msc-class>62F25 (Primary) 62J15, 62P10 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 1227-1243</journal-ref><doi>10.1214/09-AOS744</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any class of one-sided $1-\alpha$ confidence intervals with a certain
monotonicity ordering on the random confidence limit, the smallest interval, in
the sense of the set inclusion for the difference of two proportions of two
independent binomial random variables, is constructed based on a direct
analysis of coverage probability function. A special ordering on the confidence
limit is developed and the corresponding smallest confidence interval is
derived. This interval is then applied to identify the minimum effective dose
(MED) for binary data in dose-response studies, and a multiple test procedure
that controls the familywise error rate at level $\alpha$ is obtained. A
generalization of constructing the smallest one-sided confidence interval to
other discrete sample spaces is discussed in the presence of nuisance
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4946</identifier>
 <datestamp>2010-03-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4946</id><created>2010-02-26</created><authors><author><keyname>Egloff</keyname><forenames>Daniel</forenames></author><author><keyname>Leippold</keyname><forenames>Markus</forenames></author></authors><title>Quantile estimation with adaptive importance sampling</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS745 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS745</report-no><msc-class>62L20, 65C05 (Primary) 65C60 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 1244-1278</journal-ref><doi>10.1214/09-AOS745</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce new quantile estimators with adaptive importance sampling. The
adaptive estimators are based on weighted samples that are neither independent
nor identically distributed. Using a new law of iterated logarithm for
martingales, we prove the convergence of the adaptive quantile estimators for
general distributions with nonunique quantiles thereby extending the work of
Feldman and Tucker [Ann. Math. Statist. 37 (1996) 451--457]. We illustrate the
algorithm with an example from credit portfolio risk analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1002.4959</identifier>
 <datestamp>2010-03-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1002.4959</id><created>2010-02-26</created><authors><author><keyname>Jensen</keyname><forenames>Jens Ledet</forenames></author></authors><title>On some problems in the article "Efficient Likelihood Estimation in
  State Space Models" by Cheng-Der Fuh</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS748A the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS748A</report-no><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 1279-1281</journal-ref><doi>10.1214/09-AOS748A</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On some problems in the article "Efficient Likelihood Estimation in State
Space Models" by Cheng-Der Fuh [Ann. Statist. 34 (2006) 2026--2068]
[arXiv:math/0611376]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0060</identifier>
 <datestamp>2010-03-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0060</id><created>2010-02-26</created><authors><author><keyname>Guo</keyname><forenames>Z. X.</forenames></author></authors><title>Comment on "Fastest learning in small-world neural networks"</title><categories>stat.ML cs.NE</categories><comments>8 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This comment reexamines Simard et al.'s work in [D. Simard, L. Nadeau, H.
Kroger, Phys. Lett. A 336 (2005) 8-15]. We found that Simard et al. calculated
mistakenly the local connectivity lengths Dlocal of networks. The right results
of Dlocal are presented and the supervised learning performance of feedforward
neural networks (FNNs) with different rewirings are re-investigated in this
comment. This comment discredits Simard et al's work by two conclusions: 1)
Rewiring connections of FNNs cannot generate networks with small-world
connectivity; 2) For different training sets, there do not exist networks with
a certain number of rewirings generating reduced learning errors than networks
with other numbers of rewiring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0078</identifier>
 <datestamp>2010-03-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0078</id><created>2010-02-27</created><authors><author><keyname>Kloft</keyname><forenames>Marius</forenames></author><author><keyname>Laskov</keyname><forenames>Pavel</forenames></author></authors><title>Security Analysis of Online Centroid Anomaly Detection</title><categories>stat.ML</categories><report-no>UCB/EECS-2010-22</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security issues are crucial in a number of machine learning applications,
especially in scenarios dealing with human activity rather than natural
phenomena (e.g., information ranking, spam detection, malware detection, etc.).
It is to be expected in such cases that learning algorithms will have to deal
with manipulated data aimed at hampering decision making. Although some
previous work addressed the handling of malicious data in the context of
supervised learning, very little is known about the behavior of anomaly
detection methods in such scenarios. In this contribution we analyze the
performance of a particular method -- online centroid anomaly detection -- in
the presence of adversarial noise. Our analysis addresses the following
security-related issues: formalization of learning and attack processes,
derivation of an optimal attack, analysis of its efficiency and constraints. We
derive bounds on the effectiveness of a poisoning attack against centroid
anomaly under different conditions: bounded and unbounded percentage of
traffic, and bounded false positive rate. Our bounds show that whereas a
poisoning attack can be effectively staged in the unconstrained case, it can be
made arbitrarily difficult (a strict upper bound on the attacker's gain) if
external constraints are properly used. Our experimental evaluation carried out
on real HTTP and exploit traces confirms the tightness of our theoretical
bounds and practicality of our protection mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0079</identifier>
 <datestamp>2010-10-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0079</id><created>2010-02-27</created><updated>2010-10-26</updated><authors><author><keyname>Kloft</keyname><forenames>Marius</forenames></author><author><keyname>Brefeld</keyname><forenames>Ulf</forenames></author><author><keyname>Sonnenburg</keyname><forenames>Soeren</forenames></author><author><keyname>Zien</keyname><forenames>Alexander</forenames></author></authors><title>Non-Sparse Regularization for Multiple Kernel Learning</title><categories>cs.LG stat.ML</categories><report-no>UCB/EECS-2010-21</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning linear combinations of multiple kernels is an appealing strategy
when the right choice of features is unknown. Previous approaches to multiple
kernel learning (MKL) promote sparse kernel combinations to support
interpretability and scalability. Unfortunately, this 1-norm MKL is rarely
observed to outperform trivial baselines in practical applications. To allow
for robust kernel mixtures, we generalize MKL to arbitrary norms. We devise new
insights on the connection between several existing MKL formulations and
develop two efficient interleaved optimization strategies for arbitrary norms,
like p-norms with p&gt;1. Empirically, we demonstrate that the interleaved
optimization strategies are much faster compared to the commonly used wrapper
approaches. A theoretical analysis and an experiment on controlled artificial
data experiment sheds light on the appropriateness of sparse, non-sparse and
$\ell_\infty$-norm MKL in various scenarios. Empirical applications of p-norm
MKL to three real-world problems from computational biology show that
non-sparse MKL achieves accuracies that go beyond the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0173</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0173</id><created>2010-02-28</created><authors><author><keyname>Breto</keyname><forenames>Carles</forenames></author><author><keyname>Ionides</keyname><forenames>Edward L.</forenames></author></authors><title>Compound Markov counting processes and their applications to modeling
  infinitesimally over-dispersed systems</title><categories>math.ST stat.TH</categories><comments>26 pages</comments><journal-ref>Stochastic Processes and their Applications 2011, Vol. 121 (11),
  2571-2591</journal-ref><doi>10.1016/j.spa.2011.07.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an infinitesimal dispersion index for Markov counting processes.
We show that, under standard moment existence conditions, a process is
infinitesimally (over-) equi-dispersed if, and only if, it is simple
(compound), i.e. it increases in jumps of one (or more) unit(s), even though
infinitesimally equi-dispersed processes might be under-, equi- or
over-dispersed using previously studied indices. Compound processes arise, for
example, when introducing continuous-time white noise to the rates of simple
processes resulting in Levy-driven SDEs. We construct multivariate
infinitesimally over-dispersed compartment models and queuing networks,
suitable for applications where moment constraints inherent to simple processes
do not hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0182</identifier>
 <datestamp>2010-03-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0182</id><created>2010-02-28</created><authors><author><keyname>Gill</keyname><forenames>Richard D.</forenames></author><author><keyname>Keiding</keyname><forenames>Niels</forenames></author></authors><title>Product-limit estimators of the gap time distribution of a renewal
  process under different sampling patterns</title><categories>stat.AP stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonparametric estimation of the gap time distribution in a simple renewal
process may be considered a problem in survival analysis under particular
sampling frames corresponding to how the renewal process is observed. This note
describes several such situations where simple product limit estimators, though
inefficient, may still be useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0188</identifier>
 <datestamp>2010-03-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0188</id><created>2010-02-28</created><authors><author><keyname>Aalen</keyname><forenames>Odd O.</forenames></author><author><keyname>Andersen</keyname><forenames>Per Kragh</forenames></author><author><keyname>Borgan</keyname><forenames>\Ornulf</forenames></author><author><keyname>Gill</keyname><forenames>Richard D.</forenames></author><author><keyname>Keiding</keyname><forenames>Niels</forenames></author></authors><title>History of applications of martingales in survival analysis</title><categories>stat.ME</categories><journal-ref>Electronic Journal for History of Probability and Statistics, Vol.
  5, Nr. 1, June 2009 (www.jehps.net), 28 pp</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper traces the development of the use of martingale methods in survival
analysis from the mid 1970's to the early 1990's. This development was
initiated by Aalen's Berkeley PhD-thesis in 1975, progressed through the work
on estimation of Markov transition probabilities, non-parametric tests and
Cox's regression model in the late 1970's and early 1980's, and it was
consolidated in the early 1990's with the publication of the monographs by
Fleming and Harrington (1991) and Andersen, Borgan, Gill and Keiding (1993).
The development was made possible by an unusually fast technology transfer of
pure mathematical concepts, primarily from French probability, into practical
biostatistical methodology, and we attempt to outline some of the personal
relationships that helped this happen. We also point out that survival analysis
was ready for this development since the martingale ideas inherent in the deep
understanding of temporal development so intrinsic to the French theory of
processes were already quite close to the surface in survival analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0205</identifier>
 <datestamp>2010-03-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0205</id><created>2010-02-28</created><authors><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Nowak</keyname><forenames>Robert D.</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author></authors><title>Detecting Weak but Hierarchically-Structured Patterns in Networks</title><categories>cs.IT cs.LG math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to detect weak distributed activation patterns in networks is
critical to several applications, such as identifying the onset of anomalous
activity or incipient congestion in the Internet, or faint traces of a
biochemical spread by a sensor network. This is a challenging problem since
weak distributed patterns can be invisible in per node statistics as well as a
global network-wide aggregate. Most prior work considers situations in which
the activation/non-activation of each node is statistically independent, but
this is unrealistic in many problems. In this paper, we consider structured
patterns arising from statistical dependencies in the activation process. Our
contributions are three-fold. First, we propose a sparsifying transform that
succinctly represents structured activation patterns that conform to a
hierarchical dependency graph. Second, we establish that the proposed transform
facilitates detection of very weak activation patterns that cannot be detected
with existing methods. Third, we show that the structure of the hierarchical
dependency graph governing the activation process, and hence the network
transform, can be learnt from very few (logarithmic in network size)
independent snapshots of network activity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0243</identifier>
 <datestamp>2010-03-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0243</id><created>2010-02-28</created><authors><author><keyname>Ambler</keyname><forenames>Graeme K.</forenames></author><author><keyname>Silverman</keyname><forenames>Bernard W.</forenames></author></authors><title>Perfect simulation using dominated coupling from the past with
  application to area-interaction point processes and wavelet thresholding</title><categories>stat.ME stat.CO</categories><comments>27 pages, 8 figures. Chapter 3 of "Probability and Mathematical
  Genetics: Papers in Honour of Sir John Kingman" (Editors N.H. Bingham and
  C.M. Goldie), Cambridge University Press, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider perfect simulation algorithms for locally stable point processes
based on dominated coupling from the past, and apply these methods in two
different contexts. A new version of the algorithm is developed which is
feasible for processes which are neither purely attractive nor purely
repulsive. Such processes include multiscale area-interaction processes, which
are capable of modelling point patterns whose clustering structure varies
across scales. The other topic considered is nonparametric regression using
wavelets, where we use a suitable area-interaction process on the discrete
space of indices of wavelet coefficients to model the notion that if one
wavelet coefficient is non-zero then it is more likely that neighbouring
coefficients will be also. A method based on perfect simulation within this
model shows promising results compared to the standard methods which threshold
coefficients independently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0248</identifier>
 <datestamp>2010-10-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0248</id><created>2010-02-28</created><updated>2010-10-06</updated><authors><author><keyname>Giacomelli</keyname><forenames>Riccardo</forenames></author><author><keyname>Ganti</keyname><forenames>Radha Krishna</forenames></author><author><keyname>Haenggi</keyname><forenames>Martin</forenames></author></authors><title>Outage Probability of General Ad Hoc Networks in the High-Reliability
  Regime</title><categories>cs.IT cs.NI math.IT math.ST stat.TH</categories><comments>Submitted to IEEE Transactions on Networking (Revision 2)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Outage probabilities in wireless networks depend on various factors: the node
distribution, the MAC scheme, and the models for path loss, fading and
transmission success. In prior work on outage characterization for networks
with randomly placed nodes, most of the emphasis was put on networks whose
nodes are Poisson distributed and where ALOHA is used as the MAC protocol. In
this paper we provide a general framework for the analysis of outage
probabilities in the high-reliability regime. The outage probability
characterization is based on two parameters: the intrinsic spatial contention
$\gamma$ of the network, introduced in [1], and the coordination level achieved
by the MAC as measured by the interference scaling exponent $\kappa$ introduced
in this paper. We study outage probabilities under the signal-to-interference
ratio (SIR) model, Rayleigh fading, and power-law path loss, and explain how
the two parameters depend on the network model. The main result is that the
outage probability approaches $\gamma\eta^{\kappa}$ as the density of
interferers $\eta$ goes to zero, and that $\kappa$ assumes values in the range
$1\leq \kappa\leq \alpha/2$ for all practical MAC protocols, where $\alpha$ is
the path loss exponent. This asymptotic expression is valid for all
motion-invariant point processes. We suggest a novel and complete taxonomy of
MAC protocols based mainly on the value of $\kappa$. Finally, our findings
suggest a conjecture that tightly bounds the outage probability for all
interferer densities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0261</identifier>
 <datestamp>2010-03-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0261</id><created>2010-03-01</created><authors><author><keyname>Jiang</keyname><forenames>Ci-Ren</forenames></author><author><keyname>Wang</keyname><forenames>Jane-Ling</forenames></author></authors><title>Covariate adjusted functional principal components analysis for
  longitudinal data</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/09-AOS742 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS742</report-no><msc-class>62H25, 62M15 (Primary) 62G20 (Secondary)</msc-class><journal-ref>Annals of Statistics 2010, Vol. 38, No. 2, 1194-1226</journal-ref><doi>10.1214/09-AOS742</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical multivariate principal component analysis has been extended to
functional data and termed functional principal component analysis (FPCA). Most
existing FPCA approaches do not accommodate covariate information, and it is
the goal of this paper to develop two methods that do. In the first approach,
both the mean and covariance functions depend on the covariate $Z$ and time
scale $t$ while in the second approach only the mean function depends on the
covariate $Z$. Both new approaches accommodate additional measurement errors
and functional data sampled at regular time grids as well as sparse
longitudinal data sampled at irregular time grids. The first approach to fully
adjust both the mean and covariance functions adapts more to the data but is
computationally more intensive than the approach to adjust the covariate
effects on the mean function only. We develop general asymptotic theory for
both approaches and compare their performance numerically through simulation
studies and a data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0275</identifier>
 <datestamp>2012-01-31</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0275</id><created>2010-03-01</created><updated>2012-01-30</updated><authors><author><keyname>Belomestny</keyname><forenames>Denis</forenames></author></authors><title>Statistical inference for time-changed L\'{e}vy processes via composite
  characteristic function estimation</title><categories>stat.ME math.ST stat.AP stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/11-AOS901 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS901</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 4, 2205-2242</journal-ref><doi>10.1214/11-AOS901</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, the problem of semi-parametric inference on the parameters
of a multidimensional L\'{e}vy process $L_t$ with independent components based
on the low-frequency observations of the corresponding time-changed L\'{e}vy
process $L_{\mathcal{T}(t)}$, where $\mathcal{T}$ is a nonnegative,
nondecreasing real-valued process independent of $L_t$, is studied. We show
that this problem is closely related to the problem of composite function
estimation that has recently gotten much attention in statistical literature.
Under suitable identifiability conditions, we propose a consistent estimate for
the L\'{e}vy density of $L_t$ and derive the uniform as well as the pointwise
convergence rates of the estimate proposed. Moreover, we prove that the rates
obtained are optimal in a minimax sense over suitable classes of time-changed
L\'{e}vy models. Finally, we present a simulation study showing the performance
of our estimation algorithm in the case of time-changed Normal Inverse Gaussian
(NIG) L\'{e}vy processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0315</identifier>
 <datestamp>2010-03-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0315</id><created>2010-03-01</created><authors><author><keyname>Delaigle</keyname><forenames>Aurore</forenames></author><author><keyname>Hall</keyname><forenames>Peter</forenames></author></authors><title>Kernel methods and minimum contrast estimators for empirical
  deconvolution</title><categories>stat.ME math.ST stat.TH</categories><comments>To appear in: Bingham, N. H., and Goldie, C. M. (eds), Probability
  and Mathematical Genetics: Papers in Honour of Sir John Kingman. London Math.
  Soc. Lecture Note Ser. Cambridge: Cambridge Univ. Press, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey classical kernel methods for providing nonparametric solutions to
problems involving measurement error. In particular we outline kernel-based
methodology in this setting, and discuss its basic properties. Then we point to
close connections that exist between kernel methods and much newer approaches
based on minimum contrast techniques. The connections are through use of the
sinc kernel for kernel-based inference. This `infinite order' kernel is not
often used explicitly for kernel-based deconvolution, although it has received
attention in more conventional problems where measurement error is not an
issue. We show that in a comparison between kernel methods for density
deconvolution, and their counterparts based on minimum contrast, the two
approaches give identical results on a grid which becomes increasingly fine as
the bandwidth decreases. In consequence, the main numerical differences between
these two techniques are arguably the result of different approaches to
choosing smoothing parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0428</identifier>
 <datestamp>2011-04-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0428</id><created>2010-03-01</created><updated>2011-04-18</updated><authors><author><keyname>Chopin</keyname><forenames>Nicolas</forenames><affiliation>CREST/Ensae</affiliation></author><author><keyname>Lelievre</keyname><forenames>Tony</forenames><affiliation>CERMICS/Ecole des Ponts and Micmac, Inria</affiliation></author><author><keyname>Stoltz</keyname><forenames>Gabriel</forenames><affiliation>CERMICS/Ecole des Ponts and Micmac, Inria</affiliation></author></authors><title>Free Energy Methods for Bayesian Inference: Efficient Exploration of
  Univariate Gaussian Mixture Posteriors</title><categories>stat.CO math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Because of their multimodality, mixture posterior distributions are difficult
to sample with standard Markov chain Monte Carlo (MCMC) methods. We propose a
strategy to enhance the sampling of MCMC in this context, using a biasing
procedure which originates from computational Statistical Physics. The
principle is first to choose a "reaction coordinate", that is, a "direction" in
which the target distribution is multimodal. In a second step, the marginal
log-density of the reaction coordinate with respect to the posterior
distribution is estimated; minus this quantity is called "free energy" in the
computational Statistical Physics literature. To this end, we use adaptive
biasing Markov chain algorithms which adapt their targeted invariant
distribution on the fly, in order to overcome sampling barriers along the
chosen reaction coordinate. Finally, we perform an importance sampling step in
order to remove the bias and recover the true posterior. The efficiency factor
of the importance sampling step can easily be estimated \emph{a priori} once
the bias is known, and appears to be rather large for the test cases we
considered. A crucial point is the choice of the reaction coordinate. One
standard choice (used for example in the classical Wang-Landau algorithm) is
minus the log-posterior density. We discuss other choices. We show in
particular that the hyper-parameter that determines the order of magnitude of
the variance of each component is both a convenient and an efficient reaction
coordinate. We also show how to adapt the method to compute the evidence
(marginal likelihood) of a mixture model. We illustrate our approach by
analyzing two real data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0747</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0747</id><created>2010-03-03</created><updated>2013-04-20</updated><authors><author><keyname>Neuvial</keyname><forenames>Pierre</forenames><affiliation>LPMA, SG</affiliation></author></authors><title>Asymptotic Results on Adaptive False Discovery Rate Controlling
  Procedures Based on Kernel Estimators</title><categories>math.ST physics.data-an q-bio.QM stat.AP stat.ME stat.TH</categories><proxy>ccsd</proxy><journal-ref>Journal of Machine Learning Research 14 (2013) 1423-1459</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The False Discovery Rate (FDR) is a commonly used type I error rate in
multiple testing problems. It is defined as the expected False Discovery
Proportion (FDP), that is, the expected fraction of false positives among
rejected hypotheses. When the hypotheses are independent, the
Benjamini-Hochberg procedure achieves FDR control at any pre-specified level.
By construction, FDR control offers no guarantee in terms of power, or type II
error. A number of alternative procedures have been developed, including
plug-in procedures that aim at gaining power by incorporating an estimate of
the proportion of true null hypotheses. In this paper, we study the asymptotic
behavior of a class of plug-in procedures based on kernel estimators of the
density of the $p$-values, as the number $m$ of tested hypotheses grows to
infinity. In a setting where the hypotheses tested are independent, we prove
that these procedures are asymptotically more powerful in two respects: (i) a
tighter asymptotic FDR control for any target FDR level and (ii) a broader
range of target levels yielding positive asymptotic power. We also show that
this increased asymptotic power comes at the price of slower, non-parametric
convergence rates for the FDP. These rates are of the form $m^{-k/(2k+1)}$,
where $k$ is determined by the regularity of the density of the $p$-value
distribution, or, equivalently, of the test statistics distribution. These
results are applied to one- and two-sided tests statistics for Gaussian and
Laplace location models, and for the Student model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0783</identifier>
 <datestamp>2010-03-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0783</id><created>2010-03-03</created><authors><author><keyname>Blei</keyname><forenames>David M.</forenames></author><author><keyname>McAuliffe</keyname><forenames>Jon D.</forenames></author></authors><title>Supervised Topic Models</title><categories>stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce supervised latent Dirichlet allocation (sLDA), a statistical
model of labelled documents. The model accommodates a variety of response
types. We derive an approximate maximum-likelihood procedure for parameter
estimation, which relies on variational methods to handle intractable posterior
expectations. Prediction problems motivate this research: we use the fitted
model to predict response values for new documents. We test sLDA on two
real-world problems: movie ratings predicted from reviews, and the political
tone of amendments in the U.S. Senate based on the amendment text. We
illustrate the benefits of sLDA versus modern regularized regression, as well
as versus an unsupervised LDA analysis followed by a separate regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0804</identifier>
 <datestamp>2010-03-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0804</id><created>2010-03-03</created><authors><author><keyname>Franey</keyname><forenames>Mark</forenames></author><author><keyname>Ranjan</keyname><forenames>Pritam</forenames></author><author><keyname>Chipman</keyname><forenames>Hugh</forenames></author></authors><title>Branch and Bound Algorithms for Maximizing Expected Improvement
  Functions</title><categories>stat.ME stat.CO</categories><comments>26 pages, 14 figures, preprint submitted to the Journal of
  Statistical Planning and Inference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deterministic computer simulations are often used as a replacement for
complex physical experiments. Although less expensive than physical
experimentation, computer codes can still be time-consuming to run. An
effective strategy for exploring the response surface of the deterministic
simulator is the use of an approximation to the computer code, such as a
Gaussian process (GP) model, coupled with a sequential sampling strategy for
choosing design points that can be used to build the GP model. The ultimate
goal of such studies is often the estimation of specific features of interest
of the simulator output, such as the maximum, minimum, or a level set
(contour). Before approximating such features with the GP model, sufficient
runs of the computer simulator must be completed.
  Sequential designs with an expected improvement (EI) function can yield good
estimates of the features with a minimal number of runs. The challenge is that
the expected improvement function itself is often multimodal and difficult to
maximize. We develop branch and bound algorithms for efficiently maximizing the
EI function in specific problems, including the simultaneous estimation of a
minimum and a maximum, and in the estimation of a contour. These branch and
bound algorithms outperform other optimization strategies such as genetic
algorithms, and over a number of sequential design steps can lead to
dramatically superior accuracy in estimation of features of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0848</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0848</id><created>2010-03-03</created><updated>2013-04-02</updated><authors><author><keyname>Hansen</keyname><forenames>Niels Richard</forenames></author></authors><title>Penalized maximum likelihood estimation for generalized linear point
  processes</title><categories>math.ST math.PR stat.ME stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A generalized linear point process is specified in terms of an intensity that
depends upon a linear predictor process through a fixed non-linear function. We
present a framework where the linear predictor is parametrized by a Banach
space and give results on Gateaux differentiability of the log-likelihood. Of
particular interest is when the intensity is expressed in terms of a linear
filter parametrized by a Sobolev space. Using that the Sobolev spaces are
reproducing kernel Hilbert spaces we derive results on the representation of
the penalized maximum likelihood estimator in a special case and the gradient
of the negative log-likelihood in general. The latter is used to develop a
descent algorithm in the Sobolev space. We conclude the paper by extensions to
multivariate and additive model specifications. The methods are implemented in
the R-package ppstat.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0887</identifier>
 <datestamp>2010-03-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0887</id><created>2010-03-03</created><authors><author><keyname>Sriperumbudur</keyname><forenames>Bharath K.</forenames></author><author><keyname>Fukumizu</keyname><forenames>Kenji</forenames></author><author><keyname>Lanckriet</keyname><forenames>Gert R. G.</forenames></author></authors><title>Universality, Characteristic Kernels and RKHS Embedding of Measures</title><categories>stat.ML math.ST stat.TH</categories><comments>30 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Hilbert space embedding for probability measures has recently been
proposed, wherein any probability measure is represented as a mean element in a
reproducing kernel Hilbert space (RKHS). Such an embedding has found
applications in homogeneity testing, independence testing, dimensionality
reduction, etc., with the requirement that the reproducing kernel is
characteristic, i.e., the embedding is injective.
  In this paper, we generalize this embedding to finite signed Borel measures,
wherein any finite signed Borel measure is represented as a mean element in an
RKHS. We show that the proposed embedding is injective if and only if the
kernel is universal. This therefore, provides a novel characterization of
universal kernels, which are proposed in the context of achieving the Bayes
risk by kernel-based classification/regression algorithms. By exploiting this
relation between universality and the embedding of finite signed Borel measures
into an RKHS, we establish the relation between universal and characteristic
kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.0996</identifier>
 <datestamp>2011-01-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.0996</id><created>2010-03-04</created><updated>2011-01-10</updated><authors><author><keyname>Lau</keyname><forenames>Ada</forenames></author><author><keyname>McSharry</keyname><forenames>Patrick</forenames></author></authors><title>Approaches for multi-step density forecasts with application to
  aggregated wind power</title><categories>stat.AP physics.ao-ph stat.ME</categories><comments>Corrected version includes updated equation (18). Published in at
  http://dx.doi.org/10.1214/09-AOAS320 the Annals of Applied Statistics
  (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics
  (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS320</report-no><journal-ref>Annals of Applied Statistics 2010, Vol. 4, No. 3, 1311-1341</journal-ref><doi>10.1214/09-AOAS320</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generation of multi-step density forecasts for non-Gaussian data mostly
relies on Monte Carlo simulations which are computationally intensive. Using
aggregated wind power in Ireland, we study two approaches of multi-step density
forecasts which can be obtained from simple iterations so that intensive
computations are avoided. In the first approach, we apply a logistic
transformation to normalize the data approximately and describe the transformed
data using ARIMA--GARCH models so that multi-step forecasts can be iterated
easily. In the second approach, we describe the forecast densities by truncated
normal distributions which are governed by two parameters, namely, the
conditional mean and conditional variance. We apply exponential smoothing
methods to forecast the two parameters simultaneously. Since the underlying
model of exponential smoothing is Gaussian, we are able to obtain multi-step
forecasts of the parameters by simple iterations and thus generate forecast
densities as truncated normal distributions. We generate forecasts for wind
power from 15 minutes to 24 hours ahead. Results show that the first approach
generates superior forecasts and slightly outperforms the second approach under
various proper scores. Nevertheless, the second approach is computationally
more efficient and gives more robust results under different lengths of
training data. It also provides an attractive alternative approach since one is
allowed to choose a particular parametric density for the forecasts, and is
valuable when there are no obvious transformations to normalize the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.1002</identifier>
 <datestamp>2010-03-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.1002</id><created>2010-03-04</created><authors><author><keyname>Commenges</keyname><forenames>Daniel</forenames></author></authors><title>Extending The Range of Application of Permutation Tests: the Expected
  Permutation p-value Approach</title><categories>stat.ME</categories><comments>15 pages, 1 table</comments><msc-class>62F03;62F40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The limitation of permutation tests is that they assume exchangeability. It
is shown that in generalized linear models one can construct permutation tests
from score statistics in particular cases. When under the null hypothesis the
observations are not exchangeable, a representation in terms of Cox-Snell
residuals allows to develop an approach based on an expected permutation
p-value (Eppv); this is applied to the logistic regression model. A small
simulation study and an illustration with real data are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.1018</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.1018</id><created>2010-03-04</created><authors><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Zipf's law and log-normal distributions in measures of scientific output
  across fields and institutions: 40 years of Slovenia's research as an example</title><categories>physics.data-an cs.DB stat.AP</categories><comments>8 pages, 3 figures; accepted for publication in Journal of
  Informetrics [supplementary material available at
  http://www.matjazperc.com/sicris/stats.html]</comments><journal-ref>Journal of Informetrics 4 (2010) 358-364</journal-ref><doi>10.1016/j.joi.2010.03.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Slovenia's Current Research Information System (SICRIS) currently hosts
86,443 publications with citation data from 8,359 researchers working on the
whole plethora of social and natural sciences from 1970 till present. Using
these data, we show that the citation distributions derived from individual
publications have Zipfian properties in that they can be fitted by a power law
$P(x) \sim x^{-\alpha}$, with $\alpha$ between 2.4 and 3.1 depending on the
institution and field of research. Distributions of indexes that quantify the
success of researchers rather than individual publications, on the other hand,
cannot be associated with a power law. We find that for Egghe's g-index and
Hirsch's h-index the log-normal form $P(x) \sim \exp[-a\ln x -b(\ln x)^2]$
applies best, with $a$ and $b$ depending moderately on the underlying set of
researchers. In special cases, particularly for institutions with a strongly
hierarchical constitution and research fields with high self-citation rates,
exponential distributions can be observed as well. Both indexes yield
distributions with equivalent statistical properties, which is a strong
indicator for their consistency and logical connectedness. At the same time,
differences in the assessment of citation histories of individual researchers
strengthen their importance for properly evaluating the quality and impact of
scientific output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.1146</identifier>
 <datestamp>2011-05-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.1146</id><created>2010-03-04</created><updated>2011-05-13</updated><authors><author><keyname>Drton</keyname><forenames>Mathias</forenames></author><author><keyname>Foygel</keyname><forenames>Rina</forenames></author><author><keyname>Sullivant</keyname><forenames>Seth</forenames></author></authors><title>Global identifiability of linear structural equation models</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AOS859 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS859</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 2, 865-886</journal-ref><doi>10.1214/10-AOS859</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structural equation models are multivariate statistical models that are
defined by specifying noisy functional relationships among random variables. We
consider the classical case of linear relationships and additive Gaussian noise
terms. We give a necessary and sufficient condition for global identifiability
of the model in terms of a mixed graph encoding the linear structural equations
and the correlation structure of the error terms. Global identifiability is
understood to mean injectivity of the parametrization of the model and is
fundamental in particular for applicability of standard statistical
methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.1170</identifier>
 <datestamp>2010-03-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.1170</id><created>2010-03-04</created><authors><author><keyname>Hartigan</keyname><forenames>J. A.</forenames></author></authors><title>Asymptotic admissibility of priors and elliptic differential equations</title><categories>math.ST stat.TH</categories><comments>3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We evaluate priors by the second order asymptotic behavior of the
corresponding estimators.Under certain regularity conditions, the risk
differences between efficient estimators of parameters taking values in a
domain D, an open connected subset of R^d, are asymptotically expressed as
elliptic differential forms depending on the asymptotic covariance matrix V.
Each efficient estimator has the same asymptotic risk as a 'local Bayes'
estimate corresponding to a prior density p. The asymptotic decision theory of
the estimators identifies the smooth prior densities as admissible or
inadmissible, according to the existence of solutions to certain elliptic
differential equations. The prior p is admissible if the quantity pV is
sufficiently small near the boundary of D. We exhibit the unique admissible
invariant prior for V=I,D=R^d-{0). A detailed example is given for a normal
mixture model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.1189</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.1189</id><created>2010-03-05</created><updated>2012-08-17</updated><authors><author><keyname>Dalalyan</keyname><forenames>Arnak S.</forenames><affiliation>LIGM, CREST</affiliation></author><author><keyname>Tsybakov</keyname><forenames>Alexandre B.</forenames><affiliation>CREST, LPMA</affiliation></author></authors><title>Mirror averaging with sparsity priors</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/11-BEJ361 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>ccsd</proxy><report-no>IMS-BEJ-BEJ361</report-no><journal-ref>Bernoulli 18, 3 (2012) 914-944</journal-ref><doi>10.3150/11-BEJ361</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of aggregating the elements of a possibly infinite
dictionary for building a decision procedure that aims at minimizing a given
criterion. Along with the dictionary, an independent identically distributed
training sample is available, on which the performance of a given procedure can
be tested. In a fairly general set-up, we establish an oracle inequality for
the Mirror Averaging aggregate with any prior distribution. By choosing an
appropriate prior, we apply this oracle inequality in the context of prediction
under sparsity assumption for the problems of regression with random design,
density estimation and binary classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.1315</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.1315</id><created>2010-03-05</created><updated>2012-03-06</updated><authors><author><keyname>Ranjan</keyname><forenames>Pritam</forenames></author><author><keyname>Haynes</keyname><forenames>Ronald</forenames></author><author><keyname>Karsten</keyname><forenames>Richard</forenames></author></authors><title>A Computationally Stable Approach to Gaussian Process Interpolation of
  Deterministic Computer Simulation Data</title><categories>stat.ME stat.CO</categories><comments>26 pages, 12 figures</comments><journal-ref>Technometrics, 2011, 53(4), 366-378</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many expensive deterministic computer simulators, the outputs do not have
replication error and the desired metamodel (or statistical emulator) is an
interpolator of the observed data. Realizations of Gaussian spatial processes
(GP) are commonly used to model such simulator outputs. Fitting a GP model to
$n$ data points requires the computation of the inverse and determinant of $n
\times n$ correlation matrices, $R$, that are sometimes computationally
unstable due to near-singularity of $R$. This happens if any pair of design
points are very close together in the input space. The popular approach to
overcome near-singularity is to introduce a small nugget (or jitter) parameter
in the model that is estimated along with other model parameters. The inclusion
of a nugget in the model often causes unnecessary over-smoothing of the data.
In this paper, we propose a lower bound on the nugget that minimizes the
over-smoothing and an iterative regularization approach to construct a
predictor that further improves the interpolation accuracy. We also show that
the proposed predictor converges to the GP interpolator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.1325</identifier>
 <datestamp>2010-03-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.1325</id><created>2010-03-05</created><authors><author><keyname>Lora</keyname><forenames>Mayra Ivanoff</forenames></author><author><keyname>Singer</keyname><forenames>Julio M</forenames></author></authors><title>Beta-binomial/gamma-Poisson regression models for repeated counts with
  random parameters</title><categories>stat.ME stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Beta-binomial/Poisson models have been used by many authors to model
multivariate count data. Lora and Singer (Statistics in Medicine, 2008)
extended such models to accommodate repeated multivariate count data with
overdipersion in the binomial component. To overcome some of the limitations of
that model, we consider a beta-binomial/gamma-Poisson alternative that also
allows for both overdispersion and different covariances between the Poisson
counts. We obtain maximum likelihood estimates for the parameters using a
Newton-Raphson algorithm and compare both models in a practical example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.1513</identifier>
 <datestamp>2010-03-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.1513</id><created>2010-03-07</created><authors><author><keyname>Ritov</keyname><forenames>Ya'acov</forenames></author></authors><title>On the trasductive arguments in statistics</title><categories>math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper argues that a part of the current statistical discussion is not
based on the standard firm foundations of the field. Among the examples we
consider are prediction into the future, semi-supervised classification, and
causality inference based on observational data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.1535</identifier>
 <datestamp>2010-03-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.1535</id><created>2010-03-07</created><authors><author><keyname>Wishart</keyname><forenames>Justin</forenames></author><author><keyname>Kulik</keyname><forenames>Rafal</forenames></author></authors><title>Kink estimation in stochastic regression with dependent errors and
  predictors</title><categories>math.ST stat.TH</categories><comments>35 pages</comments><msc-class>62G08; 62G05; 62G20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we study the estimation of the location of jump points in the
first derivative (referred to as kinks) of a regression function \mu in two
random design models with different long-range dependent (LRD) structures. The
method is based on the zero-crossing technique and makes use of high-order
kernels. The rate of convergence of the estimator is contingent on the level of
dependence and the smoothness of the regression function \mu. In one of the
models, the convergence rate is the same as the minimax rate for kink
estimation in the fixed design scenario with i.i.d. errors which suggests that
the method is optimal in the minimax sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.1573</identifier>
 <datestamp>2010-03-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.1573</id><created>2010-03-08</created><authors><author><keyname>Gonzalez-Manteiga</keyname><forenames>Wenceslao</forenames></author><author><keyname>Henry</keyname><forenames>Guillermo</forenames></author><author><keyname>Rodriguez</keyname><forenames>Daniela</forenames></author></authors><title>Partially linear models on Riemannian manifolds</title><categories>math.ST stat.TH</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In partially linear models the dependence of the response y on (x^T,t) is
modeled through the relationship y=\x^T \beta+g(t)+\epsilon where \epsilon is
independent of (x^T,t). In this paper, estimators of \beta and g are
constructed when the explanatory variables t take values on a Riemannian
manifold. Our proposal combine the flexibility of these models with the complex
structure of a set of explanatory variables. We prove that the resulting
estimator of \beta is asymptotically normal under the suitable conditions.
Through a simulation study, we explored the performance of the estimators.
Finally, we applied the studied model to an example based on real dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.1630</identifier>
 <datestamp>2010-03-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.1630</id><created>2010-03-08</created><authors><author><keyname>Rigollet</keyname><forenames>Philippe</forenames></author><author><keyname>Zeevi</keyname><forenames>Assaf</forenames></author></authors><title>Nonparametric Bandits with Covariates</title><categories>math.ST stat.TH</categories><msc-class>Primary 62G08. Secondary 62L12, 62L05, 62C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a bandit problem which involves sequential sampling from two
populations (arms). Each arm produces a noisy reward realization which depends
on an observable random covariate. The goal is to maximize cumulative expected
reward. We derive general lower bounds on the performance of any admissible
policy, and develop an algorithm whose performance achieves the order of said
lower bound up to logarithmic terms. This is done by decomposing the global
problem into suitably "localized" bandit problems. Proofs blend ideas from
nonparametric statistics and traditional methods used in the bandit literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.1727</identifier>
 <datestamp>2010-03-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.1727</id><created>2010-03-08</created><authors><author><keyname>Barreto-Souza</keyname><forenames>Wagner</forenames></author><author><keyname>Simas</keyname><forenames>Alexandre B.</forenames></author></authors><title>The exp-$G$ family of probability distributions</title><categories>stat.ME stat.AP</categories><comments>A much improved version of the pioneering manuscript "A new family of
  distributions based on the trucanted exponential distribution" presented at
  the 18 SINAPE (Simp\'osio Nacional de Probabilidade e Estat\'istica), 2008,
  S\~ao Paulo, Brazil.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a new method to add a parameter to a family of
distributions. The additional parameter is completely studied and a full
description of its behaviour in the distribution is given. We obtain several
mathematical properties of the new class of distributions such as
Kullback-Leibler divergence, Shannon entropy, moments, order statistics,
estimation of the parameters and inference for large sample. Further, we showed
that the new distribution have the reference distribution as special case, and
that the usual inference procedures also hold in this case. Furthermore, we
applied our method to yield three-parameter extensions of the Weibull and beta
distributions. To motivate the use of our class of distributions, we present a
successful application to fatigue life data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.1771</identifier>
 <datestamp>2010-03-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.1771</id><created>2010-03-08</created><authors><author><keyname>Mandel</keyname><forenames>Jan</forenames></author><author><keyname>Beezley</keyname><forenames>Jonathan D.</forenames></author><author><keyname>Cobb</keyname><forenames>Loren</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Ashok</forenames></author></authors><title>Data Driven Computing by the Morphing Fast Fourier Transform Ensemble
  Kalman Filter in Epidemic Spread Simulations</title><categories>stat.CO q-bio.QM</categories><comments>11 pages, 3 figures. Submitted to ICCS 2010</comments><report-no>UCD CCM Report 286</report-no><msc-class>60G35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The FFT EnKF data assimilation method is proposed and applied to a stochastic
cell simulation of an epidemic, based on the S-I-R spread model. The FFT EnKF
combines spatial statistics and ensemble filtering methodologies into a
localized and computationally inexpensive version of EnKF with a very small
ensemble, and it is further combined with the morphing EnKF to assimilate
changes in the position of the epidemic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.1950</identifier>
 <datestamp>2010-03-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.1950</id><created>2010-03-09</created><authors><author><keyname>Ridder</keyname><forenames>Ad</forenames></author></authors><title>Asymptotic optimality of the cross-entropy method for Markov chain
  problems</title><categories>math.PR stat.CO</categories><comments>13 pager; 3 figures</comments><msc-class>60J22, 65C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The correspondence between the cross-entropy method and the zero-variance
approximation to simulate a rare event problem in Markov chains is shown. This
leads to a sufficient condition that the cross-entropy estimator is
asymptotically optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.1954</identifier>
 <datestamp>2010-10-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.1954</id><created>2010-03-09</created><updated>2010-10-25</updated><authors><author><keyname>Pl</keyname><forenames>Dvid</forenames></author><author><keyname>Pczos</keyname><forenames>Barnabs</forenames></author><author><keyname>Szepesvri</keyname><forenames>Csaba</forenames></author></authors><title>Estimation of R\'enyi Entropy and Mutual Information Based on
  Generalized Nearest-Neighbor Graphs</title><categories>stat.ML cs.AI</categories><comments>to appear at NIPS 2010 (Neural Information Processing Systems)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present simple and computationally efficient nonparametric estimators of
R\'enyi entropy and mutual information based on an i.i.d. sample drawn from an
unknown, absolutely continuous distribution over $\R^d$. The estimators are
calculated as the sum of $p$-th powers of the Euclidean lengths of the edges of
the `generalized nearest-neighbor' graph of the sample and the empirical copula
of the sample respectively. For the first time, we prove the almost sure
consistency of these estimators and upper bounds on their rates of convergence,
the latter of which under the assumption that the density underlying the sample
is Lipschitz continuous. Experiments demonstrate their usefulness in
independent subspace analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.2245</identifier>
 <datestamp>2010-03-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.2245</id><created>2010-03-10</created><authors><author><keyname>Agarwal</keyname><forenames>Alekh</forenames></author><author><keyname>Bartlett</keyname><forenames>Peter</forenames></author><author><keyname>Dama</keyname><forenames>Max</forenames></author></authors><title>Optimal Allocation Strategies for the Dark Pool Problem</title><categories>stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of allocating stocks to dark pools. We propose and
analyze an optimal approach for allocations, if continuous-valued allocations
are allowed. We also propose a modification for the case when only
integer-valued allocations are possible. We extend the previous work on this
problem to adversarial scenarios, while also improving on their results in the
iid setup. The resulting algorithms are efficient, and perform well in
simulations under stochastic and adversarial inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.2253</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.2253</id><created>2010-03-10</created><updated>2010-09-07</updated><authors><author><keyname>Westveld</keyname><forenames>Anton H.</forenames></author><author><keyname>Hoff</keyname><forenames>Peter D.</forenames></author></authors><title>A Statistical View of Learning in the Centipede Game</title><categories>stat.ME stat.AP</categories><journal-ref>Stat 2 (1), 242-254. 2013</journal-ref><doi>10.1002/sta4.32</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we evaluate the statistical evidence that a population of
students learn about the sub-game perfect Nash equilibrium of the centipede
game via repeated play of the game. This is done by formulating a model in
which a player's error in assessing the utility of decisions changes as they
gain experience with the game. We first estimate parameters in a statistical
model where the probabilities of choices of the players are given by a Quantal
Response Equilibrium (QRE) (McKelvey and Palfrey, 1995, 1996, 1998), but are
allowed to change with repeated play. This model gives a better fit to the data
than similar models previously considered. However, substantial correlation of
outcomes of games having a common player suggests that a statistical model that
captures within-subject correlation is more appropriate. Thus we then estimate
parameters in a model which allows for within-player correlation of decisions
and rates of learning. Through out the paper we also consider and compare the
use of randomization tests and posterior predictive tests in the context of
exploratory and confirmatory data analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.2289</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.2289</id><created>2010-03-11</created><updated>2012-03-02</updated><authors><author><keyname>Besal</keyname><forenames>Mireia</forenames></author><author><keyname>Rovira</keyname><forenames>Carles</forenames></author></authors><title>Stochastic delay equations with non-negativity constraints driven by
  fractional Brownian motion</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ327 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ327</report-no><journal-ref>Bernoulli 2012, Vol. 18, No. 1, 24-45</journal-ref><doi>10.3150/10-BEJ327</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we prove an existence and uniqueness result for the solution of
multidimensional stochastic delay differential equations with normal
reflection. The equations are driven by a fractional Brownian motion with Hurst
parameter $H&gt;1/2$. The stochastic integral with respect to the fractional
Brownian motion is a pathwise Riemann--Stieltjes integral.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.2294</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.2294</id><created>2010-03-11</created><updated>2010-04-25</updated><authors><author><keyname>Aubin</keyname><forenames>Jean-Baptiste</forenames></author><author><keyname>Leoni-Aubin</keyname><forenames>Samuela</forenames></author></authors><title>A Simple Lack-of-Fit Test for Regression Models</title><categories>stat.ME math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple test is proposed for examining the correctness of a given completely
specified response function against unspecified general alternatives in the
context of univariate regression. The usual diagnostic tools based on residuals
plots are useful but heuristic. We introduce a formal statistical test
supplementing the graphical analysis. Technically, the test statistic is the
maximum length of the sequences of ordered (with respect to the covariate)
observations that are consecutively overestimated or underestimated by the
candidate regression function. Note that the testing procedure can cope with
heteroscedastic errors and no replicates. Recursive formulae allowing to
calculate the exact distribution of the test statistic under the null
hypothesis and under a class of alternative hypotheses are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.2307</identifier>
 <datestamp>2010-03-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.2307</id><created>2010-03-11</created><authors><author><keyname>Wijnholds</keyname><forenames>Stefan J.</forenames></author><author><keyname>van der Veen</keyname><forenames>Alle-Jan</forenames></author></authors><title>Fundamental Imaging Limits of Radio Telescope Arrays</title><categories>astro-ph.IM physics.data-an stat.AP</categories><comments>12 pages, 8 figures</comments><journal-ref>IEEE Journal of Selected Topics in Signal Processing, vol. 2, no.
  5, pp613-623, October 2008</journal-ref><doi>10.1109/JSTSP.2008.2004216</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fidelity of radio astronomical images is generally assessed by practical
experience, i.e. using rules of thumb, although some aspects and cases have
been treated rigorously. In this paper we present a mathematical framework
capable of describing the fundamental limits of radio astronomical imaging
problems. Although the data model assumes a single snapshot observation, i.e.
variations in time and frequency are not considered, this framework is
sufficiently general to allow extension to synthesis observations. Using tools
from statistical signal processing and linear algebra, we discuss the
tractability of the imaging and deconvolution problem, the redistribution of
noise in the map by the imaging and deconvolution process, the covariance of
the image values due to propagation of calibration errors and thermal noise and
the upper limit on the number of sources tractable by self calibration. The
combination of covariance of the image values and the number of tractable
sources determines the effective noise floor achievable in the imaging process.
The effective noise provides a better figure of merit than dynamic range since
it includes the spatial variations of the noise. Our results provide handles
for improving the imaging performance by design of the array.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.2321</identifier>
 <datestamp>2010-03-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.2321</id><created>2010-03-11</created><authors><author><keyname>Aoyama</keyname><forenames>Hideaki</forenames></author><author><keyname>Fujiwara</keyname><forenames>Yoshi</forenames></author><author><keyname>Gallegati</keyname><forenames>Mauro</forenames></author></authors><title>Micro-Macro Relation of Production - The Double Scaling Law for
  Statistical Physics of Economy -</title><categories>q-fin.GN stat.AP</categories><comments>5 pages with 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that an economic system populated by multiple agents generates an
equilibrium distribution in the form of multiple scaling laws of conditional
PDFs, which are sufficient for characterizing the probability distribution. The
existence of the double scaling law is demonstrated empirically for the sales
and the labor of one million Japanese firms. Theoretical study of the scaling
laws suggests lognormal joint distributions of sales and labor and a scaling
law for labor productivity, both of which are confirmed empirically. This
framework offers characterization of the equilibrium distribution with a small
number of scaling indices, which determine macroscopic quantities, thus setting
the stage for an equivalence with statistical physics, bridging micro- and
macro-economics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.2390</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.2390</id><created>2010-03-11</created><updated>2012-02-29</updated><authors><author><keyname>Shahbaba</keyname><forenames>Babak</forenames></author></authors><title>Bayesian Nonparametric Variable Selection as an Exploratory Tool for
  Finding Genes that Matter</title><categories>stat.ME math.ST q-bio.QM stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-throughput scientific studies involving no clear a'priori hypothesis are
common. For example, a large-scale genomic study of a disease may examine
thousands of genes without hypothesizing that any specific gene is responsible
for the disease. In these studies, the objective is to explore a large number
of possible factors (e.g. genes) in order to identify a small number that will
be considered in follow-up studies that tend to be more thorough and on smaller
scales. For large-scale studies, we propose a nonparametric Bayesian approach
based on random partition models. Our model thus divides the set of candidate
factors into several subgroups according to their degrees of relevance, or
potential effect, in relation to the outcome of interest. The model allows for
a latent rank to be assigned to each factor according to the overall potential
importance of its corresponding group. The posterior expectation or mode of
these ranks is used to set up a threshold for selecting potentially relevant
factors. Using simulated data, we demonstrate that our approach could be quite
effective in finding relevant genes compared to several alternative methods. We
apply our model to two large-scale studies. The first study involves
transcriptome analysis of infection by human cytomegalovirus (HCMV). The
objective of the second study is to identify differentially expressed genes
between two types of leukemia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.2469</identifier>
 <datestamp>2010-03-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.2469</id><created>2010-03-11</created><authors><author><keyname>Romero</keyname><forenames>Daniel M.</forenames></author><author><keyname>Kleinberg</keyname><forenames>Jon</forenames></author></authors><title>The Directed Closure Process in Hybrid Social-Information Networks, with
  an Analysis of Link Formation on Twitter</title><categories>stat.ML cs.CY physics.soc-ph stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has often been taken as a working assumption that directed links in
information networks are frequently formed by "short-cutting" a two-step path
between the source and the destination -- a kind of implicit "link copying"
analogous to the process of triadic closure in social networks. Despite the
role of this assumption in theoretical models such as preferential attachment,
it has received very little direct empirical investigation. Here we develop a
formalization and methodology for studying this type of directed closure
process, and we provide evidence for its important role in the formation of
links on Twitter. We then analyze a sequence of models designed to capture the
structural phenomena related to directed closure that we observe in the Twitter
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.2497</identifier>
 <datestamp>2010-03-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.2497</id><created>2010-03-12</created><authors><author><keyname>Wijnholds</keyname><forenames>Stefan J.</forenames></author><author><keyname>van der Veen</keyname><forenames>Alle-Jan</forenames></author></authors><title>Self-Calibration of Radio Astronomical Arrays With Non-Diagonal Noise
  Covariance Matrix</title><categories>astro-ph.IM physics.data-an stat.AP</categories><comments>5 pages, 5 figures, conference paper</comments><journal-ref>Proceedings of the 17th European Signal Processing Conference
  (EuSiPCo), Glasgow (United Kingdom), 24-28 August 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The radio astronomy community is currently building a number of phased array
telescopes. The calibration of these telescopes is hampered by the fact that
covariances of signals from closely spaced antennas are sensitive to noise
coupling and to variations in sky brightness on large spatial scales. These
effects are difficult and computationally expensive to model. We propose to
model them phenomenologically using a non-diagonal noise covariance matrix. The
parameters can be estimated using a weighted alternating least squares (WALS)
algorithm iterating between the calibration parameters and the additive
nuisance parameters. We demonstrate the effectiveness of our method using data
from the low frequency array (LOFAR) prototype station.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.2556</identifier>
 <datestamp>2010-03-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.2556</id><created>2010-03-12</created><authors><author><keyname>Marichal</keyname><forenames>Jean-Luc</forenames></author><author><keyname>Mathonet</keyname><forenames>Pierre</forenames></author></authors><title>Measuring the influence of the k-th largest variable on functions over
  the unit hypercube</title><categories>math.OC math.ST stat.TH</categories><msc-class>41A10, 62G30, 93E24 (Primary) 39A70, 62G35 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By considering a least squares approximation of a given square integrable
function f:[0,1]^n --&gt; R by a shifted L-statistic function (a shifted linear
combination of order statistics), we define an index which measures the global
influence of the k-th largest variable on f. We show that this influence index
has appealing properties and we interpret it as an average value of the
difference quotient of f in the direction of the k-th largest variable or,
under certain natural conditions on f, as an average value of the derivative of
f in the direction of the k-th largest variable. We also discuss a few
applications of this index in statistics and aggregation theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.2619</identifier>
 <datestamp>2010-04-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.2619</id><created>2010-03-12</created><authors><author><keyname>Gelman</keyname><forenames>Andrew</forenames></author></authors><title>Causality and Statistical Learning</title><categories>math.ST stat.ME stat.TH</categories><comments>A version of this article will appear in the American Journal of
  Sociology.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review some approaches and philosophies of causal inference coming from
sociology, economics, computer science, cognitive science, and statistics
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.2711</identifier>
 <datestamp>2010-03-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.2711</id><created>2010-03-13</created><authors><author><keyname>Kuriki</keyname><forenames>Satoshi</forenames></author></authors><title>Distributions of the largest singular values of skew-symmetric random
  matrices and their applications to paired comparisons</title><categories>math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $A$ be a real skew-symmetric Gaussian random matrix whose upper
triangular elements are independently distributed according to the standard
normal distribution. We provide the distribution of the largest singular value
$\sigma_1$ of $A$. Moreover, by acknowledging the fact that the largest
singular value can be regarded as the maximum of a Gaussian field, we deduce
the distribution of the standardized largest singular value
$\sigma_1/\sqrt{\mathrm{tr}(A'A)/2}$. These distributional results are utilized
in Scheff\'{e}'s paired comparisons model. We propose tests for the hypothesis
of subtractivity based on the largest singular value of the skew-symmetric
residual matrix. Professional baseball league data are analyzed as an
illustrative example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.2804</identifier>
 <datestamp>2010-03-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.2804</id><created>2010-03-14</created><authors><author><keyname>Bartolucci</keyname><forenames>F.</forenames></author><author><keyname>Farcomeni</keyname><forenames>A.</forenames></author><author><keyname>Pennoni</keyname><forenames>F.</forenames></author></authors><title>An overview of latent Markov models for longitudinal categorical data</title><categories>math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a comprehensive overview of latent Markov (LM) models for the
analysis of longitudinal categorical data. The main assumption behind these
models is that the response variables are conditionally independent given a
latent process which follows a first-order Markov chain. We first illustrate
the basic LM model in which the conditional distribution of each response
variable given the corresponding latent variable and the initial and transition
probabilities of the latent process are unconstrained. For this model we also
illustrate in detail maximum likelihood estimation through the
Expectation-Maximization algorithm, which may be efficiently implemented by
recursions known in the hidden Markov literature. We then illustrate several
constrained versions of the basic LM model, which make the model more
parsimonious and allow us to include and test hypotheses of interest. These
constraints may be put on the conditional distribution of the response
variables given the latent process (measurement model) or on the distribution
of the latent process (latent model). We also deal with extensions of LM model
for the inclusion of individual covariates and to multilevel data. Covariates
may affect the measurement or the latent model; we discuss the implications of
these two different approaches according to the context of application.
Finally, we outline methods for obtaining standard errors for the parameter
estimates, for selecting the number of states and for path prediction. Models
and related inference are illustrated by the description of relevant
socio-economic applications available in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.2823</identifier>
 <datestamp>2010-03-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.2823</id><created>2010-03-14</created><authors><author><keyname>Stuetzle</keyname><forenames>Werner</forenames></author><author><keyname>Percival</keyname><forenames>Donald B.</forenames></author><author><keyname>Marzban</keyname><forenames>Caren</forenames></author></authors><title>Targeted Event Detection</title><categories>stat.ME physics.data-an</categories><comments>13 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of event detection based upon a (typically
multivariate) data stream characterizing some system. Most of the time the
system is quiescent - nothing of interest is happening - but occasionally
events of interest occur. The goal of event detection is to raise an alarm as
soon as possible after the onset of an event. A simple way of addressing the
event detection problem is to look for changes in the data stream and equate
`change' with `onset of event'. However, there might be many kinds of changes
in the stream that are uninteresting. We assume that we are given a segment of
the stream where interesting events have been marked. We propose a method for
using these training data to construct a `targeted' detector that is
specifically sensitive to changes signaling the onset of interesting events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.2831</identifier>
 <datestamp>2010-03-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.2831</id><created>2010-03-14</created><authors><author><keyname>Turner</keyname><forenames>Rolf</forenames></author><author><keyname>Chareka</keyname><forenames>Patrick</forenames></author></authors><title>A note on the Berman condition</title><categories>stat.ME</categories><msc-class>62M10 62G32 91B84</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is established that if a time series satisfies the Berman condition, and
another related (summability) condition, the result of filtering that series
through a certain type of filter also satisfies the two conditions. In
particular it follows that if $X_t$ satisfies the two conditions and if $X_t$
and $a_t$ are related by an invertible ARMA model, then the $a_t$ satisfy the
two conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.2934</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.2934</id><created>2010-03-15</created><authors><author><keyname>Ambaum</keyname><forenames>Maarten H. P.</forenames></author></authors><title>Significance Tests in Climate Science</title><categories>physics.ao-ph physics.data-an stat.AP</categories><comments>9 pages</comments><journal-ref>Journal of Climate, vol. 23, 5927-5932 (2010)</journal-ref><doi>10.1175/2010jcli3746.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large fraction of papers in the climate literature includes erroneous uses
of significance tests. A Bayesian analysis is presented to highlight the
meaning of significance tests and why typical misuse occurs. It is concluded
that a significance test very rarely provides useful quantitative information.
The significance statistic is not a quantitative measure of how confident we
can be of the 'reality' of a given result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.3201</identifier>
 <datestamp>2010-03-17</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.3201</id><created>2010-03-16</created><authors><author><keyname>Thompson</keyname><forenames>Madeleine</forenames></author><author><keyname>Neal</keyname><forenames>Radford M.</forenames></author></authors><title>Covariance-Adaptive Slice Sampling</title><categories>stat.CO</categories><report-no>Tech. Rep. 1002, Dept. of Statistics, Univ. of Toronto</report-no><msc-class>65C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe two slice sampling methods for taking multivariate steps using
the crumb framework. These methods use the gradients at rejected proposals to
adapt to the local curvature of the log-density surface, a technique that can
produce much better proposals when parameters are highly correlated. We
evaluate our methods on four distributions and compare their performance to
that of a non-adaptive slice sampling method and a Metropolis method. The
adaptive methods perform favorably on low-dimensional target distributions with
highly-correlated parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.3259</identifier>
 <datestamp>2011-07-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.3259</id><created>2010-03-16</created><updated>2011-07-14</updated><authors><author><keyname>Wermuth</keyname><forenames>Nanny</forenames></author></authors><title>Probability distributions with summary graph structure</title><categories>stat.ME math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ309 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ309</report-no><journal-ref>Bernoulli 2011, Vol. 17, No. 3, 845-879</journal-ref><doi>10.3150/10-BEJ309</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A set of independence statements may define the independence structure of
interest in a family of joint probability distributions. This structure is
often captured by a graph that consists of nodes representing the random
variables and of edges that couple node pairs. One important class contains
regression graphs. Regression graphs are a type of so-called chain graph and
describe stepwise processes, in which at each step single or joint responses
are generated given the relevant explanatory variables in their past. For joint
densities that result after possible marginalising or conditioning, we
introduce summary graphs. These graphs reflect the independence structure
implied by the generating process for the reduced set of variables and they
preserve the implied independences after additional marginalising and
conditioning. They can identify generating dependences that remain unchanged
and alert to possibly severe distortions due to direct and indirect
confounding. Operators for matrix representations of graphs are used to derive
these properties of summary graphs and to translate them into special types of
paths in graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.3323</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.3323</id><created>2010-03-17</created><updated>2012-03-31</updated><authors><author><keyname>Frick</keyname><forenames>Klaus</forenames></author><author><keyname>Marnitz</keyname><forenames>Philipp</forenames></author><author><keyname>Munk</keyname><forenames>Axel</forenames></author></authors><title>Shape Constrained Regularisation by Statistical Multiresolution for
  Inverse Problems: Asymptotic Analysis</title><categories>math.ST stat.TH</categories><msc-class>62G05, 49N45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with a novel regularisation technique for solving
linear ill-posed operator equations in Hilbert spaces from data that is
corrupted by white noise. We combine convex penalty functionals with
extreme-value statistics of projections of the residuals on a given set of
sub-spaces in the image-space of the operator. We prove general consistency and
convergence rate results in the framework of Bregman-divergences which allows
for a vast range of penalty functionals. Various examples that indicate the
applicability of our approach will be discussed. We will illustrate in the
context of signal and image processing that the presented method constitutes a
locally adaptive reconstruction method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.3357</identifier>
 <datestamp>2010-03-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.3357</id><created>2010-03-17</created><updated>2010-03-22</updated><authors><author><keyname>Tua</keyname><forenames>Alan</forenames></author><author><keyname>Adami</keyname><forenames>Kristian Zarb</forenames></author></authors><title>Computational Methods in Bayesian Statistics</title><categories>stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on utilizing two different Bayesian methods to deal with a
variety of toy problems which occur in data analysis. In particular we
implement the Variational Bayesian and Nested Sampling methods to tackle the
problems of polynomial selection and Gaussian Mixture Models, comparing the
algorithms in terms of processing speed and accuracy. In the problems tackled
here it is the Variational Bayesian algorithms which are the faster though both
results give similar results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.3362</identifier>
 <datestamp>2010-03-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.3362</id><created>2010-03-17</created><authors><author><keyname>Wang</keyname><forenames>Ge</forenames></author><author><keyname>Yang</keyname><forenames>Jiansheng</forenames></author></authors><title>Axiomatic Quantification of Co-authors' Relative Contributions</title><categories>stat.AP cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decades, the competition for academic resources has gradually
intensified, and worsened with the current financial crisis. To optimize the
resource allocation, individualized assessment of research results is being
actively studied but the current indices, such as the number of papers, the
number of citations, the h-factor and its variants have limitations, especially
their inability of determining co-authors' credit shares fairly. Here we
establish an axiomatic system and quantify co-authors' relative contributions.
Our methodology avoids subjective assignment of co-authors' credits using the
inflated, fractional or harmonic methods, and provides a quantitative tool for
scientific management such as funding and tenure decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.3439</identifier>
 <datestamp>2010-03-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.3439</id><created>2010-03-17</created><authors><author><keyname>Diaz-Garcia</keyname><forenames>Jose A.</forenames></author><author><keyname>Caro-Lopera</keyname><forenames>Francisco J.</forenames></author></authors><title>Shape Theory via QR decomposition</title><categories>math.ST stat.TH</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work sets the non isotropic noncentral elliptical shape distributions
via QR decomposition in the context of zonal polynomials, avoiding the
invariant polynomials and the open problems for their computation. The new
shape distributions are easily computable and then the inference procedure can
be studied under exact densities instead under the published approximations and
asymptotic densities under isotropic models. An application in Biology is
studied under the classical gaussian approach and a two non gaussian models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.3539</identifier>
 <datestamp>2010-03-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.3539</id><created>2010-03-18</created><authors><author><keyname>Kutoyants</keyname><forenames>Yury A.</forenames></author></authors><title>On Identification of the Threshold Diffusion Processes</title><categories>math.ST stat.TH</categories><comments>34 pages</comments><msc-class>62M02, 62G10, 62G20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problems of parameter estimation for several models of
threshold ergodic diffusion processes in the asymptotics of large samples.
These models are the direct continuous time analogues of the well-known in time
series analysis threshold autoregressive (TAR) models. In such models the trend
is switching when the observed process atteints some (unknown) values and the
problem is to estimate it or to test some hypotheses concerning these values.
The related statistical problems correspond to the singular estimation or
testing, for example, the rate of convergence of estimators is $T$ and not
$\sqrt{T}$ as in regular estimation problems. We study the asymptotic behavior
of the maximum likelihood and bayesian estimators and discuss the possibility
of the construction of the goodness of fit test for such models of observation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.3546</identifier>
 <datestamp>2010-03-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.3546</id><created>2010-03-18</created><authors><author><keyname>Hoepfner</keyname><forenames>Reinhard</forenames></author><author><keyname>Kutoyants</keyname><forenames>Yury</forenames></author></authors><title>On LAN for parametrized continuous periodic signals in a time
  inhomogeneous diffusion</title><categories>math.ST stat.TH</categories><msc-class>62 F 12 , 60 J 60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a diffusion $(\xi_t)_{t\ge 0}$ whose drift involves a
$T$-periodic signal. $T$ is fixed and known, whereas the signal depends on an
unknown $d$-dimensional parameter $\vartheta\in\Theta$. Assuming positive
Harris recurrence of the grid chain $(\xi_{kT})_{k\in\mathbb{N}_0}$ and
exploiting the periodic structure of the semigroup, we work with path segments
and limit theorems for certain functionals (more general than additive
functionals) of the process to prove local asymptotic normality (LAN). Then we
consider several estimators for the unknown parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.3570</identifier>
 <datestamp>2010-03-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.3570</id><created>2010-03-18</created><authors><author><keyname>Pahikkala</keyname><forenames>Tapio</forenames></author><author><keyname>Airola</keyname><forenames>Antti</forenames></author><author><keyname>Salakoski</keyname><forenames>Tapio</forenames></author></authors><title>Linear Time Feature Selection for Regularized Least-Squares</title><categories>stat.ML</categories><comments>17 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel algorithm for greedy forward feature selection for
regularized least-squares (RLS) regression and classification, also known as
the least-squares support vector machine or ridge regression. The algorithm,
which we call greedy RLS, starts from the empty feature set, and on each
iteration adds the feature whose addition provides the best leave-one-out
cross-validation performance. Our method is considerably faster than the
previously proposed ones, since its time complexity is linear in the number of
training examples, the number of features in the original data set, and the
desired size of the set of selected features. Therefore, as a side effect we
obtain a new training algorithm for learning sparse linear RLS predictors which
can be used for large scale learning. This speed is possible due to matrix
calculus based short-cuts for leave-one-out and feature addition. We
experimentally demonstrate the scalability of our algorithm and its ability to
find good quality feature sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.3700</identifier>
 <datestamp>2011-01-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.3700</id><created>2010-03-18</created><updated>2011-01-05</updated><authors><author><keyname>Aldous</keyname><forenames>David J.</forenames></author><author><keyname>Shun</keyname><forenames>Julian</forenames></author></authors><title>Connected Spatial Networks over Random Points and a Route-Length
  Statistic</title><categories>math.PR cond-mat.dis-nn stat.ME</categories><comments>Published in at http://dx.doi.org/10.1214/10-STS335 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS335</report-no><journal-ref>Statistical Science 2010, Vol. 25, No. 3, 275-288</journal-ref><doi>10.1214/10-STS335</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review mathematically tractable models for connected networks on random
points in the plane, emphasizing the class of proximity graphs which deserves
to be better known to applied probabilists and statisticians. We introduce and
motivate a particular statistic $R$ measuring shortness of routes in a network.
We illustrate, via Monte Carlo in part, the trade-off between normalized
network length and $R$ in a one-parameter family of proximity graphs. How close
this family comes to the optimal trade-off over all possible networks remains
an intriguing open question. The paper is a write-up of a talk developed by the
first author during 2007--2009.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.3800</identifier>
 <datestamp>2010-03-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.3800</id><created>2010-03-18</created><authors><author><keyname>Chan</keyname><forenames>Ngai Hang</forenames></author><author><keyname>Kutoyants</keyname><forenames>Yury A.</forenames></author></authors><title>On Parameter Estimation of Threshold Autoregressive Models</title><categories>math.ST stat.TH</categories><msc-class>62G30, 62M10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the threshold estimation of a TAR model when the
underlying threshold parameter is a random variable. It is shown that the
Bayesian estimator is consistent and its limit distribution is expressed in
terms of a limit likelihood ratio. Furthermore, convergence of moments of the
estimators is also established. The limit distribution can be computed via
explicit simulations from which testing and inference for the threshold
parameter can be conducted. The obtained results are illustrated with numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.3985</identifier>
 <datestamp>2010-03-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.3985</id><created>2010-03-21</created><authors><author><keyname>Giryes</keyname><forenames>Raja</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C</forenames></author></authors><title>The Projected GSURE for Automatic Parameter Tuning in Iterative
  Shrinkage Methods</title><categories>cs.CV stat.AP</categories><comments>20 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear inverse problems are very common in signal and image processing. Many
algorithms that aim at solving such problems include unknown parameters that
need tuning. In this work we focus on optimally selecting such parameters in
iterative shrinkage methods for image deblurring and image zooming. Our work
uses the projected Generalized Stein Unbiased Risk Estimator (GSURE) for
determining the threshold value lambda and the iterations number K in these
algorithms. The proposed parameter selection is shown to handle any degradation
operator, including ill-posed and even rectangular ones. This is achieved by
using GSURE on the projected expected error. We further propose an efficient
greedy parameter setting scheme, that tunes the parameter while iterating
without impairing the resulting deblurring performance. Finally, we provide
extensive comparisons to conventional methods for parameter selection, showing
the superiority of the use of the projected GSURE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.3988</identifier>
 <datestamp>2010-03-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.3988</id><created>2010-03-21</created><authors><author><keyname>Green</keyname><forenames>Peter J.</forenames></author></authors><title>Colouring and breaking sticks: random distributions and heterogeneous
  clustering</title><categories>stat.ME stat.CO</categories><comments>26 pages, 3 figures. Chapter 13 of "Probability and Mathematical
  Genetics: Papers in Honour of Sir John Kingman" (Editors N.H. Bingham and
  C.M. Goldie), Cambridge University Press, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We begin by reviewing some probabilistic results about the Dirichlet Process
and its close relatives, focussing on their implications for statistical
modelling and analysis. We then introduce a class of simple mixture models in
which clusters are of different `colours', with statistical characteristics
that are constant within colours, but different between colours. Thus cluster
identities are exchangeable only within colours. The basic form of our model is
a variant on the familiar Dirichlet process, and we find that much of the
standard modelling and computational machinery associated with the Dirichlet
process may be readily adapted to our generalisation. The methodology is
illustrated with an application to the partially-parametric clustering of gene
expression profiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.4156</identifier>
 <datestamp>2010-03-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.4156</id><created>2010-03-22</created><authors><author><keyname>Aubin</keyname><forenames>Jean-Baptiste</forenames></author><author><keyname>Leoni-Aubin</keyname><forenames>Samuela</forenames></author></authors><title>A longest run test for heteroscedasticity in univariate regression model</title><categories>stat.ME math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The scope of this paper is the presentation of a test that enables to detect
heteroscedasticity in univariate regression model. The test is simple to
compute and very general since no hypothesis is made on the regularity of the
response function or on the normality of errors. Simulations show that our test
fairs well with respect to other less general nonparametric tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.4254</identifier>
 <datestamp>2010-03-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.4254</id><created>2010-03-22</created><authors><author><keyname>Bhattacharya</keyname><forenames>Rabi</forenames></author><author><keyname>Holmes</keyname><forenames>Susan</forenames></author></authors><title>An Exposition of G\"otze's Estimation of the Rate of Convergence in the
  Multivariate Central Limit Theorem</title><categories>math.ST math.PR stat.TH</categories><msc-class>60F05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an explanation of the main ideas underlying G\"otze's main result
in using Stein's method. We also provide detailed derivations of various
intermediate estimates. Curiously, we are led to a different dimensional
dependence of the constant than that given G\"otze's paper. We would like to
dedicate this to Charles Stein on the occasion of his 90th birthday.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.4306</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.4306</id><created>2010-03-22</created><updated>2012-10-04</updated><authors><author><keyname>Mattingly</keyname><forenames>Jonathan C.</forenames></author><author><keyname>Pillai</keyname><forenames>Natesh S.</forenames></author><author><keyname>Stuart</keyname><forenames>Andrew M.</forenames></author></authors><title>Diffusion limits of the random walk Metropolis algorithm in high
  dimensions</title><categories>math.PR math.ST stat.CO stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/10-AAP754 the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP754</report-no><journal-ref>Annals of Applied Probability 2012, Vol. 22, No. 3, 881-930</journal-ref><doi>10.1214/10-AAP754</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion limits of MCMC methods in high dimensions provide a useful
theoretical tool for studying computational complexity. In particular, they
lead directly to precise estimates of the number of steps required to explore
the target measure, in stationarity, as a function of the dimension of the
state space. However, to date such results have mainly been proved for target
measures with a product structure, severely limiting their applicability. The
purpose of this paper is to study diffusion limits for a class of naturally
occurring high-dimensional measures found from the approximation of measures on
a Hilbert space which are absolutely continuous with respect to a Gaussian
reference measure. The diffusion limit of a random walk Metropolis algorithm to
an infinite-dimensional Hilbert space valued SDE (or SPDE) is proved,
facilitating understanding of the computational complexity of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.4466</identifier>
 <datestamp>2010-03-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.4466</id><created>2010-03-23</created><authors><author><keyname>Guillou</keyname><forenames>Armelle</forenames></author><author><keyname>Kratz</keyname><forenames>Marie</forenames></author><author><keyname>Strat</keyname><forenames>Yann Le</forenames></author></authors><title>An Extreme Value Theory approach for the early detection of time
  clusters with application to the surveillance of Salmonella</title><categories>stat.AP stat.ME</categories><comments>21 pages, 6 figures</comments><msc-class>60G70 ; 62P10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method to generate a warning system for the early detection of
time clusters applied to public health surveillance data. This new method
relies on the evaluation of a return period associated to any new count of a
particular infection reported to a surveillance system. The method is applied
to Salmonella surveillance in France and compared to the model developed by
Farrington et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.4650</identifier>
 <datestamp>2010-03-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.4650</id><created>2010-03-24</created><authors><author><keyname>Griffiths</keyname><forenames>Robert C.</forenames></author><author><keyname>Spano`</keyname><forenames>Dario</forenames></author></authors><title>Diffusion processes and coalescent trees</title><categories>math.PR math.ST q-bio.PE stat.TH</categories><comments>22 pages</comments><journal-ref>Chapter 15 of Probability and Mathematical Genetics: Papers in
  Honour of Sir John Kingman, ed. N. H. Bingham and C. M. Goldie. London
  Mathematical Society Lecture Notes Series, Cambridge University Press, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We dedicate this paper to Sir John Kingman on his 70th Birthday. In modern
mathematical population genetics the ancestral history of a population of genes
back in time is described by John Kingman's coalescent tree. Classical and
modern approaches model gene frequencies by diffusion processes. This paper,
which is partly a review, discusses how coalescent processes are dual to
diffusion processes in an analytic and probabilistic sense. Bochner (1954) and
Gasper (1972) were interested in characterizations of processes with Beta
stationary distributions and Jacobi polynomial eigenfunctions. We discuss the
connection with Wright--Fisher diffusions and the characterization of these
processes. Subordinated Wright--Fisher diffusions are of this type. An Inverse
Gaussian subordinator is interesting and important in subordinated
Wright--Fisher diffusions and is related to the Jacobi Poisson Kernel in
orthogonal polynomial theory. A related time-subordinated forest of non-mutant
edges in the Kingman coalescent is novel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.4741</identifier>
 <datestamp>2010-03-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.4741</id><created>2010-03-24</created><authors><author><keyname>Rogers</keyname><forenames>David M.</forenames></author><author><keyname>Beck</keyname><forenames>Thomas L.</forenames></author></authors><title>Resolution and Scale Independent Function Matching Using a String Energy
  Penalized Spline Prior</title><categories>stat.ML cond-mat.mes-hall</categories><comments>Submitted to Annals of Statistics, Mar. 2009, but unpublished in
  current form.</comments><msc-class>Primary 62G35; secondary 70F17, 65D10, 62G08, 41A15.</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extension of the classical Bayesian penalized spline method to inference
on vector-valued functions is considered, with an emphasis on characterizing
the suitability of the method for general application.We show that the standard
quadratic penalty is exactly analogous to the energy of a stretched string,
with the penalty parameter corresponding to its tension. This physical analogy
motivates a discussion of resolution independence, which we define as the
convergence of a computational function estimate to arbitrary accuracy with
increasing resolution.The multidimensional context makes direct application of
standard procedures for choosing the penalty parameter difficult, and a new
method is proposed and compared to the established generalized cross-validation
(GCV) and Akaike information criterion (AIC) functions.Our Bayesian method for
choosing this parameter is derived by introducing a scal e independence
criterion to ensure that simultaneously scaling the function samples and their
variances does not significantly change the posterior parameter distribution.
Due to the possibility of an exact polynomial fit, numerical issues prevent the
use of this prior, and a solution is presented based on adding a st ring
zero-point energy. This makes more complicated approaches recently propose d in
the literature unnecessary, and eliminates the requirement for sensitivity
analysis when the function deviates from the above mentioned polynomial. An
important class of problems which can be analyzed by this method are stochastic
numerical integrators, which are considered as an example problem. This work
represents the first extension of penalized spline methods to inference on
multidimensional numerical integrators reported in the literature. Several
numerical calculations illustrate the above points and address practical
application issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.4780</identifier>
 <datestamp>2010-03-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.4780</id><created>2010-03-24</created><authors><author><keyname>Diaz-Garcia</keyname><forenames>Jose A.</forenames></author><author><keyname>Caro-Lopera</keyname><forenames>Francisco J.</forenames></author></authors><title>Shape theory via SVD decomposition I</title><categories>math.ST stat.TH</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work finds the non isotropic noncentral elliptical shape distributions
via SVD decomposition in the context of zonal polynomials, avoiding the
invariant polynomials and the open problems for their computation. The new
shape distributions are easily computable and then the inference procedure is
based on exact densities instead of the published approximations and asymptotic
densities of isotropic models. An application of the technique is illustrated
with a classical landmark data in Biology, for this, three models are proposed,
the usual Gaussian and two non Gaussian; the best one is chosen by using a
modified BIC criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.4885</identifier>
 <datestamp>2011-10-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.4885</id><created>2010-03-25</created><updated>2011-10-07</updated><authors><author><keyname>Hebiri</keyname><forenames>Mohamed</forenames></author><author><keyname>Van De Geer</keyname><forenames>Sara A.</forenames></author></authors><title>The Smooth-Lasso and other $\ell_1+\ell_2$-penalized methods</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a linear regression problem in a high dimensional setting where
the number of covariates $p$ can be much larger than the sample size $n$. In
such a situation, one often assumes sparsity of the regression vector, \textit
i.e., the regression vector contains many zero components. We propose a
Lasso-type estimator $\hat{\beta}^{Quad}$ (where '$Quad$' stands for quadratic)
which is based on two penalty terms. The first one is the $\ell_1$ norm of the
regression coefficients used to exploit the sparsity of the regression as done
by the Lasso estimator, whereas the second is a quadratic penalty term
introduced to capture some additional information on the setting of the
problem. We detail two special cases: the Elastic-Net $\hat{\beta}^{EN}$, which
deals with sparse problems where correlations between variables may exist; and
the Smooth-Lasso $\hat{\beta}^{SL}$, which responds to sparse problems where
successive regression coefficients are known to vary slowly (in some
situations, this can also be interpreted in terms of correlations between
successive variables). From a theoretical point of view, we establish variable
selection consistency results and show that $\hat{\beta}^{Quad}$ achieves a
Sparsity Inequality, \textit i.e., a bound in terms of the number of non-zero
components of the 'true' regression vector. These results are provided under a
weaker assumption on the Gram matrix than the one used by the Lasso. In some
situations this guarantees a significant improvement over the Lasso.
Furthermore, a simulation study is conducted and shows that the S-Lasso
$\hat{\beta}^{SL}$ performs better than known methods as the Lasso, the
Elastic-Net $\hat{\beta}^{EN}$, and the Fused-Lasso with respect to the
estimation accuracy. This is especially the case when the regression vector is
'smooth', \textit i.e., when the variations between successive coefficients of
the unknown parameter of the regression are small. The study also reveals that
the theoretical calibration of the tuning parameters and the one based on 10
fold cross validation imply two S-Lasso solutions with close performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.4890</identifier>
 <datestamp>2010-04-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.4890</id><created>2010-03-25</created><authors><author><keyname>Poitevineau</keyname><forenames>Jacques</forenames><affiliation>LMRS</affiliation></author><author><keyname>Lecoutre</keyname><forenames>Bruno</forenames><affiliation>LMRS</affiliation></author></authors><title>Implementing Bayesian predictive procedures: The K-prime and K-square
  distributions</title><categories>stat.ME</categories><proxy>ccsd</proxy><journal-ref>Computational Statistics \&amp; Data Analysis / Computational
  Statistics and Data Analysis 54, 3 (2010) 724-731</journal-ref><doi>10.1016/j.csda.2008.11.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The implementation of Bayesian predictive procedures under standard normal
models is considered. Two distributions are of particular interest, the K-prime
and K-square distributions. They also give exact inferences for simple and
multiple correlation coefficients. Their cumulative distribution functions can
be expressed in terms of infinite series of multiples of incomplete beta
function ratios, thus adequate for recursive calculations. Efficient algorithms
are provided. To deal with special cases where possible underflows may prevent
a recurrence to work properly, a simple solution is proposed which results in a
procedure which is intermediate between two classes of algorithm. Some examples
of applications are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.4944</identifier>
 <datestamp>2010-03-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.4944</id><created>2010-03-25</created><authors><author><keyname>Adams</keyname><forenames>Ryan Prescott</forenames></author><author><keyname>Dahl</keyname><forenames>George E.</forenames></author><author><keyname>Murray</keyname><forenames>Iain</forenames></author></authors><title>Incorporating Side Information in Probabilistic Matrix Factorization
  with Gaussian Processes</title><categories>stat.ML cs.LG</categories><comments>18 pages, 4 figures, Submitted to UAI 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic matrix factorization (PMF) is a powerful method for modeling
data associated with pairwise relationships, finding use in collaborative
filtering, computational biology, and document analysis, among other areas. In
many domains, there is additional information that can assist in prediction.
For example, when modeling movie ratings, we might know when the rating
occurred, where the user lives, or what actors appear in the movie. It is
difficult, however, to incorporate this side information into the PMF model. We
propose a framework for incorporating side information by coupling together
multiple PMF problems via Gaussian process priors. We replace scalar latent
features with functions that vary over the space of side information. The GP
priors on these functions require them to vary smoothly and share information.
We successfully use this new method to predict the scores of professional
basketball games, where side information about the venue and date of the game
are relevant for the outcome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.5089</identifier>
 <datestamp>2010-04-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.5089</id><created>2010-03-26</created><authors><author><keyname>Biau</keyname><forenames>Grard</forenames><affiliation>LSTA, DMA</affiliation></author><author><keyname>Mas</keyname><forenames>Andr</forenames><affiliation>I3M</affiliation></author></authors><title>PCA-Kernel Estimation</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many statistical estimation techniques for high-dimensional or functional
data are based on a preliminary dimension reduction step, which consists in
projecting the sample $\bX_1, \hdots, \bX_n$ onto the first $D$ eigenvectors of
the Principal Component Analysis (PCA) associated with the empirical projector
$\hat \Pi_D$. Classical nonparametric inference methods such as kernel density
estimation or kernel regression analysis are then performed in the (usually
small) $D$-dimensional space. However, the mathematical analysis of this
data-driven dimension reduction scheme raises technical problems, due to the
fact that the random variables of the projected sample $(\hat
\Pi_D\bX_1,\hdots, \hat \Pi_D\bX_n)$ are no more independent. As a reference
for further studies, we offer in this paper several results showing the
asymptotic equivalencies between important kernel-related quantities based on
the empirical projector and its theoretical counterpart. As an illustration, we
provide an in-depth analysis of the nonparametric kernel regression case
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.5131</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.5131</id><created>2010-03-26</created><updated>2013-03-20</updated><authors><author><keyname>Griffiths</keyname><forenames>Robert C.</forenames></author><author><keyname>Span</keyname><forenames>Dario</forenames></author></authors><title>Orthogonal polynomial kernels and canonical correlations for Dirichlet
  measures</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/11-BEJ403 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ403</report-no><journal-ref>Bernoulli 2013, Vol. 19, No. 2, 548-598</journal-ref><doi>10.3150/11-BEJ403</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multivariate version of the so-called Lancaster problem of
characterizing canonical correlation coefficients of symmetric bivariate
distributions with identical marginals and orthogonal polynomial expansions.
The marginal distributions examined in this paper are the Dirichlet and the
Dirichlet multinomial distribution, respectively, on the continuous and the
N-discrete d-dimensional simplex. Their infinite-dimensional limit
distributions, respectively, the Poisson-Dirichlet distribution and Ewens's
sampling formula, are considered as well. We study, in particular, the
possibility of mapping canonical correlations on the d-dimensional continuous
simplex (i) to canonical correlation sequences on the d+1-dimensional simplex
and/or (ii) to canonical correlations on the discrete simplex, and vice versa.
Driven by this motivation, the first half of the paper is devoted to providing
a full characterization and probabilistic interpretation of n-orthogonal
polynomial kernels (i.e., sums of products of orthogonal polynomials of the
same degree n) with respect to the mentioned marginal distributions. We
establish several identities and some integral representations which are
multivariate extensions of important results known for the case d=2 since the
1970s. These results, along with a common interpretation of the mentioned
kernels in terms of dependent Polya urns, are shown to be key features leading
to several non-trivial solutions to Lancaster's problem, many of which can be
extended naturally to the limit as $d\rightarrow\infty$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.5165</identifier>
 <datestamp>2010-12-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.5165</id><created>2010-03-26</created><updated>2010-12-08</updated><authors><author><keyname>Ambroise</keyname><forenames>Christophe</forenames></author><author><keyname>Matias</keyname><forenames>Catherine</forenames></author></authors><title>New consistent and asymptotically normal estimators for random graph
  mixture models</title><categories>math.ST stat.ME stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random graph mixture models are now very popular for modeling real data
networks. In these setups, parameter estimation procedures usually rely on
variational approximations, either combined with the expectation-maximisation
(\textsc{em}) algorithm or with Bayesian approaches. Despite good results on
synthetic data, the validity of the variational approximation is however not
established. Moreover, the behavior of the maximum likelihood or of the maximum
a posteriori estimators approximated by these procedures is not known in these
models, due to the dependency structure on the variables. In this work, we show
that in many different affiliation contexts (for binary or weighted graphs),
estimators based either on moment equations or on the maximization of some
composite likelihood are strongly consistent and $\sqrt{n}$-convergent, where
$n$ is the number of nodes. As a consequence, our result establishes that the
overall structure of an affiliation model can be caught by the description of
the network in terms of its number of triads (order 3 structures) and edges
(order 2 structures). We illustrate the efficiency of our method on simulated
data and compare its performances with other existing procedures. A data set of
cross-citations among economics journals is also analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.5338</identifier>
 <datestamp>2011-11-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.5338</id><created>2010-03-28</created><updated>2011-11-09</updated><authors><author><keyname>Lin</keyname><forenames>Shaowei</forenames></author></authors><title>Asymptotic Approximation of Marginal Likelihood Integrals</title><categories>stat.CO</categories><comments>31 pages. Changed introduction (Section 1), added new results on
  sos-nondegeneracy (Section 4) and elaborated on statistical application
  (Section 5)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The accurate asymptotic evaluation of marginal likelihood integrals is a
fundamental problem in Bayesian statistics. Following the approach introduced
by Watanabe, we translate this into a problem of computational algebraic
geometry, namely, to determine the real log canonical threshold of a polynomial
ideal, and we present effective methods for solving this problem. Our results
are based on resolution of singularities, and they apply to all statistical
models for discrete data that admit a parametrization by real analytic
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.5457</identifier>
 <datestamp>2010-04-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.5457</id><created>2010-03-29</created><authors><author><keyname>Broniatowski</keyname><forenames>Michel</forenames><affiliation>LSTA</affiliation></author><author><keyname>Keziou</keyname><forenames>Amor</forenames><affiliation>LSTA, LM-Reims</affiliation></author></authors><title>Minimization of divergences on sets of signed measures</title><categories>math.PR math.OC math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the minimization problem of $\phi$-divergences between a given
probability measure $P$ and subsets $\Omega$ of the vector space
$\mathcal{M}_\mathcal{F}$ of all signed finite measures which integrate a given
class $\mathcal{F}$ of bounded or unbounded measurable functions. The vector
space $\mathcal{M}_\mathcal{F}$ is endowed with the weak topology induced by
the class $\mathcal{F}\cup \mathcal{B}_b$ where $\mathcal{B}_b$ is the class of
all bounded measurable functions. We treat the problems of existence and
characterization of the $\phi$-projections of $P$ on $\Omega$. We consider also
the dual equality and the dual attainment problems when $\Omega$ is defined by
linear constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.5535</identifier>
 <datestamp>2010-03-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.5535</id><created>2010-03-29</created><authors><author><keyname>Little</keyname><forenames>Max A.</forenames></author><author><keyname>Jones</keyname><forenames>Nick S.</forenames></author></authors><title>Sparse bayesian step-filtering for high-throughput analysis of molecular
  machine dynamics</title><categories>q-bio.QM stat.AP</categories><comments>4 pages, link to code available from author's website.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nature has evolved many molecular machines such as kinesin, myosin, and the
rotary flagellar motor powered by an ion current from the mitochondria. Direct
observation of the step-like motion of these machines with time series from
novel experimental assays has recently become possible. These time series are
corrupted by molecular and experimental noise that requires removal, but
classical signal processing is of limited use for recovering such step-like
dynamics. This paper reports simple, novel Bayesian filters that are robust to
step-like dynamics in noise, and introduce an L1-regularized, global filter
whose sparse solution can be rapidly obtained by standard convex optimization
methods. We show these techniques outperforming classical filters on simulated
time series in terms of their ability to accurately recover the underlying step
dynamics. To show the techniques in action, we extract step-like speed
transitions from Rhodobacter sphaeroides flagellar motor time series. Code
implementing these algorithms available from
http://www.eng.ox.ac.uk/samp/members/max/software/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.5539</identifier>
 <datestamp>2010-03-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.5539</id><created>2010-03-25</created><authors><author><keyname>Lee</keyname><forenames>Gyemin</forenames><affiliation>Department of Electrical Engineering and Computer Science, University of Michigan</affiliation></author><author><keyname>Finn</keyname><forenames>William</forenames><affiliation>Department of Pathology, University of Michigan</affiliation></author><author><keyname>Scott</keyname><forenames>Clayton</forenames><affiliation>Department of Electrical Engineering and Computer Science, University of Michigan</affiliation><affiliation>Department of Statistics, University of Michigan</affiliation></author></authors><title>Statistical File Matching of Flow Cytometry Data</title><categories>stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flow cytometry is a technology that rapidly measures antigen-based markers
associated to cells in a cell population. Although analysis of flow cytometry
data has traditionally considered one or two markers at a time, there has been
increasing interest in multidimensional analysis. However, flow cytometers are
limited in the number of markers they can jointly observe, which is typically a
fraction of the number of markers of interest. For this reason, practitioners
often perform multiple assays based on different, overlapping combinations of
markers. In this paper, we address the challenge of imputing the high
dimensional jointly distributed values of marker attributes based on
overlapping marginal observations. We show that simple nearest neighbor based
imputation can lead to spurious subpopulations in the imputed data, and
introduce an alternative approach based on nearest neighbor imputation
restricted to a cell's subpopulation. This requires us to perform clustering
with missing data, which we address with a mixture model approach and novel EM
algorithm. Since mixture model fitting may be ill-posed, we also develop
techniques to initialize the EM algorithm using domain knowledge. We
demonstrate our approach on real flow cytometry data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.5544</identifier>
 <datestamp>2010-10-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.5544</id><created>2010-03-29</created><updated>2010-10-17</updated><authors><author><keyname>Robert</keyname><forenames>Christian P.</forenames></author></authors><title>An attempt at reading Keynes' Treatise on Probability</title><categories>math.ST math.HO stat.TH</categories><comments>Revised version, 18 pages, five pictures of Keynes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The book A Treatise on Probability was published by John Maynard Keynes in
1921. It contains a critical assessment of the foundations of probability and
of the current statistical methodology. As a modern reader, we review here the
aspects that are most related with statistics, avoiding a neophyte's
perspective on the philosophical issues. In particular, the book is quite
critical of the Bayesian approach and we examine the arguments provided by
Keynes, as well as the alternative he proposes. This review does not subsume
the scholarly study of Aldrich (2008a) relating Keynes with the statistics
community of the time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.6039</identifier>
 <datestamp>2010-10-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.6039</id><created>2010-03-31</created><updated>2010-10-25</updated><authors><author><keyname>Chen</keyname><forenames>Louis H. Y.</forenames></author><author><keyname>Rllin</keyname><forenames>Adrian</forenames></author></authors><title>Stein couplings for normal approximation</title><categories>math.PR math.ST stat.TH</categories><comments>54 pages; minor changes and corrections</comments><msc-class>60F05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we propose a general framework for normal approximation using
Stein's method. We introduce the new concept of Stein couplings and we show
that it lies at the heart of popular approaches such as the local approach,
exchangeable pairs, size biasing and many other approaches. We prove several
theorems with which normal approximation for the Wasserstein and Kolmogorov
metrics becomes routine once a Stein coupling is found. To illustrate the
versatility of our framework we give applications in Hoeffding's combinatorial
central limit theorem, functionals in the classic occupancy scheme,
neighbourhood statistics of point patterns with fixed number of points and
functionals of the components of randomly chosen vertices of sub-critical
Erdos-Renyi random graphs. In all these cases, we use new, non-standard
couplings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1003.6119</identifier>
 <datestamp>2010-04-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1003.6119</id><created>2010-03-29</created><authors><author><keyname>Hwang</keyname><forenames>Hsien-Kuei</forenames></author><author><keyname>Tsai</keyname><forenames>Tsung-Hsi</forenames></author></authors><title>Multivariate records based on dominance</title><categories>math.ST math.PR stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider three types of multivariate records in this paper and derive the
mean and the variance of their numbers for independent and uniform random
samples from two prototype regions: hypercubes $[0,1]^d$ and $d$-dimensional
simplex. Central limit theorems with convergence rates are established when the
variance tends to infinity. Effective numerical procedures are also provided
for computing the variance constants to high degree of precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.0234</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.0234</id><created>2010-04-01</created><updated>2013-03-12</updated><authors><author><keyname>Maruyama</keyname><forenames>Yuzo</forenames></author><author><keyname>Strawderman</keyname><forenames>William E.</forenames></author></authors><title>Improved robust Bayes estimators of the error variance in linear models</title><categories>math.ST stat.TH</categories><comments>11 pages</comments><msc-class>62C20, 62F15, 62A15</msc-class><journal-ref>Journal of Statistical Planning and Inference 143 (2013), pp.
  1091-1097</journal-ref><doi>10.1016/j.jspi.2013.01.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the error variance in a general linear
model when the error distribution is assumed to be spherically symmetric, but
not necessary Gaussian. In particular we study the case of a scale mixture of
Gaussians including the particularly important case of the multivariate-t
distribution. Under Stein's loss, we construct a class of estimators that
improve on the usual best unbiased (and best equivariant) estimator. Our class
has the interesting double robustness property of being simultaneously
generalized Bayes (for the same generalized prior) and minimax over the entire
class of scale mixture of Gaussian distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.0242</identifier>
 <datestamp>2010-04-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.0242</id><created>2010-04-01</created><authors><author><keyname>Diaz-Garcia</keyname><forenames>Jose A.</forenames></author><author><keyname>Caro-Lopera</keyname><forenames>Francisco J.</forenames></author></authors><title>Shape Theory Via SV Decomposition II</title><categories>math.ST stat.TH</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The non isotropic and non central elliptical shape distributions via the Le
and Kendall SVD decomposition approach are derived in this paper in the context
of invariant polynomials and zonal polynomials. The so termed cone and disk
densities here obtained generalise some results of the literature. Finally,
some particular densities are applied in a classical data of Biology, and the
inference is performed after choosing the best model by using a modified BIC
criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.0314</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.0314</id><created>2010-04-02</created><updated>2013-03-07</updated><authors><author><keyname>Fiori</keyname><forenames>Simone</forenames></author></authors><title>Visualization of Manifold-Valued Elements by Multidimensional Scaling</title><categories>stat.ML</categories><comments>The paper has been published in 2011, hence I think it is time to
  withdraw its draft version from the arXiv</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present contribution suggests the use of a multidimensional scaling (MDS)
algorithm as a visualization tool for manifold-valued elements. A visualization
tool of this kind is useful in signal processing and machine learning whenever
learning/adaptation algorithms insist on high-dimensional parameter manifolds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.0356</identifier>
 <datestamp>2010-08-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.0356</id><created>2010-04-02</created><updated>2010-08-27</updated><authors><author><keyname>Dandach</keyname><forenames>Sandra H.</forenames></author><author><keyname>Carli</keyname><forenames>Ruggero</forenames></author><author><keyname>Bullo</keyname><forenames>Francesco</forenames></author></authors><title>Accuracy and Decision Time for Sequential Decision Aggregation</title><categories>stat.AP math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies prototypical strategies to sequentially aggregate
independent decisions. We consider a collection of agents, each performing
binary hypothesis testing and each obtaining a decision over time. We assume
the agents are identical and receive independent information. Individual
decisions are sequentially aggregated via a threshold-based rule. In other
words, a collective decision is taken as soon as a specified number of agents
report a concordant decision (simultaneous discordant decisions and no-decision
outcomes are also handled).
  We obtain the following results. First, we characterize the probabilities of
correct and wrong decisions as a function of time, group size and decision
threshold. The computational requirements of our approach are linear in the
group size. Second, we consider the so-called fastest and majority rules,
corresponding to specific decision thresholds. For these rules, we provide a
comprehensive scalability analysis of both accuracy and decision time. In the
limit of large group sizes, we show that the decision time for the fastest rule
converges to the earliest possible individual time, and that the decision
accuracy for the majority rule shows an exponential improvement over the
individual accuracy. Additionally, via a theoretical and numerical analysis, we
characterize various speed/accuracy tradeoffs. Finally, we relate our results
to some recent observations reported in the cognitive information processing
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.0432</identifier>
 <datestamp>2010-04-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.0432</id><created>2010-04-03</created><authors><author><keyname>Bavaud</keyname><forenames>Franois</forenames></author></authors><title>Stereotype bias: a simple formal model</title><categories>stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimizing the relative inertia of a statistical group with respect to the
inertia of the overall sample defines an unique point, the in-focus, which
constitutes a context-dependent measure of typical group tendency, biased in
comparison to the group centroid. Maximizing the relative inertia yields an
unique out-focal point, polarized in the reverse direction. This mechanism
evokes the relative variability reduction of the outgroup reported in Social
Psychology, and the stereotypic-like behavior of the in-focus, whose bias
vanishes if the outgroup is constituted of a single individual. In this
picture, the out-focus plays the role of an anti-stereotypical position,
identical to the in-focus of the complementary group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.0456</identifier>
 <datestamp>2010-04-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.0456</id><created>2010-04-03</created><authors><author><keyname>Hbrail</keyname><forenames>Georges</forenames></author><author><keyname>Hugueney</keyname><forenames>Bernard</forenames></author><author><keyname>Lechevallier</keyname><forenames>Yves</forenames></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames></author></authors><title>Exploratory Analysis of Functional Data via Clustering and Optimal
  Segmentation</title><categories>stat.ML cs.LG</categories><journal-ref>Neurocomputing, Volume 73, Issues 7-9, March 2010, Pages 1125-1141</journal-ref><doi>10.1016/j.neucom.2009.11.022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose in this paper an exploratory analysis algorithm for functional
data. The method partitions a set of functions into $K$ clusters and represents
each cluster by a simple prototype (e.g., piecewise constant). The total number
of segments in the prototypes, $P$, is chosen by the user and optimally
distributed among the clusters via two dynamic programming algorithms. The
practical relevance of the method is shown on two real world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.0482</identifier>
 <datestamp>2010-04-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.0482</id><created>2010-04-03</created><authors><author><keyname>Pang</keyname><forenames>Zhen</forenames></author><author><keyname>Xue</keyname><forenames>Liugen</forenames></author></authors><title>Estimation for Single-Index mixed models with Longitudinal data</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a single-index mixed model with longitudinal data.
A new set of estimating equations is proposed to estimate the single-index
coefficient. The link function is estimated by using the local linear
smoothing. Asymptotic normality is established for the proposed estimators.
Also, the estimator of the link function achieves optimal convergence rates;
and the estimators of variance components have root-$n$ consistency. These
results facilitate the construction of confidence regions/intervals and
hypothesis testing for the parameters of interest. Some simulations and an
application to real data are included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.0483</identifier>
 <datestamp>2010-04-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.0483</id><created>2010-04-03</created><authors><author><keyname>Diaz-Garcia</keyname><forenames>Jose A.</forenames></author><author><keyname>Caro-Lopera</keyname><forenames>Francisco J.</forenames></author></authors><title>Shape theory via polar decomposition</title><categories>math.ST stat.TH</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a new model in the context of statistical theory of shape,
based on the polar decomposition. The non isotropic noncentral elliptical shape
distributions via polar decomposition is derived in the context of zonal
polynomials, avoiding the invariant polynomials and the open problems for their
computation. The new polar shape distributions are easily computable and then
the inference procedure can be studied under exact densities. As an example of
the technique, a classical application in Biology is studied under three
models, the usual Gaussian and two non normal Kotz models; the best model is
selected by a modified BIC criterion, then a test for equality in polar shapes
is performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.0524</identifier>
 <datestamp>2010-04-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.0524</id><created>2010-04-04</created><authors><author><keyname>He</keyname><forenames>Yunxiao</forenames></author><author><keyname>Liu</keyname><forenames>Chuanhai</forenames></author></authors><title>The Dynamic ECME Algorithm</title><categories>stat.CO stat.ML</categories><comments>24 pages, 9 figures, and 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ECME algorithm has proven to be an effective way of accelerating the EM
algorithm for many problems. Recognising the limitation of using prefixed
acceleration subspace in ECME, we propose the new Dynamic ECME (DECME)
algorithm which allows the acceleration subspace to be chosen dynamically. Our
investigation of an inefficient special case of DECME, the classical Successive
Overrelaxation (SOR) method, leads to an efficient, simple, and widely
applicable DECME implementation, called DECME_v1. The fast convergence of
DECME_v1 is established by the theoretical result that, in a small
neighbourhood of the maximum likelihood estimate (MLE), DECME_v1 is equivalent
to a conjugate direction method. Numerical results show that DECME_v1 and its
two variants are very stable and often converge faster than EM by a factor of
one hundred in terms of number of iterations and a factor of thirty in terms of
CPU time when EM is very slow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.0533</identifier>
 <datestamp>2010-04-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.0533</id><created>2010-04-04</created><authors><author><keyname>Hosseini</keyname><forenames>Reza</forenames></author></authors><title>Quantiles Equivariance</title><categories>math.ST math.PR stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is widely claimed that the quantile function is equivariant under
increasing transformations. We show by a counterexample that this is not true
(even for strictly increasing transformations). However, we show that the
quantile function is equivariant under left continuous increasing
transformations. We also provide an equivariance relation for continuous
decreasing transformations. In the case that the transformation is not
continuous, we show that while the transformed quantile at p can be arbitrarily
far from the quantile of the transformed at p (in terms of absolute
difference), the probability mass between the two is zero. We also show by an
example that weighted definition of the median is not equivariant under even
strictly increasing continuous transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.0540</identifier>
 <datestamp>2010-04-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.0540</id><created>2010-04-04</created><authors><author><keyname>Hosseini</keyname><forenames>Reza</forenames></author></authors><title>Quantiles symmetry</title><categories>math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper finds a symmetry relation (between quantiles of a random variable
and its negative) that is intuitively appealing. We show this symmetry is quite
useful in finding new relations for quantiles, in particular an equivariance
property for quantiles under continuous decreasing transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.0552</identifier>
 <datestamp>2010-04-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.0552</id><created>2010-04-04</created><authors><author><keyname>Nikulin</keyname><forenames>Vladimir</forenames></author></authors><title>An Algorithm to Estimate a Nonuniform Convergence Bound in the Central
  Limit Theorem</title><categories>math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A nonuniform version of the Berry-Esseen bound has been proved. The most
important feature of the new bound is a monotonically decreasing function
C(|t|) instead of the universal constant C=29.1174: C(|t|)&lt;C if |t| &gt; 3.2, and
C(|t|) tends to 1+e if |t| is increasing.
  The function C(|t|) has very complex analytical expression based on indicator
functions. A general algorithm was developed in order to estimate values of
C(|t|) for an arbitrary t.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.0662</identifier>
 <datestamp>2010-04-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.0662</id><created>2010-04-05</created><authors><author><keyname>Sirota</keyname><forenames>E. Ostrovsky L.</forenames></author></authors><title>Adaptive optimal regularization of the linear ill posed integral
  equations</title><categories>math.NA math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct an adaptive asymptotically optimal in order in the $ L(2) $
sense a solution (estimation) of an integral linear equation of a first kind
and energy of this solution with the confidence region building, also adaptive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.0784</identifier>
 <datestamp>2011-05-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.0784</id><created>2010-04-06</created><updated>2011-05-24</updated><authors><author><keyname>Auffray</keyname><forenames>Yves</forenames></author><author><keyname>Barbillon</keyname><forenames>Pierre</forenames></author><author><keyname>Marin</keyname><forenames>Jean-Michel</forenames></author></authors><title>Maximin design on non hypercube domain and kernel interpolation</title><categories>stat.CO</categories><comments>3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paradigm of computer experiments, the choice of an experimental design
is an important issue. When no information is available about the black-box
function to be approximated, an exploratory design have to be used. In this
context, two dispersion criteria are usually considered: the minimax and the
maximin ones. In the case of a hypercube domain, a standard strategy consists
of taking the maximin design within the class of Latin hypercube designs.
However, in a non hypercube context, it does not make sense to use the Latin
hypercube strategy. Moreover, whatever the design is, the black-box function is
typically approximated thanks to kernel interpolation. Here, we first provide a
theoretical justification to the maximin criterion with respect to kernel
interpolations. Then, we propose simulated annealing algorithms to determine
maximin designs in any bounded connected domain. We prove the convergence of
the different schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.0858</identifier>
 <datestamp>2010-04-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.0858</id><created>2010-04-06</created><updated>2010-04-07</updated><authors><author><keyname>Golub</keyname><forenames>Benjamin</forenames></author><author><keyname>Livne</keyname><forenames>Yair</forenames></author></authors><title>Strategic Random Networks: Why Social Networking Technology Matters</title><categories>stat.AP math.PR physics.soc-ph</categories><comments>34 pages, 3 figures; v2 and v3 correct typographical errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops strategic foundations for an important statistical model
of random networks with heterogeneous expected degrees. Based on this, we show
how social networking services that subtly alter the costs and indirect
benefits of relationships can cause large changes in behavior and welfare. In
the model, agents who value friends and friends of friends choose how much to
socialize, which increases the probabilities of links but is costly. There is a
sharp transition from fragmented, sparse equilibrium networks to connected,
dense ones when the value of friends of friends crosses a cost-dependent
threshold. This transition mitigates an extreme inefficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.0911</identifier>
 <datestamp>2010-04-07</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.0911</id><created>2010-04-06</created><authors><author><keyname>Carrasco</keyname><forenames>Jalmar M. F.</forenames></author><author><keyname>Ferrari</keyname><forenames>Silvia L. P.</forenames></author><author><keyname>Cordeiro</keyname><forenames>Gauss M.</forenames></author></authors><title>A New Generalized Kumaraswamy Distribution</title><categories>stat.ME</categories><comments>Beta distribution; Continuous proportions; Generalized Kumaraswamy
  distribution; Kumaraswamy distribution; Maximum likelihood; McDonald
  Distribution; Moments.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new five-parameter continuous distribution which generalizes the
Kumaraswamy and the beta distributions as well as some other well-known
distributions is proposed and studied. The model has as special cases new four-
and three-parameter distributions on the standard unit interval. Moments, mean
deviations, R\'enyi's entropy and the moments of order statistics are obtained
for the new generalized Kumaraswamy distribution. The score function is given
and estimation is performed by maximum likelihood. Hypothesis testing is also
discussed. A data set is used to illustrate an application of the proposed
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.1234</identifier>
 <datestamp>2010-04-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.1234</id><created>2010-04-07</created><authors><author><keyname>Little</keyname><forenames>Max A.</forenames></author><author><keyname>Steel</keyname><forenames>Bradley C.</forenames></author><author><keyname>Bai</keyname><forenames>Fan</forenames></author><author><keyname>Sowa</keyname><forenames>Yoshiyuki</forenames></author><author><keyname>Bilyard</keyname><forenames>Thomas</forenames></author><author><keyname>Mueller</keyname><forenames>David M.</forenames></author><author><keyname>Berry</keyname><forenames>Richard M.</forenames></author><author><keyname>Jones</keyname><forenames>Nick S.</forenames></author></authors><title>Steps and bumps: precision extraction of discrete states of molecular
  machines using physically-based, high-throughput time series analysis</title><categories>q-bio.QM physics.data-an stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report new statistical time-series analysis tools providing significant
improvements in the rapid, precision extraction of discrete state dynamics from
large databases of experimental observations of molecular machines. By building
physical knowledge and statistical innovations into analysis tools, we
demonstrate new techniques for recovering discrete state transitions buried in
highly correlated molecular noise. We demonstrate the effectiveness of our
approach on simulated and real examples of step-like rotation of the bacterial
flagellar motor and the F1-ATPase enzyme. We show that our method can clearly
identify molecular steps, symmetries and cascaded processes that are too weak
for existing algorithms to detect, and can do so much faster than existing
algorithms. Our techniques represent a major advance in the drive towards
automated, precision, highthroughput studies of molecular machine dynamics.
Modular, open-source software that implements these techniques is provided at
http://www.eng.ox.ac.uk/samp/members/max/software/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.1341</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.1341</id><created>2010-04-08</created><authors><author><keyname>Jurman</keyname><forenames>Giuseppe</forenames></author><author><keyname>Riccadonna</keyname><forenames>Samantha</forenames></author><author><keyname>Visintainer</keyname><forenames>Roberto</forenames></author><author><keyname>Furlanello</keyname><forenames>Cesare</forenames></author></authors><title>Algebraic Comparison of Partial Lists in Bioinformatics</title><categories>stat.ML q-bio.QM</categories><journal-ref>PLoS ONE 7(5): e36540 (2012)</journal-ref><doi>10.1371/journal.pone.0036540</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The outcome of a functional genomics pipeline is usually a partial list of
genomic features, ranked by their relevance in modelling biological phenotype
in terms of a classification or regression model. Due to resampling protocols
or just within a meta-analysis comparison, instead of one list it is often the
case that sets of alternative feature lists (possibly of different lengths) are
obtained. Here we introduce a method, based on the algebraic theory of
symmetric groups, for studying the variability between lists ("list stability")
in the case of lists of unequal length. We provide algorithms evaluating
stability for lists embedded in the full feature set or just limited to the
features occurring in the partial lists. The method is demonstrated first on
synthetic data in a gene filtering task and then for finding gene profiles on a
recent prostate cancer dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.1605</identifier>
 <datestamp>2010-04-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.1605</id><created>2010-04-09</created><authors><author><keyname>Wang</keyname><forenames>Yan</forenames></author><author><keyname>Mei</keyname><forenames>Yajun</forenames></author></authors><title>Decentralized Multihypothesis Sequential Detection</title><categories>math.ST stat.TH</categories><comments>5 pages, 1 figures, conference paper for isit 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is concerned with decentralized sequential testing of multiple
hypotheses. In a sensor network system with limited local memory, raw
observations are observed at the local sensors, and quantized into binary
sensor messages that are sent to a fusion center, which makes a final decision.
It is assumed that the raw sensor observations are distributed according to a
set of M&gt;=2 specified distributions, and the fusion center has to utilize
quantized sensor messages to decide which one is the true distribution.
Asymptotically Bayes tests are offered for decentralized multihypothesis
sequential detection by combining three existing methodologies together: tandem
quantizers, unambiguous likelihood quantizers, and randomized quantizers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.1729</identifier>
 <datestamp>2011-02-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.1729</id><created>2010-04-10</created><authors><author><keyname>Kurant</keyname><forenames>Maciej</forenames></author><author><keyname>Markopoulou</keyname><forenames>Athina</forenames></author><author><keyname>Thiran</keyname><forenames>Patrick</forenames></author></authors><title>On the bias of BFS</title><categories>cs.DM cs.DS cs.NI cs.SI stat.ME</categories><comments>9 pages</comments><journal-ref>International Teletraffic Congress (ITC 22), 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Breadth First Search (BFS) and other graph traversal techniques are widely
used for measuring large unknown graphs, such as online social networks. It has
been empirically observed that an incomplete BFS is biased toward high degree
nodes. In contrast to more studied sampling techniques, such as random walks,
the precise bias of BFS has not been characterized to date. In this paper, we
quantify the degree bias of BFS sampling. In particular, we calculate the node
degree distribution expected to be observed by BFS as a function of the
fraction of covered nodes, in a random graph $RG(p_k)$ with a given degree
distribution $p_k$. Furthermore, we also show that, for $RG(p_k)$, all commonly
used graph traversal techniques (BFS, DFS, Forest Fire, and Snowball Sampling)
lead to the same bias, and we show how to correct for this bias. To give a
broader perspective, we compare this class of exploration techniques to random
walks that are well-studied and easier to analyze. Next, we study by simulation
the effect of graph properties not captured directly by our model. We find that
the bias gets amplified in graphs with strong positive assortativity. Finally,
we demonstrate the above results by sampling the Facebook social network, and
we provide some practical guidelines for graph sampling in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.1876</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.1876</id><created>2010-04-12</created><authors><author><keyname>Aoki</keyname><forenames>Satoshi</forenames></author><author><keyname>Takemura</keyname><forenames>Akimichi</forenames></author></authors><title>Design and analysis of fractional factorial experiments from the
  viewpoint of computational algebraic statistics</title><categories>stat.ME stat.CO</categories><comments>16 pages</comments><msc-class>62K15</msc-class><journal-ref>Journal of Statistical Theory and Practice, Vol. 6 (2012), No. 1,
  147--161,</journal-ref><doi>10.1080/15598608.2012.647556</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an expository review of applications of computational algebraic
statistics to design and analysis of fractional factorial experiments based on
our recent works. For the purpose of design, the techniques of Gr\"obner bases
and indicator functions allow us to treat fractional factorial designs without
distinction between regular designs and non-regular designs. For the purpose of
analysis of data from fractional factorial designs, the techniques of Markov
bases allow us to handle discrete observations. Thus the approach of
computational algebraic statistics greatly enlarges the scope of fractional
factorial designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.1951</identifier>
 <datestamp>2010-11-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.1951</id><created>2010-04-12</created><updated>2010-11-23</updated><authors><author><keyname>Andjel</keyname><forenames>Enrique</forenames></author><author><keyname>Mountford</keyname><forenames>Thomas</forenames></author><author><keyname>Pimentel</keyname><forenames>Leandro P. R.</forenames></author><author><keyname>Valesin</keyname><forenames>Daniel</forenames></author></authors><title>Tightness for the interface of the one-dimensional contact process</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/09-BEJ236 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ236</report-no><journal-ref>Bernoulli 2010, Vol. 16, No. 4, 909-925</journal-ref><doi>10.3150/09-BEJ236</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a symmetric, finite-range contact process with two types of
infection; both have the same (supercritical) infection rate and heal at rate
1, but sites infected by Infection 1 are immune to Infection 2. We take the
initial configuration where sites in $(-\infty,0]$ have Infection 1 and sites
in $[1,\infty)$ have Infection 2, then consider the process $\rho_t$ defined as
the size of the interface area between the two infections at time $t$. We show
that the distribution of $\rho_t$ is tight, thus proving a conjecture posed by
Cox and Durrett in [Bernoulli 1 (1995) 343--370].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.2000</identifier>
 <datestamp>2010-04-13</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.2000</id><created>2010-04-12</created><authors><author><keyname>Vu</keyname><forenames>Van</forenames></author></authors><title>Singular vectors under random perturbation</title><categories>math.NA math.CO math.PR math.ST stat.TH</categories><msc-class>65F15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing the first few singular vectors of a large matrix is a problem that
frequently comes up in statistics and numerical analysis. Given the presence of
noise, exact calculation is hard to achieve, and the following problem is of
importance:
  \vskip2mm \centerline {\it How much a small perturbation to the matrix
changes the singular vectors ?} \vskip2mm Answering this question, classical
theorems, such as those of Davis-Kahan and Wedin, give tight estimates for the
worst-case scenario. In this paper, we show that if the perturbation (noise) is
random and our matrix has low rank, then better estimates can be obtained. Our
method relies on high dimensional geometry and is different from those used an
earlier papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.2006</identifier>
 <datestamp>2011-05-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.2006</id><created>2010-04-12</created><updated>2011-05-25</updated><authors><author><keyname>Gagunashvili</keyname><forenames>Nikolai</forenames></author></authors><title>Machine learning approach to inverse problem and unfolding procedure</title><categories>physics.data-an astro-ph.IM hep-ex stat.AP stat.ML</categories><comments>19 pages, 7 figures</comments><msc-class>62-07 (Primary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A procedure for unfolding the true distribution from experimental data is
presented. Machine learning methods are applied for simultaneous identification
of an apparatus function and solving of an inverse problem. A priori
information about the true distribution from theory or previous experiments is
used for Monte-Carlo simulation of the training sample. The training sample can
be used to calculate a transformation from the true distribution to the
measured one. This transformation provides a robust solution for an unfolding
problem with minimal biases and statistical errors for the set of distributions
used to create the training sample. The dimensionality of the solved problem
can be arbitrary. A numerical example is presented to illustrate and validate
the procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.2027</identifier>
 <datestamp>2011-09-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.2027</id><created>2010-04-12</created><updated>2011-09-06</updated><authors><author><keyname>Azar</keyname><forenames>Mohammad Gheshlaghi</forenames></author><author><keyname>Gomez</keyname><forenames>Vicenc</forenames></author><author><keyname>Kappen</keyname><forenames>Hilbert J.</forenames></author></authors><title>Dynamic Policy Programming</title><categories>cs.LG cs.AI cs.SY math.OC stat.ML</categories><comments>Submitted to Journal of Machine Learning Research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel policy iteration method, called dynamic
policy programming (DPP), to estimate the optimal policy in the
infinite-horizon Markov decision processes. We prove the finite-iteration and
asymptotic l\infty-norm performance-loss bounds for DPP in the presence of
approximation/estimation error. The bounds are expressed in terms of the
l\infty-norm of the average accumulated error as opposed to the l\infty-norm of
the error in the case of the standard approximate value iteration (AVI) and the
approximate policy iteration (API). This suggests that DPP can achieve a better
performance than AVI and API since it averages out the simulation noise caused
by Monte-Carlo sampling throughout the learning process. We examine this
theoretical results numerically by com- paring the performance of the
approximate variants of DPP with existing reinforcement learning (RL) methods
on different problem domains. Our results show that, in all cases, DPP-based
algorithms outperform other RL methods by a wide margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.2138</identifier>
 <datestamp>2010-06-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.2138</id><created>2010-04-13</created><updated>2010-06-14</updated><authors><author><keyname>Lam</keyname><forenames>Clifford</forenames></author><author><keyname>Yao</keyname><forenames>Qiwei</forenames></author><author><keyname>Bathia</keyname><forenames>Neil</forenames></author></authors><title>Estimation for Latent Factor Models for High-Dimensional Time Series</title><categories>math.ST stat.ME stat.TH</categories><comments>35 pages article, 4 figures</comments><msc-class>62F12, 62H25, 62H12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the dimension reduction for high-dimensional time
series based on common factors. In particular we allow the dimension of time
series $p$ to be as large as, or even larger than, the sample size $n$. The
estimation for the factor loading matrix and the factor process itself is
carried out via an eigenanalysis for a $p\times p$ non-negative definite
matrix. We show that when all the factors are strong in the sense that the norm
of each column in the factor loading matrix is of the order $p^{1/2}$, the
estimator for the factor loading matrix, as well as the resulting estimator for
the precision matrix of the original $p$-variant time series, are weakly
consistent in $L_2$-norm with the convergence rates independent of $p$. This
result exhibits clearly that the `curse' is canceled out by the `blessings' in
dimensionality. We also establish the asymptotic properties of the estimation
when not all factors are strong. For the latter case, a two-step estimation
procedure is preferred accordingly to the asymptotic theory. The proposed
methods together with their asymptotic properties are further illustrated in a
simulation study. An application to a real data set is also reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.2287</identifier>
 <datestamp>2010-04-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.2287</id><created>2010-04-13</created><authors><author><keyname>Viallon</keyname><forenames>Vivian</forenames></author><author><keyname>Banerjee</keyname><forenames>Onureena</forenames></author><author><keyname>Rey</keyname><forenames>Gregoire</forenames></author><author><keyname>Jougla</keyname><forenames>Eric</forenames></author><author><keyname>Coste</keyname><forenames>Joel</forenames></author></authors><title>An empirical comparative study of approximate methods for binary
  graphical models; application to the search of associations among causes of
  death in French death certificates</title><categories>stat.ML stat.AP stat.ME</categories><comments>29 pages, 4 figures.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Looking for associations among multiple variables is a topical issue in
statistics due to the increasing amount of data encountered in biology,
medicine and many other domains involving statistical applications. Graphical
models have recently gained popularity for this purpose in the statistical
literature. Following the ideas of the LASSO procedure designed for the linear
regression framework, recent developments dealing with graphical model
selection have been based on $\ell_1$-penalization. In the binary case,
however, exact inference is generally very slow or even intractable because of
the form of the so-called log-partition function. Various approximate methods
have recently been proposed in the literature and the main objective of this
paper is to compare them. Through an extensive simulation study, we show that a
simple modification of a method relying on a Gaussian approximation achieves
good performance and is very fast. We present a real application in which we
search for associations among causes of death recorded on French death
certificates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.2304</identifier>
 <datestamp>2010-04-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.2304</id><created>2010-04-13</created><authors><author><keyname>Harrington</keyname><forenames>Patrick L.</forenames><suffix>Jr.</suffix></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Spatio-Temporal Graphical Model Selection</title><categories>stat.ML cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the topology of spatial interactions in
a discrete state, discrete time spatio-temporal graphical model where the
interactions affect the temporal evolution of each agent in a network. Among
other models, the susceptible, infected, recovered ($SIR$) model for
interaction events fall into this framework. We pose the problem as a structure
learning problem and solve it using an $\ell_1$-penalized likelihood convex
program. We evaluate the solution on a simulated spread of infectious over a
complex network. Our topology estimates outperform those of a standard spatial
Markov random field graphical model selection using $\ell_1$-regularized
logistic regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.2452</identifier>
 <datestamp>2011-06-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.2452</id><created>2010-04-14</created><updated>2010-05-04</updated><authors><author><keyname>Guta</keyname><forenames>Madalin</forenames></author><author><keyname>Butucea</keyname><forenames>Cristina</forenames></author></authors><title>Quantum U-statistics</title><categories>quant-ph math.ST stat.TH</categories><comments>30 pages, added section on quantum metrology</comments><journal-ref>J. Math. Phys. 51, 102202 (2010)</journal-ref><doi>10.1063/1.3476776</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of a $U$-statistic for an $n$-tuple of identical quantum systems
is introduced in analogy to the classical (commutative) case: given a
selfadjoint `kernel' $K$ acting on $(\mathbb{C}^{d})^{\otimes r}$ with $r&lt;n$,
we define the symmetric operator $U_{n}= {n \choose r} \sum_{\beta}K^{(\beta)}$
with $K^{(\beta)}$ being the kernel acting on the subset $\beta$ of $\{1,\dots
,n\}$. If the systems are prepared in the i.i.d state $\rho^{\otimes n}$ it is
shown that the sequence of properly normalised $U$-statistics converges in
moments to a linear combination of Hermite polynomials in canonical variables
of a CCR algebra defined through the Quantum Central Limit Theorem. In the
special cases of non-degenerate kernels and kernels of order $2$ it is shown
that the convergence holds in the stronger distribution sense. Two types of
applications in quantum statistics are described: testing beyond the two simple
hypotheses scenario, and quantum metrology with interacting hamiltonians.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.2468</identifier>
 <datestamp>2011-06-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.2468</id><created>2010-04-14</created><authors><author><keyname>Guta</keyname><forenames>Madalin</forenames></author><author><keyname>Kotlowski</keyname><forenames>Wojciech</forenames></author></authors><title>Quantum learning: optimal classification of qubit states</title><categories>quant-ph stat.ML</categories><comments>24 pages, 4 figures</comments><journal-ref>New J. Phys. 12 123032 (2010)</journal-ref><doi>10.1088/1367-2630/12/12/123032</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pattern recognition is a central topic in Learning Theory with numerous
applications such as voice and text recognition, image analysis, computer
diagnosis. The statistical set-up in classification is the following: we are
given an i.i.d. training set $(X_{1},Y_{1}),... (X_{n},Y_{n})$ where $X_{i}$
represents a feature and $Y_{i}\in \{0,1\}$ is a label attached to that
feature. The underlying joint distribution of $(X,Y)$ is unknown, but we can
learn about it from the training set and we aim at devising low error
classifiers $f:X\to Y$ used to predict the label of new incoming features.
  Here we solve a quantum analogue of this problem, namely the classification
of two arbitrary unknown qubit states. Given a number of `training' copies from
each of the states, we would like to `learn' about them by performing a
measurement on the training set. The outcome is then used to design mesurements
for the classification of future systems with unknown labels. We find the
asymptotically optimal classification strategy and show that typically, it
performs strictly better than a plug-in strategy based on state estimation.
  The figure of merit is the excess risk which is the difference between the
probability of error and the probability of error of the optimal measurement
when the states are known, that is the Helstrom measurement. We show that the
excess risk has rate $n^{-1}$ and compute the exact constant of the rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.2548</identifier>
 <datestamp>2010-04-16</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.2548</id><created>2010-04-15</created><authors><author><keyname>Peters</keyname><forenames>Gareth W.</forenames></author><author><keyname>Wthrich</keyname><forenames>Mario V.</forenames></author><author><keyname>Shevchenko</keyname><forenames>Pavel V.</forenames></author></authors><title>Chain ladder method: Bayesian bootstrap versus classical bootstrap</title><categories>q-fin.CP q-fin.RM stat.CO stat.ME</categories><journal-ref>Insurance: Mathematics and Economics (2010)</journal-ref><doi>10.1016/j.insmatheco.2010.03.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The intention of this paper is to estimate a Bayesian distribution-free chain
ladder (DFCL) model using approximate Bayesian computation (ABC) methodology.
We demonstrate how to estimate quantities of interest in claims reserving and
compare the estimates to those obtained from classical and credibility
approaches. In this context, a novel numerical procedure utilising Markov chain
Monte Carlo (MCMC), ABC and a Bayesian bootstrap procedure was developed in a
truly distribution-free setting. The ABC methodology arises because we work in
a distribution-free setting in which we make no parametric assumptions, meaning
we can not evaluate the likelihood point-wise or in this case simulate directly
from the likelihood model. The use of a bootstrap procedure allows us to
generate samples from the intractable likelihood without the requirement of
distributional assumptions, this is crucial to the ABC framework. The developed
methodology is used to obtain the empirical distribution of the DFCL model
parameters and the predictive distribution of the outstanding loss liabilities
conditional on the observed claims. We then estimate predictive Bayesian
capital estimates, the Value at Risk (VaR) and the mean square error of
prediction (MSEP). The latter is compared with the classical bootstrap and
credibility methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.2581</identifier>
 <datestamp>2011-08-19</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.2581</id><created>2010-04-15</created><updated>2011-03-14</updated><authors><author><keyname>Wendler</keyname><forenames>Martin</forenames></author></authors><title>Bahadur Representation for U-Quantiles of Dependent Data</title><categories>math.ST math.PR stat.TH</categories><msc-class>62G30, 62G20 (primary), 62M10 (secondary)</msc-class><journal-ref>J. Multivariate Anal. 102 (2011), no. 6, 1064-1079</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  U-quantiles are applied in robust statistics, like the Hodges-Lehmann
estimator of location for example. They have been analyzed in the case of
independent random variables with the help of a generalized Bahadur
representation. Our main aim is to extend these results to U-quantiles of
strongly mixing random variables and functionals of absolutely regular
sequences. We obtain the central limit theorem and the law of the iterated
logarithm for U-quantiles as straightforward corollaries. Furthermore, we
improve the existing result for sample quantiles of mixing data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.2593</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.2593</id><created>2010-04-15</created><updated>2012-04-30</updated><authors><author><keyname>Marichal</keyname><forenames>Jean-Luc</forenames></author><author><keyname>Mathonet</keyname><forenames>Pierre</forenames></author></authors><title>Symmetric approximations of pseudo-Boolean functions with applications
  to influence indexes</title><categories>math.OC cs.DM math.ST stat.TH</categories><msc-class>41A10, 93E24 (Primary) 62G30, 90B25, 91A12 (Secondary)</msc-class><journal-ref>Applied Mathematics Letters 25 (8) (2012) 1121-1126</journal-ref><doi>10.1016/j.aml.2012.02.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an index for measuring the influence of the k-th smallest
variable on a pseudo-Boolean function. This index is defined from a weighted
least squares approximation of the function by linear combinations of order
statistic functions. We give explicit expressions for both the index and the
approximation and discuss some properties of the index. Finally, we show that
this index subsumes the concept of system signature in engineering reliability
and that of cardinality index in decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.2910</identifier>
 <datestamp>2011-04-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.2910</id><created>2010-04-16</created><updated>2011-04-08</updated><authors><author><keyname>Harrison</keyname><forenames>Matthew T.</forenames></author></authors><title>Conservative Hypothesis Tests and Confidence Intervals using Importance
  Sampling</title><categories>stat.CO stat.ME</categories><comments>26 pages, 3 figures, 3 tables [significant rewrite of version 1,
  including additional examples, title change]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Importance sampling is a common technique for Monte Carlo approximation,
including Monte Carlo approximation of p-values. Here it is shown that a simple
correction of the usual importance sampling p-values creates valid p-values,
meaning that a hypothesis test created by rejecting the null when the p-value
is &lt;= alpha will also have a type I error rate &lt;= alpha. This correction uses
the importance weight of the original observation, which gives valuable
diagnostic information under the null hypothesis. Using the corrected p-values
can be crucial for multiple testing and also in problems where evaluating the
accuracy of importance sampling approximations is difficult. Inverting the
corrected p-values provides a useful way to create Monte Carlo confidence
intervals that maintain the nominal significance level and use only a single
Monte Carlo sample. Several applications are described, including accelerated
multiple testing for a large neurophysiological dataset and exact conditional
inference for a logistic regression model with nuisance parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.2995</identifier>
 <datestamp>2011-10-18</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.2995</id><created>2010-04-18</created><updated>2011-10-16</updated><authors><author><keyname>Bunea</keyname><forenames>Florentina</forenames></author><author><keyname>She</keyname><forenames>Yiyuan</forenames></author><author><keyname>Wegkamp</keyname><forenames>Marten H.</forenames></author></authors><title>Optimal selection of reduced rank estimators of high-dimensional
  matrices</title><categories>math.ST stat.ME stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/11-AOS876 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org) (some typos corrected)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS876</report-no><journal-ref>Annals of Statistics 2011, Vol. 39, No. 2, 1282-1309</journal-ref><doi>10.1214/11-AOS876</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new criterion, the Rank Selection Criterion (RSC), for
selecting the optimal reduced rank estimator of the coefficient matrix in
multivariate response regression models. The corresponding RSC estimator
minimizes the Frobenius norm of the fit plus a regularization term proportional
to the number of parameters in the reduced rank model. The rank of the RSC
estimator provides a consistent estimator of the rank of the coefficient
matrix; in general, the rank of our estimator is a consistent estimate of the
effective rank, which we define to be the number of singular values of the
target matrix that are appropriately large. The consistency results are valid
not only in the classic asymptotic regime, when $n$, the number of responses,
and $p$, the number of predictors, stay bounded, and $m$, the number of
observations, grows, but also when either, or both, $n$ and $p$ grow, possibly
much faster than $m$. We establish minimax optimal bounds on the mean squared
errors of our estimators. Our finite sample performance bounds for the RSC
estimator show that it achieves the optimal balance between the approximation
error and the penalty term. Furthermore, our procedure has very low
computational complexity, linear in the number of candidate models, making it
particularly appealing for large scale problems. We contrast our estimator with
the nuclear norm penalized least squares (NNP) estimator, which has an
inherently higher computational complexity than RSC, for multivariate
regression models. We show that NNP has estimation properties similar to those
of RSC, albeit under stronger conditions. However, it is not as parsimonious as
RSC. We offer a simple correction of the NNP estimator which leads to
consistent rank estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3101</identifier>
 <datestamp>2010-04-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3101</id><created>2010-04-19</created><authors><author><keyname>Nikulin</keyname><forenames>Vladimir</forenames></author><author><keyname>McLachlan</keyname><forenames>Geoffrey J.</forenames></author></authors><title>Strong Consistency of Prototype Based Clustering in Probabilistic Space</title><categories>stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we formulate in general terms an approach to prove strong
consistency of the Empirical Risk Minimisation inductive principle applied to
the prototype or distance based clustering. This approach was motivated by the
Divisive Information-Theoretic Feature Clustering model in probabilistic space
with Kullback-Leibler divergence which may be regarded as a special case within
the Clustering Minimisation framework. Also, we propose clustering
regularization restricting creation of additional clusters which are not
significant or are not essentially different comparing with existing clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3105</identifier>
 <datestamp>2010-04-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3105</id><created>2010-04-19</created><updated>2010-04-19</updated><authors><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author></authors><title>Fast normal random number generators on vector processors</title><categories>cs.DS math.NA stat.CO</categories><comments>An old Technical Report, not published elsewhere. 6 pages. For
  details see http://wwwmaths.anu.edu.au/~brent/pub/pub141.html</comments><report-no>Technical Report TR-CS-93-04, Computer Sciences Laboratory,
  Australian National University, March 1993.</report-no><msc-class>11K45</msc-class><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider pseudo-random number generators suitable for vector processors.
In particular, we describe vectorised implementations of the Box-Muller and
Polar methods, and show that they give good performance on the Fujitsu VP2200.
We also consider some other popular methods, e.g. the Ratio method of Kinderman
and Monahan (1977) (as improved by Leva (1992)), and the method of Von Neumann
and Forsythe, and show why they are unlikely to be competitive with the Polar
method on vector processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3114</identifier>
 <datestamp>2010-04-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3114</id><created>2010-04-19</created><authors><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author></authors><title>A fast vectorised implementation of Wallace's normal random number
  generator</title><categories>cs.DS math.NA stat.CO</categories><comments>An old Technical Report, not published elsewhere. 9 pages. For
  further details see http://wwwmaths.anu.edu.au/~brent/pub/pub170.html</comments><report-no>Technical Report TR-CS-97-07, Computer Sciences Laboratory,
  Australian National University, April 1997</report-no><msc-class>11K45</msc-class><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wallace has proposed a new class of pseudo-random generators for normal
variates. These generators do not require a stream of uniform pseudo-random
numbers, except for initialisation. The inner loops are essentially
matrix-vector multiplications and are very suitable for implementation on
vector processors or vector/parallel processors such as the Fujitsu VPP300. In
this report we outline Wallace's idea, consider some variations on it, and
describe a vectorised implementation RANN4 which is more than three times
faster than its best competitors (the Polar and Box-Muller methods) on the
Fujitsu VP2200 and VPP300.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3115</identifier>
 <datestamp>2010-04-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3115</id><created>2010-04-19</created><authors><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author></authors><title>Some long-period random number generators using shifts and xors</title><categories>cs.DS math.NT stat.CO</categories><comments>11 pages</comments><msc-class>11K45</msc-class><acm-class>G.3</acm-class><journal-ref>ANZIAM Journal 48 (CTAC2006), C188-C202, 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Marsaglia recently introduced a class of xorshift random number generators
(RNGs) with periods 2n-1 for n = 32, 64, etc. Here we give a generalisation of
Marsaglia's xorshift generators in order to obtain fast and high-quality RNGs
with extremely long periods. RNGs based on primitive trinomials may be
unsatisfactory because a trinomial has very small weight. In contrast, our
generators can be chosen so that their minimal polynomials have large weight
(number of nonzero terms). A computer search using Magma has found good
generators for n a power of two up to 4096. These have been implemented in a
free software package xorgens.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3146</identifier>
 <datestamp>2010-04-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3146</id><created>2010-04-19</created><authors><author><keyname>Devroye</keyname><forenames>Luc</forenames></author><author><keyname>Letac</keyname><forenames>Gerard</forenames></author></authors><title>Copulas in three dimensions with prescribed correlations</title><categories>math.ST stat.TH</categories><comments>15 pages, 2 figures</comments><msc-class>62J10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an arbitrary three-dimensional correlation matrix, we prove that there
exists a three-dimensional joint distribution for the random variable $(X,Y,Z)$
such that $X$,$Y$ and $Z$ are identically distributed with beta distribution
$\beta_{k,k}(dx)$ on $(0,1)$ if $k\geq 1/2$. This implies that any correlation
structure can be attained for three-dimensional copulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3148</identifier>
 <datestamp>2010-04-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3148</id><created>2010-04-19</created><authors><author><keyname>Letac</keyname><forenames>Gerard</forenames></author><author><keyname>Weso\lowski</keyname><forenames>Jacek</forenames></author></authors><title>Why Jordan algebras are natural in statistics:quadratic regression
  implies Wishart distributions</title><categories>math.ST stat.TH</categories><comments>11 pages</comments><msc-class>62H05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If the space $\mathcal{Q}$ of quadratic forms in $\mathbb{R}^n$ is splitted
in a direct sum $\mathcal{Q}_1\oplus...\oplus \mathcal{Q}_k$ and if $X$ and $Y$
are independent random variables of $\mathbb{R}^n$, assume that there exist a
real number $a$ such that $E(X|X+Y)=a(X+Y)$ and real distinct numbers
$b_1,...,b_k$ such that $E(q(X)|X+Y)=b_iq(X+Y)$ for any $q$ in $\mathcal{Q}_i.$
We prove that this happens only when $k=2$, when $\mathbb{R}^n$ can be
structured in a Euclidean Jordan algebra and when $X$ and $Y$ have Wishart
distributions corresponding to this structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3393</identifier>
 <datestamp>2010-04-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3393</id><created>2010-04-20</created><authors><author><keyname>Ruckdeschel</keyname><forenames>Peter</forenames></author></authors><title>Optimally (Distributional-)Robust Kalman Filtering</title><categories>math.ST stat.TH</categories><msc-class>93E11, 62F35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present optimality results for robust Kalman filtering where robustness is
understood in a distributional sense, i.e.; we enlarge the distribution
assumptions made in the ideal model by suitable neighborhoods. This allows for
outliers which in our context may be system-endogenous or -exogenous, which
induces the somewhat conflicting goals of tracking and attenuation. The
corresponding minimax MSE-problems are solved for both types of outliers
separately, resulting in closed-form saddle-points which consist of an
optimally-robust procedure and a corresponding least favorable outlier
situation. The results are valid in a surprisingly general setup of state space
models, which is not limited to a Euclidean or time-discrete framework. The
solution however involves computation of conditional means in the ideal model,
which may pose computational problems. In the particular situation that the
ideal conditional mean is linear in the observation innovation, we come up with
a straight-forward Huberization, the rLS filter, which is very easy to compute.
For this linearity we obtain an again surprising characterization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3441</identifier>
 <datestamp>2011-10-31</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3441</id><created>2010-04-20</created><updated>2010-08-24</updated><authors><author><keyname>Sun</keyname><forenames>Wenxiang</forenames></author><author><keyname>Tian</keyname><forenames>Xueting</forenames></author></authors><title>Dominated Splitting and Pesin's Entropy Formula</title><categories>math.DS math-ph math.MP math.ST physics.data-an stat.TH</categories><msc-class>37A05, 37A05, 37A35, 37D25, 37D30</msc-class><journal-ref>Discrete &amp; Continuous Dynamical Systems-A vol. 32-4 April 2012;
  1421-1434</journal-ref><doi>10.3934/dcds.2012.32.1421</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $M$ be a compact manifold and $f:\,M\to M$ be a $C^1$ diffeomorphism on
$M$. If $\mu$ is an $f$-invariant probability measure which is absolutely
continuous relative to Lebesgue measure and for $\mu$ $a.\,\,e.\,\,x\in M,$
there is a dominated splitting $T_{orb(x)}M=E\oplus F$ on its orbit $orb(x)$,
then we give an estimation through Lyapunov characteristic exponents from below
in Pesin's entropy formula, i.e., the metric entropy $h_\mu(f)$ satisfies
$$h_{\mu}(f)\geq\int \chi(x)d\mu,$$ where
$\chi(x)=\sum_{i=1}^{dim\,F(x)}\lambda_i(x)$ and
$\lambda_1(x)\geq\lambda_2(x)\geq...\geq\lambda_{dim\,M}(x)$ are the Lyapunov
exponents at $x$ with respect to $\mu.$ Consequently, by using a dichotomy for
generic volume-preserving diffeomorphism we show that Pesin's entropy formula
holds for generic volume-preserving diffeomorphisms, which generalizes a result
of Tahzibi in dimension 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3476</identifier>
 <datestamp>2010-04-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3476</id><created>2010-04-20</created><authors><author><keyname>Koyama</keyname><forenames>Shinsuke</forenames></author><author><keyname>Prez-Bolde</keyname><forenames>Lucia Castellanos</forenames></author><author><keyname>Shalizi</keyname><forenames>Cosma Rohilla</forenames></author><author><keyname>Kass</keyname><forenames>Robert E.</forenames></author></authors><title>Approximate Methods for State-Space Models</title><categories>stat.ME physics.data-an q-bio.NC</categories><comments>31 pages, 4 figures. Different pagination from journal version due to
  incompatible style files but same content; the supplemental file for the
  journal appears here as appendices B--E.</comments><journal-ref>Journal of the American Statistical Association, volume 105, 2010,
  pp. 170--180</journal-ref><doi>10.1198/jasa.2009.tm08326</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-space models provide an important body of techniques for analyzing
time-series, but their use requires estimating unobserved states. The optimal
estimate of the state is its conditional expectation given the observation
histories, and computing this expectation is hard when there are
nonlinearities. Existing filtering methods, including sequential Monte Carlo,
tend to be either inaccurate or slow. In this paper, we study a nonlinear
filter for nonlinear/non-Gaussian state-space models, which uses Laplace's
method, an asymptotic series expansion, to approximate the state's conditional
mean and variance, together with a Gaussian conditional distribution. This {\em
Laplace-Gaussian filter} (LGF) gives fast, recursive, deterministic state
estimates, with an error which is set by the stochastic characteristics of the
model and is, we show, stable over time. We illustrate the estimation ability
of the LGF by applying it to the problem of neural decoding and compare it to
sequential Monte Carlo both in simulations and with real data. We find that the
LGF can deliver superior results in a small fraction of the computing time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3484</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3484</id><created>2010-04-20</created><updated>2010-12-15</updated><authors><author><keyname>Vershynin</keyname><forenames>Roman</forenames></author></authors><title>How close is the sample covariance matrix to the actual covariance
  matrix?</title><categories>math.PR math.FA math.ST stat.TH</categories><comments>34 pages. Typos and minor inaccuracies corrected, references updated</comments><msc-class>60H12, 60B20, 46B09</msc-class><journal-ref>Journal of Theoretical Probability 25 (2012), 655--686</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a probability distribution in R^n with general (non-white) covariance,
a classical estimator of the covariance matrix is the sample covariance matrix
obtained from a sample of N independent points. What is the optimal sample size
N = N(n) that guarantees estimation with a fixed accuracy in the operator norm?
Suppose the distribution is supported in a centered Euclidean ball of radius
\sqrt{n}. We conjecture that the optimal sample size is N = O(n) for all
distributions with finite fourth moment, and we prove this up to an iterated
logarithmic factor. This problem is motivated by the optimal theorem of
Rudelson which states that N = O(n \log n) for distributions with finite second
moment, and a recent result of Adamczak, Litvak, Pajor and Tomczak-Jaegermann
which guarantees that N = O(n) for sub-exponential distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3599</identifier>
 <datestamp>2012-01-11</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3599</id><created>2010-04-20</created><updated>2010-08-04</updated><authors><author><keyname>Takemura</keyname><forenames>Akimichi</forenames></author><author><keyname>Hara</keyname><forenames>Hisayuki</forenames></author></authors><title>Markov chain Monte Carlo test of toric homogeneous Markov chains</title><categories>math.ST stat.TH</categories><comments>lots of TikZ figures</comments><msc-class>62G17, 62H15</msc-class><journal-ref>Statistical Methodology 9 (2012) 392-406</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov chain models are used in various fields, such behavioral sciences or
econometrics. Although the goodness of fit of the model is usually assessed by
large sample approximation, it is desirable to use conditional tests if the
sample size is not large. We study Markov bases for performing conditional
tests of the toric homogeneous Markov chain model, which is the envelope
exponential family for the usual homogeneous Markov chain model. We give a
complete description of a Markov basis for the following cases: i) two-state,
arbitrary length, ii) arbitrary finite state space and length of three. The
general case remains to be a conjecture. We also present a numerical example of
conditional tests based on our Markov basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3616</identifier>
 <datestamp>2010-04-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3616</id><created>2010-04-21</created><authors><author><keyname>Meyer</keyname><forenames>Christian</forenames></author></authors><title>Recursive Numerical Evaluation of the Cumulative Bivariate Normal
  Distribution</title><categories>math.NA stat.CO</categories><comments>12 pages, 1 figure</comments><msc-class>65D15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an algorithm for evaluation of the cumulative bivariate normal
distribution, building upon Marsaglia's ideas for evaluation of the cumulative
univariate normal distribution. The algorithm is mathematically transparent,
delivers competitive performance and can easily be extended to arbitrary
precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3717</identifier>
 <datestamp>2010-04-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3717</id><created>2010-04-21</created><authors><author><keyname>Bahamonde</keyname><forenames>Natalia</forenames></author><author><keyname>Doukhan</keyname><forenames>Paul</forenames></author><author><keyname>Moulines</keyname><forenames>Eric</forenames></author></authors><title>Estimation of the autocovariance function with missing observations</title><categories>stat.ME</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel estimator of the autocorrelation function in presence of
missing observations. We establish the consistency, the asymptotic normality,
and we derive deviation bounds for various classes of weakly dependent
stationary time series, including causal or non causal models. In addition, we
introduce a modified version periodogram defined from these autocorrelation
estimators and derive asymptotic distribution of linear functionals of this
estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3726</identifier>
 <datestamp>2010-04-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3726</id><created>2010-04-21</created><authors><author><keyname>Mari</keyname><forenames>Dominique Drouet</forenames></author><author><keyname>Monbet</keyname><forenames>Valerie</forenames></author></authors><title>Using a priori knowledge to construct copulas</title><categories>stat.ME stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our purpose is to model the dependence between two random variables, taking
into account a priori knowledge on these variables. For example, in many
applications (oceanography, finance...), there exists an order relation between
the two variables; when one takes high values, the other cannot take low
values, but the contrary is possible. The dependence for the high values of the
two variables is, therefore, not symmetric.
  However a minimal dependence also exists: low values of one variable are
associated with low values of the other variable. The dependence can also be
extreme for the maxima or the minima of the two variables. In this paper, we
construct step by step asymmetric copulas with asymptotic minimal dependence,
and with or without asymptotic maximal dependence, using mixture variables to
get at first asymmetric dependence and then minimal dependence. We fit these
models to a real dataset of sea states and compare them using Likelihood Ratio
Tests when they are nested, and BIC- criterion (Bayesian Information criterion)
otherwise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3794</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3794</id><created>2010-04-21</created><updated>2011-09-13</updated><authors><author><keyname>Buscemi</keyname><forenames>Francesco</forenames></author></authors><title>Comparison of quantum statistical models: equivalent conditions for
  sufficiency</title><categories>quant-ph math.ST stat.TH</categories><comments>v4: final version to appear on Communications in Mathematical
  Physics. v3: submitted version, further improvements and results added, still
  23 pages. v2: presentation improved and new results added, now 23 pages. v1:
  20 pages, article class, no figures</comments><journal-ref>Commun. Math. Phys. 310, 625-647 (2012)</journal-ref><doi>10.1007/s00220-012-1421-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A family of probability distributions (i.e. a statistical model) is said to
be sufficient for another, if there exists a transition matrix transforming the
probability distributions in the former to the probability distributions in the
latter. The Blackwell-Sherman-Stein (BSS) theorem provides necessary and
sufficient conditions for one statistical model to be sufficient for another,
by comparing their information values in statistical decision problems. In this
paper we extend the BSS theorem to quantum statistical decision theory, where
statistical models are replaced by families of density matrices defined on
finite-dimensional Hilbert spaces, and transition matrices are replaced by
completely positive, trace-preserving maps (i.e. coarse-grainings). The
framework we propose is suitable for unifying results that previously were
independent, like the BSS theorem for classical statistical models and its
analogue for pairs of bipartite quantum states, recently proved by Shmaya. An
important role in this paper is played by statistical morphisms, namely, affine
maps whose definition generalizes that of coarse-grainings given by Petz and
induces a corresponding criterion for statistical sufficiency that is weaker,
and hence easier to be characterized, than Petz's.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3830</identifier>
 <datestamp>2010-04-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3830</id><created>2010-04-21</created><authors><author><keyname>Peters</keyname><forenames>Gareth W.</forenames></author><author><keyname>Kannan</keyname><forenames>Balakrishnan</forenames></author><author><keyname>Lasscock</keyname><forenames>Ben</forenames></author><author><keyname>Mellen</keyname><forenames>Chris</forenames></author></authors><title>Model Selection and Adaptive Markov chain Monte Carlo for Bayesian
  Cointegrated VAR model</title><categories>q-fin.CP q-fin.PM q-fin.ST stat.CO stat.ME</categories><comments>to appear journal Bayesian Analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a matrix-variate adaptive Markov chain Monte Carlo (MCMC)
methodology for Bayesian Cointegrated Vector Auto Regressions (CVAR). We
replace the popular approach to sampling Bayesian CVAR models, involving griddy
Gibbs, with an automated efficient alternative, based on the Adaptive
Metropolis algorithm of Roberts and Rosenthal, (2009). Developing the adaptive
MCMC framework for Bayesian CVAR models allows for efficient estimation of
posterior parameters in significantly higher dimensional CVAR series than
previously possible with existing griddy Gibbs samplers. For a n-dimensional
CVAR series, the matrix-variate posterior is in dimension $3n^2 + n$, with
significant correlation present between the blocks of matrix random variables.
We also treat the rank of the CVAR model as a random variable and perform joint
inference on the rank and model parameters. This is achieved with a Bayesian
posterior distribution defined over both the rank and the CVAR model
parameters, and inference is made via Bayes Factor analysis of rank.
Practically the adaptive sampler also aids in the development of automated
Bayesian cointegration models for algorithmic trading systems considering
instruments made up of several assets, such as currency baskets. Previously the
literature on financial applications of CVAR trading models typically only
considers pairs trading (n=2) due to the computational cost of the griddy
Gibbs. We are able to extend under our adaptive framework to $n &gt;&gt; 2$ and
demonstrate an example with n = 10, resulting in a posterior distribution with
parameters up to dimension 310. By also considering the rank as a random
quantity we can ensure our resulting trading models are able to adjust to
potentially time varying market conditions in a coherent statistical framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3871</identifier>
 <datestamp>2012-05-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3871</id><created>2010-04-22</created><updated>2010-10-03</updated><authors><author><keyname>Picchini</keyname><forenames>Umberto</forenames></author><author><keyname>Ditlevsen</keyname><forenames>Susanne</forenames></author></authors><title>Practical Estimation of High Dimensional Stochastic Differential
  Mixed-Effects Models</title><categories>stat.CO math.DS stat.ME</categories><comments>Forthcoming in "Computational Statistics &amp; Data Analysis"</comments><journal-ref>Computational Statistics &amp; Data Analysis, 2011, Volume 55, Issue
  3, pages 1426-1444</journal-ref><doi>10.1016/j.csda.2010.10.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic differential equations (SDEs) are established tools to model
physical phenomena whose dynamics are affected by random noise. By estimating
parameters of an SDE intrinsic randomness of a system around its drift can be
identified and separated from the drift itself. When it is of interest to model
dynamics within a given population, i.e. to model simultaneously the
performance of several experiments or subjects, mixed-effects modelling allows
for the distinction of between and within experiment variability. A framework
to model dynamics within a population using SDEs is proposed, representing
simultaneously several sources of variation: variability between experiments
using a mixed-effects approach and stochasticity in the individual dynamics
using SDEs. These "stochastic differential mixed-effects models" have
applications in e.g. pharmacokinetics/pharmacodynamics and biomedical
modelling. A parameter estimation method is proposed and computational
guidelines for an efficient implementation are given. Finally the method is
evaluated using simulations from standard models like the two-dimensional
Ornstein-Uhlenbeck (OU) and the square root models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3895</identifier>
 <datestamp>2010-04-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3895</id><created>2010-04-22</created><authors><author><keyname>Ruckdeschel</keyname><forenames>Peter</forenames></author></authors><title>Optimally Robust Kalman Filtering at Work: AO-, IO-, and Simultaneously
  IO- and AO- Robust Filters</title><categories>stat.CO</categories><msc-class>93E11, 62F35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We take up optimality results for robust Kalman filtering from
Ruckdeschel[2001,2010] where robustness is understood in a distributional
sense, i.e.; we enlarge the distribution assumptions made in the ideal model by
suitable neighborhoods, allowing for outliers which in our context may be
system-endogenous/propagating or -exogenous/non-propagating, inducing the
somewhat conflicting goals of tracking and attenuation. Correspondingly, the
cited references provide optimally-robust procedures to deal with each type of
outliers separately, but in case of IO-robustness does not say much about the
implementation. We discuss this in more detail in this paper. Most importantly,
we define a hybrid filter combining AO- and IO-optimal ones, which is able to
treat both types of outliers simultaneously, albeit with a certain delay. We
check our filters at a reference state space model, and compare the results
with those obtained by the ACM filter Martin and Masreliez[1977], Martin[1979]
and non-parametric, repeated-median based filters Fried et al.[2006,2007].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3925</identifier>
 <datestamp>2010-06-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3925</id><created>2010-04-22</created><updated>2010-06-01</updated><authors><author><keyname>Friel</keyname><forenames>Nial</forenames></author><author><keyname>Pettitt</keyname><forenames>Anthony N.</forenames></author></authors><title>Classification using distance nearest neighbours</title><categories>stat.CO stat.AP</categories><comments>12 pages, 2 figures. To appear in Statistics and Computing</comments><doi>10.1007/s11222-010-9179-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new probabilistic classification algorithm using a
Markov random field approach. The joint distribution of class labels is
explicitly modelled using the distances between feature vectors. Intuitively, a
class label should depend more on class labels which are closer in the feature
space, than those which are further away. Our approach builds on previous work
by Holmes and Adams (2002, 2003) and Cucala et al. (2008). Our work shares many
of the advantages of these approaches in providing a probabilistic basis for
the statistical inference. In comparison to previous work, we present a more
efficient computational algorithm to overcome the intractability of the Markov
random field model. The results of our algorithm are encouraging in comparison
to the k-nearest neighbour algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.3938</identifier>
 <datestamp>2011-11-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.3938</id><created>2010-04-22</created><updated>2011-11-21</updated><authors><author><keyname>Frahm</keyname><forenames>Gabriel</forenames></author><author><keyname>Glombek</keyname><forenames>Konstantin</forenames></author></authors><title>Semicircle Law for Tyler's M-Estimator of Scatter</title><categories>math.ST math.PR stat.TH</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show convergence in probability of the spectral distribution of Tyler's
M-estimator for scatter to the semicircle law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4027</identifier>
 <datestamp>2010-07-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4027</id><created>2010-04-22</created><updated>2010-07-05</updated><authors><author><keyname>Gramacy</keyname><forenames>Robert B.</forenames></author><author><keyname>Lee</keyname><forenames>Herbert K. H.</forenames></author></authors><title>Optimization Under Unknown Constraints</title><categories>stat.ME stat.AP</categories><comments>19 pages, 8 figures, Valencia discussion paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimization of complex functions, such as the output of computer simulators,
is a difficult task that has received much attention in the literature. A less
studied problem is that of optimization under unknown constraints, i.e., when
the simulator must be invoked both to determine the typical real-valued
response and to determine if a constraint has been violated, either for
physical or policy reasons. We develop a statistical approach based on Gaussian
processes and Bayesian learning to both approximate the unknown function and
estimate the probability of meeting the constraints. A new integrated
improvement criterion is proposed to recognize that responses from inputs that
violate the constraint may still be informative about the function, and thus
could potentially be useful in the optimization. The new criterion is
illustrated on synthetic data, and on a motivating optimization problem from
health care policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4041</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4041</id><created>2010-04-22</created><updated>2010-04-25</updated><authors><author><keyname>Kanamori</keyname><forenames>Takafumi</forenames></author><author><keyname>Uehara</keyname><forenames>Hiroaki</forenames></author><author><keyname>Jimbo</keyname><forenames>Masakazu</forenames></author></authors><title>Pooling Design and Bias Correction in DNA Library Screening</title><categories>stat.CO q-bio.QM</categories><comments>18 pages, 1 figure, 8 tables, submitted.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the group test for DNA library screening based on probabilistic
approach. Group test is a method of detecting a few positive items from among a
large number of items, and has wide range of applications. In DNA library
screening, positive item corresponds to the clone having a specified DNA
segment, and it is necessary to identify and isolate the positive clones for
compiling the libraries. In the group test, a group of items, called pool, is
assayed in a lump in order to save the cost of testing, and positive items are
detected based on the observation from each pool. It is known that the design
of grouping, that is, pooling design is important to %reduce the estimation
bias and achieve accurate detection. In the probabilistic approach, positive
clones are picked up based on the posterior probability. Naive methods of
computing the posterior, however, involves exponentially many sums, and thus we
need a device. Loopy belief propagation (loopy BP) algorithm is one of popular
methods to obtain approximate posterior probability efficiently. There are some
works investigating the relation between the accuracy of the loopy BP and the
pooling design. Based on these works, we develop pooling design with small
estimation bias of posterior probability, and we show that the balanced
incomplete block design (BIBD) has nice property for our purpose. Some
numerical experiments show that the bias correction under the BIBD is useful to
improve the estimation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4116</identifier>
 <datestamp>2010-04-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4116</id><created>2010-04-13</created><authors><author><keyname>Barbour</keyname><forenames>A. D.</forenames></author><author><keyname>Tavar</keyname><forenames>Simon</forenames></author></authors><title>Assessing molecular variability in cancer genomes</title><categories>q-bio.PE stat.CO</categories><comments>22 pages, 1 figure. Chapter 4 of "Probability and Mathematical
  Genetics: Papers in Honour of Sir John Kingman" (Editors N.H. Bingham and
  C.M. Goldie), Cambridge University Press, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamics of tumour evolution are not well understood. In this paper we
provide a statistical framework for evaluating the molecular variation observed
in different parts of a colorectal tumour. A multi-sample version of the Ewens
Sampling Formula forms the basis for our modelling of the data, and we provide
a simulation procedure for use in obtaining reference distributions for the
statistics of interest. We also describe the large-sample asymptotics of the
joint distributions of the variation observed in different parts of the tumour.
While actual data should be evaluated with reference to the simulation
procedure, the asymptotics serve to provide theoretical guidelines, for
instance with reference to the choice of possible statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4258</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4258</id><created>2010-04-24</created><authors><author><keyname>Bar-Lev</keyname><forenames>Shaul K.</forenames></author><author><keyname>Letac</keyname><forenames>Gerard</forenames></author></authors><title>Increasing hazard rate of mixtures for natural exponential families</title><categories>math.ST stat.TH</categories><comments>21 pages</comments><msc-class>62N05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hazard rates play an important role in various areas, e.g., reliability
theory, survival analysis, biostatistics, queueing theory and actuarial
studies. Mixtures of distributions are also of a great preeminence in such
areas as most populations of components are indeed heterogeneous. In this study
we present a sufficient condition for mixtures of two elements\ of the same
natural exponential family (NEF) to have an increasing hazard rate. We then
apply this condition to some classical NEF's having either quadratic, or cubic
variance functions (VF) and others as well. A particular attention is devoted
to the hyperbolic cosine NEF having a quadratic VF, the Ressel NEF having a
cubic VF and to the Kummer distributions of type 2 NEF. The application of such
a sufficient condition is quite intricate and cumbersome, in particular when
applied to the latter three NEF's. Various lemmas and propositions are needed
then to verify this condition for these NEF's.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4314</identifier>
 <datestamp>2012-11-26</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4314</id><created>2010-04-24</created><updated>2012-11-23</updated><authors><author><keyname>Fasano</keyname><forenames>Mara V.</forenames></author><author><keyname>Maronna</keyname><forenames>Ricardo A.</forenames></author><author><keyname>Sued</keyname><forenames>Mariela</forenames></author><author><keyname>Yohai</keyname><forenames>Vctor J.</forenames></author></authors><title>Continuity and differentiability of regression M functionals</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/11-BEJ368 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ368</report-no><journal-ref>Bernoulli 2012, Vol. 18, No. 4, 1284-1309</journal-ref><doi>10.3150/11-BEJ368</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the Fisher-consistency, weak continuity and
differentiability of estimating functionals corresponding to a class of both
linear and nonlinear regression high breakdown M estimates, which includes S
and MM estimates. A restricted type of differentiability, called weak
differentiability, is defined, which suffices to prove the asymptotic normality
of estimates based on the functionals. This approach allows to prove the
consistency, asymptotic normality and qualitative robustness of M estimates
under more general conditions than those required in standard approaches. In
particular, we prove that regression MM-estimates are asymptotically normal
when the observations are $\phi$-mixing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4360</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4360</id><created>2010-04-25</created><updated>2012-03-05</updated><authors><author><keyname>Zwiernik</keyname><forenames>Piotr</forenames></author><author><keyname>Smith</keyname><forenames>Jim Q.</forenames></author></authors><title>Tree cumulants and the geometry of binary tree models</title><categories>math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ338 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ338</report-no><journal-ref>Bernoulli 2012, Vol. 18, No. 1, 290-321</journal-ref><doi>10.3150/10-BEJ338</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate undirected discrete graphical tree models when
all the variables in the system are binary, where leaves represent the
observable variables and where all the inner nodes are unobserved. A novel
approach based on the theory of partially ordered sets allows us to obtain a
convenient parametrization of this model class. The construction of the
proposed coordinate system mirrors the combinatorial definition of cumulants. A
simple product-like form of the resulting parametrization gives insight into
identifiability issues associated with this model class. In particular, we
provide necessary and sufficient conditions for such a model to be identified
up to the switching of labels of the inner nodes. When these conditions hold,
we give explicit formulas for the parameters of the model. Whenever the model
fails to be identified, we use the new parametrization to describe the geometry
of the unidentified parameter space. We illustrate these results using a simple
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4391</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4391</id><created>2010-04-25</created><authors><author><keyname>Novikov</keyname><forenames>Andrey</forenames></author><author><keyname>Novikov</keyname><forenames>Petr</forenames></author></authors><title>Locally most powerful sequential tests of a simple hypothesis vs.
  One-sided alternatives for independent observations</title><categories>stat.ME math.ST stat.TH</categories><comments>26 pages</comments><msc-class>62L10, 62L15, 60G40, 62C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $X_1,X_2,..., X_n,...$ be a stochastic process with independent values
whose distribution $P_\theta$ depends on an unknown parameter $\theta$,
$\theta\in\Theta$, where $\Theta$ is an open subset of the real line. The
problem of testing $H_0:$ $\theta=\theta_0$ vs. a composite alternative $H_1:$
$\theta&gt;\theta_0$ is considered, where $\theta_0\in\Theta$ is a fixed value of
the parameter. The main objective of this work is the characterization of the
structure of the locally most powerful (in the sense of Berk) sequential tests
in this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4522</identifier>
 <datestamp>2012-09-10</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4522</id><created>2010-04-26</created><authors><author><keyname>Snarska</keyname><forenames>Magorzata</forenames></author></authors><title>Toy Model for Large Non-Symmetric Random Matrices</title><categories>physics.data-an q-fin.ST stat.ME</categories><comments>5 pages, 3 figures, Proceedings of the 3rd Polish Symposium on Econo-
  and Sociophysics, Wroclaw 2007,</comments><journal-ref>Acta Physica Polonica A, 2008 Vol.114 Issue 3 p.555 - 559</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-symmetric rectangular correlation matrices occur in many problems in
economics. We test the method of extracting statistically meaningful
correlations between input and output variables of large dimensionality and
build a toy model for artificially included correlations in large random time
series.The results are then applied to analysis of polish macroeconomic data
and can be used as an alternative to classical cointegration approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4668</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4668</id><created>2010-04-26</created><updated>2012-08-03</updated><authors><author><keyname>Jones</keyname><forenames>Nick S.</forenames></author><author><keyname>Moriarty</keyname><forenames>John</forenames></author></authors><title>Evolutionary Inference for Function-valued Traits: Gaussian Process
  Regression on Phylogenies</title><categories>q-bio.QM cs.LG physics.data-an stat.ML</categories><comments>7 pages, 1 figure</comments><journal-ref>Journal of the Royal Society Interface vol. 10 no. 78 20120616
  (2013)</journal-ref><doi>10.1098/rsif.2012.0616</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biological data objects often have both of the following features: (i) they
are functions rather than single numbers or vectors, and (ii) they are
correlated due to phylogenetic relationships. In this paper we give a flexible
statistical model for such data, by combining assumptions from phylogenetics
with Gaussian processes. We describe its use as a nonparametric Bayesian prior
distribution, both for prediction (placing posterior distributions on ancestral
functions) and model selection (comparing rates of evolution across a
phylogeny, or identifying the most likely phylogenies consistent with the
observed data). Our work is integrative, extending the popular phylogenetic
Brownian Motion and Ornstein-Uhlenbeck models to functional data and Bayesian
inference, and extending Gaussian Process regression to phylogenies. We provide
a brief illustration of the application of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4686</identifier>
 <datestamp>2011-06-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4686</id><created>2010-04-26</created><authors><author><keyname>Srivastava</keyname><forenames>Radhendushka</forenames></author><author><keyname>Sengupta</keyname><forenames>Debasis</forenames></author></authors><title>Effect of inter-sample spacing constraint on spectrum estimation with
  irregular sampling</title><categories>math.ST stat.TH</categories><doi>10.1109/TIT.2011.2146510</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A practical constraint that comes in the way of spectrum estimation of a
continuous time stationary stochastic process is the minimum separation between
successively observed samples of the process. When the underlying process is
not band-limited, sampling at any uniform rate leads to aliasing, while certain
stochastic sampling schemes, including Poisson process sampling, are rendered
infeasible by the constraint of minimum separation. It is shown in this paper
that, subject to this constraint, no point process sampling scheme is
alias-free for the class of all spectra. It turns out that point process
sampling under this constraint can be alias-free for band-limited spectra.
However, the usual construction of a consistent spectrum estimator does not
work in such a case. Simulations indicate that a commonly used estimator, which
is consistent in the absence of this constraint, performs poorly when the
constraint is present. These results should help practitioners in rationalizing
their expectations from point process sampling as far as spectrum estimation is
concerned, and motivate researchers to look for appropriate estimators of
bandlimited spectra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4704</identifier>
 <datestamp>2011-05-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4704</id><created>2010-04-27</created><updated>2010-11-29</updated><authors><author><keyname>Shalizi</keyname><forenames>Cosma Rohilla</forenames></author><author><keyname>Thomas</keyname><forenames>Andrew C.</forenames></author></authors><title>Homophily and Contagion Are Generically Confounded in Observational
  Social Network Studies</title><categories>stat.AP cs.SI physics.data-an physics.soc-ph</categories><comments>27 pages, 9 figures. V2: Revised in response to referees. V3: Ditto</comments><journal-ref>Sociological Methods and Research, vol. 40 (2011), pp. 211--239</journal-ref><doi>10.1177/0049124111404820</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider processes on social networks that can potentially involve three
factors: homophily, or the formation of social ties due to matching individual
traits; social contagion, also known as social influence; and the causal effect
of an individual's covariates on their behavior or other measurable responses.
We show that, generically, all of these are confounded with each other.
Distinguishing them from one another requires strong assumptions on the
parametrization of the social process or on the adequacy of the covariates used
(or both). In particular we demonstrate, with simple examples, that asymmetries
in regression coefficients cannot identify causal effects, and that very simple
models of imitation (a form of social contagion) can produce substantial
correlations between an individual's enduring traits and their choices, even
when there is no intrinsic affinity between them. We also suggest some possible
constructive responses to these results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4956</identifier>
 <datestamp>2010-04-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4956</id><created>2010-04-28</created><authors><author><keyname>Fan</keyname><forenames>Jianqing</forenames></author><author><keyname>Li</keyname><forenames>Yingying</forenames></author><author><keyname>Yu</keyname><forenames>Ke</forenames></author></authors><title>Vast Volatility Matrix Estimation using High Frequency Data for
  Portfolio Selection</title><categories>q-fin.PM math.ST q-fin.ST stat.AP stat.ME stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Portfolio allocation with gross-exposure constraint is an effective method to
increase the efficiency and stability of selected portfolios among a vast pool
of assets, as demonstrated in Fan et al (2008). The required high-dimensional
volatility matrix can be estimated by using high frequency financial data. This
enables us to better adapt to the local volatilities and local correlations
among vast number of assets and to increase significantly the sample size for
estimating the volatility matrix. This paper studies the volatility matrix
estimation using high-dimensional high-frequency data from the perspective of
portfolio selection. Specifically, we propose the use of "pairwise-refresh
time" and "all-refresh time" methods proposed by Barndorff-Nielsen et al (2008)
for estimation of vast covariance matrix and compare their merits in the
portfolio selection. We also establish the concentration inequalities of the
estimates, which guarantee desirable properties of the estimated volatility
matrix in vast asset allocation with gross exposure constraints. Extensive
numerical studies are made via carefully designed simulations. Comparing with
the methods based on low frequency daily data, our methods can capture the most
recent trend of the time varying volatility and correlation, hence provide more
accurate guidance for the portfolio allocation in the next time period. The
advantage of using high-frequency data is significant in our simulation and
empirical studies, which consist of 50 simulated assets and 30 constituent
stocks of Dow Jones Industrial Average index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4965</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4965</id><created>2010-04-28</created><authors><author><keyname>Zaslavskiy</keyname><forenames>Mikhail</forenames><affiliation>CBIO</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Rocquencourt, LIENS</affiliation></author><author><keyname>Vert</keyname><forenames>Jean-Philippe</forenames><affiliation>CBIO</affiliation></author></authors><title>Many-to-Many Graph Matching: a Continuous Relaxation Approach</title><categories>stat.ML cs.CV</categories><comments>19</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs provide an efficient tool for object representation in various
computer vision applications. Once graph-based representations are constructed,
an important question is how to compare graphs. This problem is often
formulated as a graph matching problem where one seeks a mapping between
vertices of two graphs which optimally aligns their structure. In the classical
formulation of graph matching, only one-to-one correspondences between vertices
are considered. However, in many applications, graphs cannot be matched
perfectly and it is more interesting to consider many-to-many correspondences
where clusters of vertices in one graph are matched to clusters of vertices in
the other graph. In this paper, we formulate the many-to-many graph matching
problem as a discrete optimization problem and propose an approximate algorithm
based on a continuous relaxation of the combinatorial problem. We compare our
method with other existing methods on several benchmark computer vision
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5031</identifier>
 <datestamp>2010-04-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5031</id><created>2010-04-28</created><authors><author><keyname>Ballo</keyname><forenames>Amparo</forenames></author><author><keyname>Cuesta-Albertos</keyname><forenames>Juan Antonio</forenames></author><author><keyname>Cuevas</keyname><forenames>Antonio</forenames></author></authors><title>Supervised classification for a family of Gaussian functional models</title><categories>stat.ML math.ST stat.TH</categories><comments>30 pages, 6 figures, 2 tables</comments><msc-class>60G15, 60G35, 62G05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the framework of supervised classification (discrimination) for functional
data, it is shown that the optimal classification rule can be explicitly
obtained for a class of Gaussian processes with "triangular" covariance
functions. This explicit knowledge has two practical consequences. First, the
consistency of the well-known nearest neighbors classifier (which is not
guaranteed in the problems with functional data) is established for the
indicated class of processes. Second, and more important, parametric and
nonparametric plug-in classifiers can be obtained by estimating the unknown
elements in the optimal rule. The performance of these new plug-in classifiers
is checked, with positive results, through a simulation study and a real data
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5074</identifier>
 <datestamp>2010-06-23</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5074</id><created>2010-04-28</created><authors><author><keyname>Robert</keyname><forenames>Christian P.</forenames></author></authors><title>Evidence and Evolution: A Review</title><categories>stat.ME math.ST q-bio.PE stat.TH</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  "Evidence and Evolution: the Logic behind the Science" was published in 2008
by Elliott Sober. It examines the philosophical foundations of the statistical
arguments used to evaluate hypotheses in evolutionary biology, based on simple
examples and likelihood ratios. The difficulty with reading the book from a
statistician's perspective is the reluctance of the author to engage into model
building and even less into parameter estimation. The first chapter nonetheless
constitutes a splendid coverage of the most common statistical approaches to
testing and model comparison, even though the advocation of the Akaike
information criterion against Bayesian alternatives is rather forceful. The
book also covers an examination of the "intelligent design" arguments against
the Darwinian evolution theory, predictably if unnecessarily resorting to
Popperian arguments to correctly argue that the creationist perspective fails
to predict anything. The following chapters cover the more relevant issues of
assessing selection versus drift and of testing for the presence of a common
ancestor. While remaining a philosophy treatise, Evidence and Evolution is
written in a way that is accessible to laymen, if rather unusual from a
statistician viewpoint, and the insight about testing issues gained from
Evidence and Evolution makes it a worthwhile read.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5178</identifier>
 <datestamp>2010-12-27</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5178</id><created>2010-04-28</created><updated>2010-12-24</updated><authors><author><keyname>Fan</keyname><forenames>Jianqing</forenames></author><author><keyname>Guo</keyname><forenames>Shaojun</forenames></author><author><keyname>Hao</keyname><forenames>Ning</forenames></author></authors><title>Variance Estimation Using Refitted Cross-validation in Ultrahigh
  Dimensional Regression</title><categories>stat.ME math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variance estimation is a fundamental problem in statistical modeling. In
ultrahigh dimensional linear regressions where the dimensionality is much
larger than sample size, traditional variance estimation techniques are not
applicable. Recent advances on variable selection in ultrahigh dimensional
linear regressions make this problem accessible. One of the major problems in
ultrahigh dimensional regression is the high spurious correlation between the
unobserved realized noise and some of the predictors. As a result, the realized
noises are actually predicted when extra irrelevant variables are selected,
leading to serious underestimate of the noise level. In this paper, we propose
a two-stage refitted procedure via a data splitting technique, called refitted
cross-validation (RCV), to attenuate the influence of irrelevant variables with
high spurious correlations. Our asymptotic results show that the resulting
procedure performs as well as the oracle estimator, which knows in advance the
mean regression function. The simulation studies lend further support to our
theoretical claims. The naive two-stage estimator which fits the selected
variables in the first stage and the plug-in one stage estimators using LASSO
and SCAD are also studied and compared. Their performances can be improved by
the proposed RCV method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5194</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5194</id><created>2010-04-29</created><authors><author><keyname>Ryabko</keyname><forenames>Daniil</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Clustering processes</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>in proceedings of ICML 2010. arXiv-admin note: for version 2 of this
  article please see: arXiv:1005.0826v1</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of clustering is considered, for the case when each data point is
a sample generated by a stationary ergodic process. We propose a very natural
asymptotic notion of consistency, and show that simple consistent algorithms
exist, under most general non-parametric assumptions. The notion of consistency
is as follows: two samples should be put into the same cluster if and only if
they were generated by the same distribution. With this notion of consistency,
clustering generalizes such classical statistical problems as homogeneity
testing and process classification. We show that, for the case of a known
number of clusters, consistency can be achieved under the only assumption that
the joint distribution of the data is stationary ergodic (no parametric or
Markovian assumptions, no assumptions of independence, neither between nor
within the samples). If the number of clusters is unknown, consistency can be
achieved under appropriate assumptions on the mixing rates of the processes.
(again, no parametric or independence assumptions). In both cases we give
examples of simple (at most quadratic in each argument) algorithms which are
consistent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5199</identifier>
 <datestamp>2010-11-12</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5199</id><created>2010-04-29</created><updated>2010-11-10</updated><authors><author><keyname>Arkoun</keyname><forenames>Ouerdia</forenames><affiliation>LMRS</affiliation></author></authors><title>Sequential adaptive estimators in nonparametric autoregressive models</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We constuct a sequential adaptive procedure for estimating the autoregressive
function at a given point in nonparametric autoregression models with Gaussian
noise. We make use of the sequential kernel estimators. The optimal adaptive
convergence rate is given as well as the upper bound for the minimax risk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5225</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5225</id><created>2010-04-29</created><authors><author><keyname>Palla</keyname><forenames>G.</forenames></author><author><keyname>Lovasz</keyname><forenames>L.</forenames></author><author><keyname>Vicsek</keyname><forenames>T.</forenames></author></authors><title>Multifractal Network Generator</title><categories>physics.data-an math-ph math.MP physics.soc-ph stat.ME</categories><comments>Preprint. Final version appeared in PNAS.</comments><journal-ref>Proc. Natl. Acad. Sci. USA 107: 7640-7645, (2010)</journal-ref><doi>10.1073/pnas.0912983107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new approach to constructing networks with realistic features.
Our method, in spite of its conceptual simplicity (it has only two parameters)
is capable of generating a wide variety of network types with prescribed
statistical properties, e.g., with degree- or clustering coefficient
distributions of various, very different forms. In turn, these graphs can be
used to test hypotheses, or, as models of actual data. The method is based on a
mapping between suitably chosen singular measures defined on the unit square
and sparse infinite networks. Such a mapping has the great potential of
allowing for graph theoretical results for a variety of network topologies. The
main idea of our approach is to go to the infinite limit of the singular
measure and the size of the corresponding graph simultaneously. A very unique
feature of this construction is that the complexity of the generated network is
increasing with the size. We present analytic expressions derived from the
parameters of the -- to be iterated-- initial generating measure for such major
characteristics of graphs as their degree, clustering coefficient and
assortativity coefficient distributions. The optimal parameters of the
generating measure are determined from a simple simulated annealing process.
Thus, the present work provides a tool for researchers from a variety of fields
(such as biology, computer science, biology, or complex systems) enabling them
to create a versatile model of their network data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5229</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5229</id><created>2010-04-29</created><updated>2010-10-13</updated><authors><author><keyname>Filippi</keyname><forenames>Sarah</forenames><affiliation>LTCI</affiliation></author><author><keyname>Capp</keyname><forenames>Olivier</forenames><affiliation>LTCI</affiliation></author><author><keyname>Garivier</keyname><forenames>Aurlien</forenames><affiliation>LTCI</affiliation></author></authors><title>Optimism in Reinforcement Learning and Kullback-Leibler Divergence</title><categories>cs.LG math.ST stat.ML stat.TH</categories><comments>This work has been accepted and presented at ALLERTON 2010;
  Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton
  Conference on, Monticello (Illinois) : \'Etats-Unis (2010)</comments><proxy>ccsd</proxy><doi>10.1109/ALLERTON.2010.5706896</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider model-based reinforcement learning in finite Markov De- cision
Processes (MDPs), focussing on so-called optimistic strategies. In MDPs,
optimism can be implemented by carrying out extended value it- erations under a
constraint of consistency with the estimated model tran- sition probabilities.
The UCRL2 algorithm by Auer, Jaksch and Ortner (2009), which follows this
strategy, has recently been shown to guarantee near-optimal regret bounds. In
this paper, we strongly argue in favor of using the Kullback-Leibler (KL)
divergence for this purpose. By studying the linear maximization problem under
KL constraints, we provide an ef- ficient algorithm, termed KL-UCRL, for
solving KL-optimistic extended value iteration. Using recent deviation bounds
on the KL divergence, we prove that KL-UCRL provides the same guarantees as
UCRL2 in terms of regret. However, numerical experiments on classical
benchmarks show a significantly improved behavior, particularly when the MDP
has reduced connectivity. To support this observation, we provide elements of
com- parison between the two algorithms based on geometric considerations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5265</identifier>
 <datestamp>2011-06-24</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5265</id><created>2010-04-29</created><updated>2011-06-23</updated><authors><author><keyname>Henao</keyname><forenames>Ricardo</forenames></author><author><keyname>Winther</keyname><forenames>Ole</forenames></author></authors><title>Sparse Linear Identifiable Multivariate Modeling</title><categories>stat.ML</categories><comments>45 pages, 17 figures</comments><journal-ref>JMLR, 12(Mar):863-905, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider sparse and identifiable linear latent variable
(factor) and linear Bayesian network models for parsimonious analysis of
multivariate data. We propose a computationally efficient method for joint
parameter and model inference, and model comparison. It consists of a fully
Bayesian hierarchy for sparse models using slab and spike priors (two-component
delta-function and continuous mixtures), non-Gaussian latent factors and a
stochastic search over the ordering of the variables. The framework, which we
call SLIM (Sparse Linear Identifiable Multivariate modeling), is validated and
bench-marked on artificial and real biological data sets. SLIM is closest in
spirit to LiNGAM (Shimizu et al., 2006), but differs substantially in
inference, Bayesian network structure learning and model comparison.
Experimentally, SLIM performs equally well or better than LiNGAM with
comparable computational complexity. We attribute this mainly to the stochastic
search strategy used, and to parsimony (sparsity and identifiability), which is
an explicit part of the model. We propose two extensions to the basic i.i.d.
linear framework: non-linear dependence on observed variables, called SNIM
(Sparse Non-linear Identifiable Multivariate modeling) and allowing for
correlations between latent variables, called CSLIM (Correlated SLIM), for the
temporal and/or spatial data. The source code and scripts are available from
http://cogsys.imm.dtu.dk/slim/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5300</identifier>
 <datestamp>2010-12-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5300</id><created>2010-04-29</created><updated>2010-12-19</updated><authors><author><keyname>Kim</keyname><forenames>Kyung In</forenames><affiliation>PMA</affiliation></author><author><keyname>Roquain</keyname><forenames>Etienne</forenames><affiliation>PMA</affiliation></author><author><keyname>Van De Wiel</keyname><forenames>Mark</forenames></author></authors><title>Spatial clustering of array CGH features in combination with
  hierarchical multiple testing</title><categories>stat.AP stat.ME</categories><proxy>ccsd</proxy><journal-ref>Statistical Applications in Genetics and Molecular Biology (2010)
  Vol. 9 : Iss. 1, Article 40</journal-ref><doi>10.2202/1544-6115.1532</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new approach for clustering DNA features using array CGH data
from multiple tumor samples. We distinguish data-collapsing: joining contiguous
DNA clones or probes with extremely similar data into regions, from clustering:
joining contiguous, correlated regions based on a maximum likelihood principle.
The model-based clustering algorithm accounts for the apparent spatial patterns
in the data. We evaluate the randomness of the clustering result by a cluster
stability score in combination with cross-validation. Moreover, we argue that
the clustering really captures spatial genomic dependency by showing that
coincidental clustering of independent regions is very unlikely. Using the
region and cluster information, we combine testing of these for association
with a clinical variable in an hierarchical multiple testing approach. This
allows for interpreting the significance of both regions and clusters while
controlling the Family-Wise Error Rate simultaneously. We prove that in the
context of permutation tests and permutation-invariant clusters it is allowed
to perform clustering and testing on the same data set. Our procedures are
illustrated on two cancer data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5328</identifier>
 <datestamp>2012-01-09</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5328</id><created>2010-04-29</created><updated>2010-12-27</updated><authors><author><keyname>Krivitsky</keyname><forenames>Pavel N.</forenames><affiliation>Department of Statistics and iLab, Carnegie Mellon University, Pittsburgh, USA</affiliation><affiliation>Institute for Systems and Robotics, Instituto Superior Tcnico, Lisbon, Portugal</affiliation></author><author><keyname>Handcock</keyname><forenames>Mark S.</forenames><affiliation>Department of Statistics, University of California at Los Angeles, Los Angeles, USA</affiliation></author><author><keyname>Morris</keyname><forenames>Martina</forenames><affiliation>Department of Sociology and Department of Statistics, University of Washington, Seattle, USA</affiliation></author></authors><title>Adjusting for Network Size and Composition Effects in Exponential-Family
  Random Graph Models</title><categories>stat.ME</categories><comments>37 pages, 2 figures, 5 tables; notation revised and clarified, some
  sections (particularly 4.3 and 5) made more rigorous, some derivations moved
  into the appendix, typos fixed, some wording changed</comments><msc-class>91D30 (Primary) 62D, 62F12, 62F40, 62P25, 62M40 (Secondary)</msc-class><journal-ref>Statistical Methodology 8 (2011) 319-339</journal-ref><doi>10.1016/j.stamet.2011.01.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exponential-family random graph models (ERGMs) provide a principled way to
model and simulate features common in human social networks, such as
propensities for homophily and friend-of-a-friend triad closure. We show that,
without adjustment, ERGMs preserve density as network size increases. Density
invariance is often not appropriate for social networks. We suggest a simple
modification based on an offset which instead preserves the mean degree and
accommodates changes in network composition asymptotically. We demonstrate that
this approach allows ERGMs to be applied to the important situation of
egocentrically sampled data. We analyze data from the National Health and
Social Life Survey (NHSLS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5418</identifier>
 <datestamp>2010-09-20</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5418</id><created>2010-04-29</created><updated>2010-09-17</updated><authors><author><keyname>Sued</keyname><forenames>Mariela</forenames></author><author><keyname>Yohai</keyname><forenames>Victor J.</forenames></author></authors><title>Robust location estimation with missing data</title><categories>math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a missing-data setting, we have a sample in which a vector of explanatory
variables x_i is observed for every subject i, while scalar outcomes y_i are
missing by happenstance on some individuals. In this work we propose robust
estimates of the distribution of the responses assuming missing at random (MAR)
data, under a semiparametric regression model. Our approach allows the
consistent estimation of any weakly continuous functional of the response's
distribution. In particular, strongly consistent estimates of any continuous
location functional, such as the median or MM functionals, are proposed. A
robust fit for the regression model combined with the robust properties of the
location functional gives rise to a robust recipe for estimating the location
parameter. Robustness is quantified through the breakdown point of the proposed
procedure. The asymptotic distribution of the location estimates is also
derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5479</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5479</id><created>2010-04-30</created><authors><author><keyname>Zhang</keyname><forenames>Wenyi</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>On Minimax Robust Detection of Stationary Gaussian Signals in White
  Gaussian Noise</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>Submitted; extended abstract to appear in ISIT 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of detecting a wide-sense stationary Gaussian signal process
embedded in white Gaussian noise, where the power spectral density of the
signal process exhibits uncertainty, is investigated. The performance of
minimax robust detection is characterized by the exponential decay rate of the
miss probability under a Neyman-Pearson criterion with a fixed false alarm
probability, as the length of the observation interval grows without bound. A
dominance condition is identified for the uncertainty set of spectral density
functions, and it is established that, under the dominance condition, the
resulting minimax problem possesses a saddle point, which is achievable by the
likelihood ratio tests matched to a so-called dominated power spectral density
in the uncertainty set. No convexity condition on the uncertainty set is
required to establish this result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5485</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5485</id><created>2010-04-30</created><updated>2011-06-09</updated><authors><author><keyname>Arias-Castro</keyname><forenames>Ery</forenames><affiliation>Math Dept, UCSD</affiliation></author><author><keyname>Pelletier</keyname><forenames>Bruno</forenames><affiliation>IRMAR</affiliation></author><author><keyname>Pudlo</keyname><forenames>Pierre</forenames><affiliation>I3M</affiliation></author></authors><title>The Normalized Graph Cut and Cheeger Constant: from Discrete to
  Continuous</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><journal-ref>Advances in Applied Probability (2012) Volume 44, Number 4,
  907-937</journal-ref><doi>10.1239/aap/1354716583</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let M be a bounded domain of a Euclidian space with smooth boundary. We
relate the Cheeger constant of M and the conductance of a neighborhood graph
defined on a random sample from M. By restricting the minimization defining the
latter over a particular class of subsets, we obtain consistency (after
normalization) as the sample size increases, and show that any minimizing
sequence of subsets has a subsequence converging to a Cheeger set of M.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5529</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5529</id><created>2010-04-30</created><updated>2011-05-04</updated><authors><author><keyname>Villard</keyname><forenames>Joffrey</forenames></author><author><keyname>Bianchi</keyname><forenames>Pascal</forenames></author></authors><title>High-Rate Vector Quantization for the Neyman-Pearson Detection of
  Correlated Processes</title><categories>cs.IT math.IT math.PR math.ST stat.TH</categories><comments>47 pages, 7 figures, 1 table. To appear in the IEEE Transactions on
  Information Theory</comments><doi>10.1109/TIT.2011.2158479</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the effect of quantization on the performance of the
Neyman-Pearson test. It is assumed that a sensing unit observes samples of a
correlated stationary ergodic multivariate process. Each sample is passed
through an N-point quantizer and transmitted to a decision device which
performs a binary hypothesis test. For any false alarm level, it is shown that
the miss probability of the Neyman-Pearson test converges to zero exponentially
as the number of samples tends to infinity, assuming that the observed process
satisfies certain mixing conditions. The main contribution of this paper is to
provide a compact closed-form expression of the error exponent in the high-rate
regime i.e., when the number N of quantization levels tends to infinity,
generalizing previous results of Gupta and Hero to the case of non-independent
observations. If d represents the dimension of one sample, it is proved that
the error exponent converges at rate N^{2/d} to the one obtained in the absence
of quantization. As an application, relevant high-rate quantization strategies
which lead to a large error exponent are determined. Numerical results indicate
that the proposed quantization rule can yield better performance than existing
ones in terms of detection error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5587</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5587</id><created>2010-04-30</created><authors><author><keyname>Evans</keyname><forenames>Steven N.</forenames></author><author><keyname>Hower</keyname><forenames>Valerie</forenames></author><author><keyname>Pachter</keyname><forenames>Lior</forenames></author></authors><title>Coverage statistics for sequence census methods</title><categories>q-bio.GN math.PR stat.AP</categories><comments>10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: We study the statistical properties of fragment coverage in
genome sequencing experiments. In an extension of the classic Lander-Waterman
model, we consider the effect of the length distribution of fragments. We also
introduce the notion of the shape of a coverage function, which can be used to
detect abberations in coverage. The probability theory underlying these
problems is essential for constructing models of current high-throughput
sequencing experiments, where both sample preparation protocols and sequencing
technology particulars can affect fragment length distributions.
  Results: We show that regardless of fragment length distribution and under
the mild assumption that fragment start sites are Poisson distributed, the
fragments produced in a sequencing experiment can be viewed as resulting from a
two-dimensional spatial Poisson process. We then study the jump skeleton of the
the coverage function, and show that the induced trees are Galton-Watson trees
whose parameters can be computed.
  Conclusions: Our results extend standard analyses of shotgun sequencing that
focus on coverage statistics at individual sites, and provide a null model for
detecting deviations from random coverage in high-throughput sequence census
based experiments. By focusing on fragments, we are also led to a new approach
for visualizing sequencing data that should be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0188</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0188</id><created>2010-05-03</created><authors><author><keyname>Mehta</keyname><forenames>Nishant A.</forenames></author><author><keyname>Gray</keyname><forenames>Alexander G.</forenames></author></authors><title>Generative and Latent Mean Map Kernels</title><categories>cs.LG stat.ML</categories><comments>16 pages, 1 figure, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce two kernels that extend the mean map, which embeds probability
measures in Hilbert spaces. The generative mean map kernel (GMMK) is a smooth
similarity measure between probabilistic models. The latent mean map kernel
(LMMK) generalizes the non-iid formulation of Hilbert space embeddings of
empirical distributions in order to incorporate latent variable models. When
comparing certain classes of distributions, the GMMK exhibits beneficial
regularization and generalization properties not shown for previous generative
kernels. We present experiments comparing support vector machine performance
using the GMMK and LMMK between hidden Markov models to the performance of
other methods on discrete and continuous observation sequence data. The results
suggest that, in many cases, the GMMK has generalization error competitive with
or better than other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0208</identifier>
 <datestamp>2012-03-28</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0208</id><created>2010-05-03</created><updated>2012-03-26</updated><authors><author><keyname>Biau</keyname><forenames>Grard</forenames><affiliation>LSTA, DMA, LPMA</affiliation></author></authors><title>Analysis of a Random Forests Model</title><categories>stat.ML math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random forests are a scheme proposed by Leo Breiman in the 2000's for
building a predictor ensemble with a set of decision trees that grow in
randomly selected subspaces of data. Despite growing interest and practical
use, there has been little exploration of the statistical properties of random
forests, and little is known about the mathematical forces driving the
algorithm. In this paper, we offer an in-depth analysis of a random forests
model suggested by Breiman in \cite{Bre04}, which is very close to the original
algorithm. We show in particular that the procedure is consistent and adapts to
sparsity, in the sense that its rate of convergence depends only on the number
of strong features and not on how many noise variables are present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0312</identifier>
 <datestamp>2010-11-29</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0312</id><created>2010-05-03</created><updated>2010-11-25</updated><authors><author><keyname>Wang</keyname><forenames>Yizao</forenames></author><author><keyname>Stoev</keyname><forenames>Stilian A.</forenames></author></authors><title>Conditional Sampling for Spectrally Discrete Max-Stable Random Fields</title><categories>stat.CO math.PR</categories><comments>31 pages. 4 figures. Data analysis removed from the Technical Report
  (previous version). To appear in Advances in Applied Probability</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Max-stable random fields play a central role in modeling extreme value
phenomena. We obtain an explicit formula for the conditional probability in
general max-linear models, which include a large class of max-stable random
fields. As a consequence, we develop an algorithm for efficient and exact
sampling from the conditional distributions. Our method provides a
computational solution to the prediction problem for spectrally discrete
max-stable random fields. This work offers new tools and a new perspective to
many statistical inference problems for spatial extremes, arising, for example,
in meteorology, geology, and environmental applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0366</identifier>
 <datestamp>2012-11-21</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0366</id><created>2010-05-03</created><updated>2012-11-20</updated><authors><author><keyname>Stdler</keyname><forenames>Nicolas</forenames></author><author><keyname>Stekhoven</keyname><forenames>Daniel J.</forenames></author><author><keyname>Bhlmann</keyname><forenames>Peter</forenames></author></authors><title>Pattern Alternating Maximization Algorithm for Missing Data in Large P,
  Small N Problems</title><categories>stat.ME</categories><comments>extended version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new and computationally efficient algorithm for maximizing the
observed log-likelihood for a multivariate normal data matrix with missing
values. We show that our procedure based on iteratively regressing the missing
on the observed variables, generalizes the standard EM algorithm by alternating
between different complete data spaces and performing the E-Step incrementally.
In this non-standard setup we prove numerical convergence to a stationary point
of the observed log-likelihood.
  For high-dimensional data, where the number of variables may greatly exceed
sample size, we add a Lasso penalty in the regression part of our algorithm and
perform coordinate descent approximations. This leads to a computationally very
attractive technique with sparse regression coefficients for missing data
imputation. Simulations and results on four microarray datasets show that the
new method often outperforms other imputation techniques as k-nearest neighbors
imputation, nuclear norm minimization or a penalized likelihood approach with
an l1-penalty on the inverse covariance matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0437</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0437</id><created>2010-05-04</created><authors><author><keyname>Kloft</keyname><forenames>Marius</forenames></author><author><keyname>Rckert</keyname><forenames>Ulrich</forenames></author><author><keyname>Bartlett</keyname><forenames>Peter L.</forenames></author></authors><title>A Unifying View of Multiple Kernel Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research on multiple kernel learning has lead to a number of
approaches for combining kernels in regularized risk minimization. The proposed
approaches include different formulations of objectives and varying
regularization strategies. In this paper we present a unifying general
optimization criterion for multiple kernel learning and show how existing
formulations are subsumed as special cases. We also derive the criterion's dual
representation, which is suitable for general smooth optimization algorithms.
Finally, we evaluate multiple kernel learning in this framework analytically
using a Rademacher complexity bound on the generalization error and empirically
in a set of experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0483</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0483</id><created>2010-05-04</created><updated>2012-03-01</updated><authors><author><keyname>Bulinski</keyname><forenames>Alexander</forenames></author><author><keyname>Spodarev</keyname><forenames>Evgeny</forenames></author><author><keyname>Timmermann</keyname><forenames>Florian</forenames></author></authors><title>Central limit theorems for the excursion set volumes of weakly dependent
  random fields</title><categories>math.PR math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.3150/10-BEJ339 the Bernoulli
  (http://isi.cbs.nl/bernoulli/) by the International Statistical
  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</comments><proxy>vtex</proxy><report-no>IMS-BEJ-BEJ339</report-no><journal-ref>Bernoulli 2012, Vol. 18, No. 1, 100-118</journal-ref><doi>10.3150/10-BEJ339</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multivariate central limit theorems (CLT) for the volumes of excursion
sets of stationary quasi-associated random fields on $\mathbb{R}^d$ are proved.
Special attention is paid to Gaussian and shot noise fields. Formulae for the
covariance matrix of the limiting distribution are provided. A statistical
version of the CLT is considered as well. Some numerical results are also
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0530</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0530</id><created>2010-05-04</created><authors><author><keyname>Shah</keyname><forenames>Mohak</forenames></author><author><keyname>Marchand</keyname><forenames>Mario</forenames></author><author><keyname>Corbeil</keyname><forenames>Jacques</forenames></author></authors><title>Feature Selection with Conjunctions of Decision Stumps and Learning from
  Microarray Data</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the objectives of designing feature selection learning algorithms is
to obtain classifiers that depend on a small number of attributes and have
verifiable future performance guarantees. There are few, if any, approaches
that successfully address the two goals simultaneously. Performance guarantees
become crucial for tasks such as microarray data analysis due to very small
sample sizes resulting in limited empirical evaluation. To the best of our
knowledge, such algorithms that give theoretical bounds on the future
performance have not been proposed so far in the context of the classification
of gene expression data. In this work, we investigate the premise of learning a
conjunction (or disjunction) of decision stumps in Occam's Razor, Sample
Compression, and PAC-Bayes learning settings for identifying a small subset of
attributes that can be used to perform reliable classification tasks. We apply
the proposed approaches for gene identification from DNA microarray data and
compare our results to those of well known successful approaches proposed for
the task. We show that our algorithm not only finds hypotheses with much
smaller number of genes while giving competitive classification accuracy but
also have tight risk guarantees on future performance unlike other approaches.
The proposed approaches are general and extensible in terms of both designing
novel algorithms and application to other domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0714</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0714</id><created>2010-05-05</created><updated>2014-12-28</updated><authors><author><keyname>Verbic</keyname><forenames>Srdjan</forenames></author></authors><title>Does it matter if you answer slowly?</title><categories>physics.ed-ph stat.AP</categories><comments>11 pages, 3 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have analyzed item response times measured at a large scale
unspeeded low stakes test for primary-school students. We have demonstrated the
existence of significant difference in the response time for boys and girls as
well as difference in response time of correct and incorrect answers on this
test. We have also demonstrated existence of the warm up effect for this test.
The results show that responses given by girls exhibit much greater warm up
effect and that difference appears to be the most important cause of the
difference on the test level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0766</identifier>
 <datestamp>2011-02-15</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0766</id><created>2010-05-05</created><updated>2011-02-12</updated><authors><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Willsky</keyname><forenames>Alan S.</forenames></author></authors><title>Learning High-Dimensional Markov Forest Distributions: Analysis of Error
  Rates</title><categories>cs.IT math.IT stat.ML</categories><comments>Accepted to the Journal of Machine Learning Research (Feb 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of learning forest-structured discrete graphical models from
i.i.d. samples is considered. An algorithm based on pruning of the Chow-Liu
tree through adaptive thresholding is proposed. It is shown that this algorithm
is both structurally consistent and risk consistent and the error probability
of structure learning decays faster than any polynomial in the number of
samples under fixed model size. For the high-dimensional scenario where the
size of the model d and the number of edges k scale with the number of samples
n, sufficient conditions on (n,d,k) are given for the algorithm to satisfy
structural and risk consistencies. In addition, the extremal structures for
learning are identified; we prove that the independent (resp. tree) model is
the hardest (resp. easiest) to learn using the proposed algorithm in terms of
error rates for structure learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0794</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0794</id><created>2010-05-05</created><authors><author><keyname>Yan</keyname><forenames>Xiaoran</forenames></author><author><keyname>Zhu</keyname><forenames>Yaojia</forenames></author><author><keyname>Rouquier</keyname><forenames>Jean-Baptiste</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>Active Learning for Hidden Attributes in Networks</title><categories>stat.ML cond-mat.stat-mech cs.IT cs.LG math.IT physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many networks, vertices have hidden attributes, or types, that are
correlated with the networks topology. If the topology is known but these
attributes are not, and if learning the attributes is costly, we need a method
for choosing which vertex to query in order to learn as much as possible about
the attributes of the other vertices. We assume the network is generated by a
stochastic block model, but we make no assumptions about its assortativity or
disassortativity. We choose which vertex to query using two methods: 1)
maximizing the mutual information between its attributes and those of the
others (a well-known approach in active learning) and 2) maximizing the average
agreement between two independent samples of the conditional Gibbs
distribution. Experimental results show that both these methods do much better
than simple heuristics. They also consistently identify certain vertices as
important by querying them early on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0826</identifier>
 <datestamp>2013-05-01</datestamp>
 <setSpec>stat</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0826</id><created>2010-05-05</created><updated>2013-04-30</updated><authors><author><keyname>Ryabko</keyname><forenames>Daniil</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Clustering processes</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>in proceedings of ICML 2010. arXiv-admin Note: This is a newer
  version of the article arXiv:1004.5194v1, please see that article for any
  previous version</comments><proxy>ccsd</proxy><journal-ref>27th International Conference on Machine Learning (2010) 919-926</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of clustering is considered, for the case when each data point is
a sample generated by a stationary ergodic process. We propose a very natural
asymptotic notion of consistency, and show that simple consistent algorithms
exist, under most general non-parametric assumptions. The notion of consistency
is as follows: two samples should be put into the same cluster if and only if
they were generated by the same distribution. With this notion of consistency,
clustering generalizes such classical statistical problems as homogeneity
testing and process classification. We show that, for the case of a known
number of clusters, consistency can be achieved under the only assumption that
the joint distribution of the data is stationary ergodic (no parametric or
Markovian assumptions, no assumptions of independence, neither between nor
within the samples). If the number of clusters is unknown, consistency can be
achieved under appropriate assumptions on the mixing rates of the processes.
(again, no parametric or independence assumptions). In both cases we give
examples of simple (at most quadratic in each argument) algorithms which are
consistent.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="0" completeListSize="12531">1150337|1001</resumptionToken>
</ListRecords>
</OAI-PMH>